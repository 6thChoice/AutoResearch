{"type":"queue-operation","operation":"enqueue","timestamp":"2026-02-23T12:49:04.971Z","sessionId":"f068886d-0e50-4922-9128-2594a90f9c43","content":"<!-- TASK_ID: task-52313d31 -->\n\n# é¡¹ç›®è®¡åˆ’\n\nç›®æ ‡ï¼šå¤ç°è®ºæ–‡å·¥ä½œ https://github.com/vivva/DLinear\n\næ­¥éª¤ï¼š\n  1. é˜…è¯»å’Œç†è§£è®ºæ–‡: è®¿é—® arXiv æˆ–å…¶ä»–è®ºæ–‡å‘å¸ƒå¹³å°ï¼Œæœç´¢æ ‡é¢˜ä¸º 'DLinear: A Simple Yet Effective Baseline for Time Seri\n  2. å®¡æŸ¥å’Œåˆ†ææºä»£ç ä»“åº“: è®¿é—® GitHub ä»“åº“ https://github.com/vivva/DLinearï¼Œä»”ç»†é˜…è¯» README.md æ–‡ä»¶ï¼Œäº†è§£é¡¹ç›®æ¦‚è¿°ã€å®‰è£…æŒ‡å—ã€ä½¿ç”¨æ–¹æ³•\n  3. è®¾ç½®å’ŒéªŒè¯å¼€å‘ç¯å¢ƒ: æ ¹æ®ä»“åº“è¦æ±‚ï¼Œåˆ›å»ºä¸€ä¸ªæ–°çš„ Python è™šæ‹Ÿç¯å¢ƒï¼ˆæ¨èä½¿ç”¨ conda æˆ– venvï¼‰ã€‚æ¿€æ´»è™šæ‹Ÿç¯å¢ƒåï¼Œä½¿ç”¨ pip æˆ– conda å®‰è£…æ‰€æœ‰ä¾èµ–åŒ…ï¼ˆé€šå¸¸é€šè¿‡\n  4. å‡†å¤‡å®éªŒæ•°æ®é›†: æŒ‰ç…§ä»“åº“çš„æŒ‡ç¤ºæˆ–è®ºæ–‡çš„æè¿°ï¼Œä¸‹è½½å®éªŒæ‰€éœ€çš„æ•°æ®é›†ï¼ˆä¾‹å¦‚ ETT, Weather, Electricity, Traffic ç­‰ï¼‰ã€‚ä½¿ç”¨ä»“åº“æä¾›çš„è„šæœ¬ï¼ˆå¦‚æœæœ‰ï¼‰\n  5. è¿è¡Œè®­ç»ƒè„šæœ¬å¹¶è°ƒè¯•: è¿è¡Œä»“åº“ä¸­çš„ä¸»è®­ç»ƒè„šæœ¬ï¼ˆä¾‹å¦‚ main.py æˆ– train.pyï¼‰ï¼Œä½¿ç”¨é»˜è®¤é…ç½®æˆ–è®ºæ–‡ä¸­æŠ¥å‘Šçš„è¶…å‚æ•°ï¼ˆå¦‚å­¦ä¹ ç‡ã€æ‰¹é‡å¤§å°ã€è¿­ä»£æ¬¡æ•°ï¼‰åœ¨å…¶ä¸­ä¸€ä¸ªæ•°æ®é›†ï¼ˆå¦‚ETT\n  6. å…¨é¢å¤ç°å®éªŒ: åœ¨æˆåŠŸè¿è¡Œå•ä¸ªå®éªŒåï¼Œæ‰©å±•è‡³æ‰€æœ‰è®ºæ–‡ä¸­æŠ¥å‘Šçš„æ•°æ®é›†å’Œé¢„æµ‹é•¿åº¦ï¼ˆhorizonï¼‰ã€‚ç¼–å†™æˆ–ä¿®æ”¹è„šæœ¬ä»¥æ‰¹é‡è¿è¡Œå®éªŒï¼Œæˆ–æ‰‹åŠ¨ä¾æ¬¡è¿è¡Œä¸åŒé…ç½®ã€‚å¯¹äºæ¯ä¸ªå®éªŒï¼Œè®°å½•æœ€ç»ˆçš„æµ‹\n  7. å¯¹æ¯”å’Œåˆ†æå¤ç°ç»“æœ: å°†ä¸Šä¸€æ­¥å¾—åˆ°çš„å®éªŒç»“æœä¸è®ºæ–‡ä¸­æŠ¥å‘Šçš„ç»“æœè¿›è¡Œé€é¡¹å¯¹æ¯”ã€‚è®¡ç®—ç›¸å¯¹å·®å¼‚ï¼Œå¹¶åˆ†ææ˜¯å¦å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼ˆä¾‹å¦‚ï¼ŒæŒ‡æ ‡è¯¯å·®åœ¨5%ä»¥å†…é€šå¸¸è®¤ä¸ºå¤ç°æˆåŠŸï¼‰ã€‚å¦‚æœå·®å¼‚è¾ƒå¤§ï¼Œæ£€æŸ¥å¯èƒ½çš„\n  8. å¯è§†åŒ–å’Œç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š: ä½¿ç”¨ matplotlib æˆ–ç±»ä¼¼åº“ç»˜åˆ¶å…³é”®å›¾è¡¨ï¼šè®­ç»ƒæŸå¤±æ›²çº¿ã€æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„é¢„æµ‹å€¼ä¸çœŸå®å€¼å¯¹æ¯”å›¾ï¼ˆé’ˆå¯¹ä¸åŒé¢„æµ‹é•¿åº¦ï¼‰ã€‚æ•´ç†æ•´ä¸ªå¤ç°è¿‡ç¨‹çš„æ­¥éª¤ã€ç¯å¢ƒé…ç½®ã€å®éªŒ\n\n---\n\n# é¡¹ç›®ä¸Šä¸‹æ–‡\n\n# DLinearè®ºæ–‡å¤ç°é¡¹ç›®ä¸Šä¸‹æ–‡\n\n## é¡¹ç›®ç›®æ ‡ä¸æ„¿æ™¯\n- **æ ¸å¿ƒç›®æ ‡**ï¼šå®Œæ•´å¤ç°è®ºæ–‡ã€ŠAre Transformers Effective for Time Series Forecasting?ã€‹ï¼ˆAAAI 2023 Oral, arXiv:2205.13504ï¼‰ä¸­æå‡ºçš„DLinearæ¨¡å‹å®éªŒä¸ç»“æœ\n- **å­¦æœ¯ä»·å€¼**ï¼šéªŒè¯DLinearä½œä¸ºæ—¶é—´åºåˆ—é¢„æµ‹ç®€å•æœ‰æ•ˆåŸºå‡†æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼ŒæŒ‘æˆ˜Transformeråœ¨è¯¥é¢†åŸŸçš„æœ‰æ•ˆæ€§å‡è®¾ï¼Œä¸ºåç»­ç ”ç©¶æä¾›å¯é å‚è€ƒ\n- **å·¥ç¨‹äº¤ä»˜**ï¼šäº§å‡ºå¯å¤ç°çš„ä»£ç åº“ã€å®éªŒè®°å½•ä¸å¯¹æ¯”åˆ†ææŠ¥å‘Š\n\n## æŠ€æœ¯æ ˆä¸æ¶æ„æ¦‚è¿°\n- **æ ¸å¿ƒæ¡†æ¶**ï¼šPyTorch\n- **æ•°æ®å¤„ç†**ï¼špandas, numpy\n- **å®éªŒç®¡ç†**ï¼šè‡ªå®šä¹‰é…ç½®è„šæœ¬ï¼ˆåŸºäºargparseæˆ–hydraï¼‰\n- **è¯„ä¼°æŒ‡æ ‡**ï¼šMSE, MAE\n- **æ¨¡å‹æ¶æ„**ï¼šDLinearï¼ˆåˆ†è§£+çº¿æ€§å±‚ï¼‰\n- **æ•°æ®é›†**ï¼šETTï¼ˆç”µåŠ›å˜å‹å™¨æ¸©åº¦ï¼‰ã€Weatherã€Electricityã€Traffic\n\n## å½“å‰è¿›åº¦æ‘˜è¦\n1. **è®ºæ–‡ç†è§£é˜¶æ®µ** âœ… å·²å®Œæˆè®ºæ–‡çš„å®šä½ã€æ£€ç´¢å’ŒåŸºæœ¬ä¿¡æ¯ç¡®è®¤ï¼ˆæ ‡é¢˜ã€æ ¸å¿ƒè´¡çŒ®ã€æ•°æ®é›†ã€è¯„ä¼°æŒ‡æ ‡ï¼‰\n2. **ä»£ç å®¡æŸ¥é˜¶æ®µ** âœ… å·²åˆ†æåŸå§‹ä»“åº“ç»“æ„ã€ä¾èµ–ä¸è¿è¡Œé€»è¾‘\n3. **ç¯å¢ƒé…ç½®é˜¶æ®µ** âœ… å·²å®Œæˆè™šæ‹Ÿç¯å¢ƒæ­å»ºä¸ä¾èµ–å®‰è£…\n4. **æ•°æ®å‡†å¤‡é˜¶æ®µ** âœ… å·²ä¸‹è½½ETTç­‰å…³é”®æ•°æ®é›†å¹¶å®Œæˆé¢„å¤„ç†\n5. **å•å®éªŒéªŒè¯é˜¶æ®µ** â³ æ­£åœ¨è¿›è¡Œç¬¬ä¸€ä¸ªæ•°æ®é›†ï¼ˆETTï¼‰çš„è®­ç»ƒä¸è°ƒè¯•\n6. **å…¨é¢å¤ç°é˜¶æ®µ** ğŸ“… è®¡åˆ’ä¸­\n7. **ç»“æœå¯¹æ¯”é˜¶æ®µ** ğŸ“… è®¡åˆ’ä¸­\n8. **æŠ¥å‘Šç”Ÿæˆé˜¶æ®µ** ğŸ“… è®¡åˆ’ä¸­\n\n## å…³é”®è®¾è®¡å†³ç­–\n1. **å¿ å®å¤ç°**ï¼šä¸¥æ ¼éµå¾ªè®ºæ–‡è¶…å‚æ•°ä¸å®éªŒè®¾ç½®ï¼Œé¿å…å¼•å…¥æœªç»éªŒè¯çš„ä¼˜åŒ–\n2. **æ¨¡å—åŒ–ä»£ç **ï¼šåˆ†ç¦»æ•°æ®åŠ è½½ã€æ¨¡å‹å®šä¹‰ã€è®­ç»ƒå¾ªç¯ä¸è¯„ä¼°é€»è¾‘ï¼Œä¾¿äºè°ƒè¯•ä¸æ‰©å±•\n3. **å¯å¤ç°æ€§**ï¼šå›ºå®šéšæœºç§å­ï¼Œè®°å½•å®Œæ•´ç¯å¢ƒé…ç½®ï¼ˆPythonç‰ˆæœ¬ã€åº“ç‰ˆæœ¬ï¼‰\n4. **æ¸è¿›éªŒè¯**ï¼šä»å•ä¸ªæ•°æ®é›†/é…ç½®å¼€å§‹ï¼ŒéªŒè¯æˆåŠŸåå†æ‰©å±•è‡³å…¨å®éªŒ\n5. **ç»“æœå¯¹æ¯”åŸºå‡†**ï¼šä»¥è®ºæ–‡æŠ¥å‘Šç»“æœä¸ºæ ‡å‡†ï¼Œæ¥å—Â±5%å†…çš„è¯¯å·®ä¸ºå¤ç°æˆåŠŸ\n\n---\n\n# å½“å‰ä»»åŠ¡ä¸Šä¸‹æ–‡\n\n### ä»»åŠ¡ä¸Šä¸‹æ–‡ï¼šæ­¥éª¤ 1/8 - ç¡®å®šæ•°æ®ç»“æ„å’Œå­˜å‚¨æ–¹æ¡ˆ\n\n**1. å½“å‰æ­¥éª¤åœ¨æ•´ä½“è®¡åˆ’ä¸­çš„ä½ç½®**\n*   è¿™æ˜¯é¡¹ç›®çš„ç¬¬ä¸€æ­¥ï¼Œå±äº**åˆå§‹åŒ–ä¸è®¾è®¡é˜¶æ®µ**ã€‚æœ¬æ­¥éª¤å°†ä¸ºåç»­çš„æ•°æ®é‡‡é›†ã€å¤„ç†ã€æ¨¡å‹éƒ¨ç½²ç­‰æ‰€æœ‰ç¯èŠ‚å¥ å®šæ•°æ®å±‚é¢çš„åŸºç¡€ã€‚\n\n**2. å‰ç½®æ­¥éª¤çš„å®Œæˆæƒ…å†µ**\n*   æ— ã€‚æœ¬é¡¹ç›®ä»æ­¤æ­¥éª¤å¼€å§‹ã€‚\n\n**3. æœ¬æ­¥éª¤çš„å…·ä½“ç›®æ ‡ä¸çº¦æŸ**\n*   **ç›®æ ‡**ï¼šè®¾è®¡å¹¶ç¡®å®šç”¨äºå­˜å‚¨ç³»ç»Ÿç›‘æ§æ•°æ®ï¼ˆå¦‚CPUã€å†…å­˜ã€ç£ç›˜I/Oã€ç½‘ç»œæµé‡ç­‰æ—¶é—´åºåˆ—ï¼‰çš„æ ¸å¿ƒæ•°æ®ç»“æ„å’Œå­˜å‚¨æ–¹æ¡ˆã€‚\n*   **å…³é”®è¦æ±‚**ï¼š\n    *   æ”¯æŒé«˜æ•ˆçš„æ—¶é—´åºåˆ—æ•°æ®å†™å…¥ä¸æŸ¥è¯¢ã€‚\n    *   æ•°æ®ç»“æ„éœ€å…·å¤‡çµæ´»æ€§ï¼Œä¾¿äºæœªæ¥æ·»åŠ æ–°çš„ç›‘æ§æŒ‡æ ‡ã€‚\n    *   æ–¹æ¡ˆéœ€ä¸ºåç»­å¯èƒ½çš„å®æ—¶åˆ†æä¸æ¨¡å‹é¢„æµ‹é¢„ç•™æ¥å£ã€‚\n*   **ä¸»è¦çº¦æŸ**ï¼š\n    *   å¿…é¡»æ»¡è¶³å¿«é€Ÿæ•°æ®å†™å…¥å’Œé«˜å¹¶å‘æŸ¥è¯¢çš„æ€§èƒ½éœ€æ±‚ã€‚\n    *   æ•°æ®ç»“æ„è®¾è®¡åº”é¿å…åœ¨æŒ‡æ ‡æ‰©å±•æ—¶è¿›è¡Œå¤§è§„æ¨¡é‡æ„ã€‚\n\n**4. ä¸å…¶ä»–æ­¥éª¤çš„ä¾èµ–å…³ç³»**\n*   **åç»­æ­¥éª¤å‡ä¾èµ–äºæ­¤**ï¼šæ­¥éª¤2ï¼ˆå®ç°æ•°æ®é‡‡é›†å™¨ï¼‰å°†ä¾æ®æœ¬æ­¥éª¤å®šä¹‰çš„æ ¼å¼å†™å…¥æ•°æ®ï¼›æ­¥éª¤3åŠä¹‹åçš„æŸ¥è¯¢ã€åˆ†æä¸é¢„æµ‹åŠŸèƒ½å°†ç›´æ¥ä¾èµ–æœ¬æ­¥éª¤ç¡®å®šçš„å­˜å‚¨æ–¹æ¡ˆè¿›è¡Œæ•°æ®è¯»å–ã€‚æ­¤æ­¥éª¤çš„å†³ç­–å°†ç›´æ¥å½±å“æ•´ä¸ªç³»ç»Ÿçš„æ€§èƒ½å’Œå¯ç»´æŠ¤æ€§ã€‚\n\n## ä»»åŠ¡æè¿°\n\nè®¿é—® arXiv æˆ–å…¶ä»–è®ºæ–‡å‘å¸ƒå¹³å°ï¼Œæœç´¢æ ‡é¢˜ä¸º 'DLinear: A Simple Yet Effective Baseline for Time Series Forecasting' çš„è®ºæ–‡å¹¶ä¸‹è½½ã€‚ä»”ç»†é˜…è¯»è®ºæ–‡ï¼Œé‡ç‚¹å…³æ³¨æ¨¡å‹æ¶æ„ã€æ•°å­¦åŸç†ã€å®éªŒè®¾ç½®ã€æ•°æ®é›†ã€è¯„ä¼°æŒ‡æ ‡å’Œæ ¸å¿ƒç»“è®ºã€‚åœ¨ç¬”è®°ä¸­è®°å½•å…³é”®ä¿¡æ¯ï¼ŒåŒ…æ‹¬æ¨¡å‹ç»“æ„å›¾ã€å…¬å¼ã€è¶…å‚æ•°å’ŒåŸºå‡†ç»“æœã€‚\n\n## çº¦æŸæ¡ä»¶\n\nç¡®ä¿å¯¹æ—¶é—´åºåˆ—é¢„æµ‹å’Œæ·±åº¦å­¦ä¹ æœ‰åŸºæœ¬ç†è§£ã€‚å¦‚æœé‡åˆ°ä¸ç†Ÿæ‚‰çš„æœ¯è¯­æˆ–æ¦‚å¿µï¼Œéœ€è¦è¿›è¡Œé¢å¤–æŸ¥é˜…ã€‚\n\n## é¢„æœŸè¾“å‡º\n\nä¸€ä»½åŒ…å«è®ºæ–‡æ ¸å¿ƒæ€æƒ³ã€æ¨¡å‹ç»†èŠ‚å’Œå®éªŒè®¾ç½®çš„é˜…è¯»ç¬”è®°ã€‚"}
{"type":"queue-operation","operation":"dequeue","timestamp":"2026-02-23T12:49:04.973Z","sessionId":"f068886d-0e50-4922-9128-2594a90f9c43"}
{"parentUuid":null,"isSidechain":false,"userType":"external","cwd":"/app/workspace","sessionId":"f068886d-0e50-4922-9128-2594a90f9c43","version":"2.1.50","gitBranch":"HEAD","type":"user","message":{"role":"user","content":"<!-- TASK_ID: task-52313d31 -->\n\n# é¡¹ç›®è®¡åˆ’\n\nç›®æ ‡ï¼šå¤ç°è®ºæ–‡å·¥ä½œ https://github.com/vivva/DLinear\n\næ­¥éª¤ï¼š\n  1. é˜…è¯»å’Œç†è§£è®ºæ–‡: è®¿é—® arXiv æˆ–å…¶ä»–è®ºæ–‡å‘å¸ƒå¹³å°ï¼Œæœç´¢æ ‡é¢˜ä¸º 'DLinear: A Simple Yet Effective Baseline for Time Seri\n  2. å®¡æŸ¥å’Œåˆ†ææºä»£ç ä»“åº“: è®¿é—® GitHub ä»“åº“ https://github.com/vivva/DLinearï¼Œä»”ç»†é˜…è¯» README.md æ–‡ä»¶ï¼Œäº†è§£é¡¹ç›®æ¦‚è¿°ã€å®‰è£…æŒ‡å—ã€ä½¿ç”¨æ–¹æ³•\n  3. è®¾ç½®å’ŒéªŒè¯å¼€å‘ç¯å¢ƒ: æ ¹æ®ä»“åº“è¦æ±‚ï¼Œåˆ›å»ºä¸€ä¸ªæ–°çš„ Python è™šæ‹Ÿç¯å¢ƒï¼ˆæ¨èä½¿ç”¨ conda æˆ– venvï¼‰ã€‚æ¿€æ´»è™šæ‹Ÿç¯å¢ƒåï¼Œä½¿ç”¨ pip æˆ– conda å®‰è£…æ‰€æœ‰ä¾èµ–åŒ…ï¼ˆé€šå¸¸é€šè¿‡\n  4. å‡†å¤‡å®éªŒæ•°æ®é›†: æŒ‰ç…§ä»“åº“çš„æŒ‡ç¤ºæˆ–è®ºæ–‡çš„æè¿°ï¼Œä¸‹è½½å®éªŒæ‰€éœ€çš„æ•°æ®é›†ï¼ˆä¾‹å¦‚ ETT, Weather, Electricity, Traffic ç­‰ï¼‰ã€‚ä½¿ç”¨ä»“åº“æä¾›çš„è„šæœ¬ï¼ˆå¦‚æœæœ‰ï¼‰\n  5. è¿è¡Œè®­ç»ƒè„šæœ¬å¹¶è°ƒè¯•: è¿è¡Œä»“åº“ä¸­çš„ä¸»è®­ç»ƒè„šæœ¬ï¼ˆä¾‹å¦‚ main.py æˆ– train.pyï¼‰ï¼Œä½¿ç”¨é»˜è®¤é…ç½®æˆ–è®ºæ–‡ä¸­æŠ¥å‘Šçš„è¶…å‚æ•°ï¼ˆå¦‚å­¦ä¹ ç‡ã€æ‰¹é‡å¤§å°ã€è¿­ä»£æ¬¡æ•°ï¼‰åœ¨å…¶ä¸­ä¸€ä¸ªæ•°æ®é›†ï¼ˆå¦‚ETT\n  6. å…¨é¢å¤ç°å®éªŒ: åœ¨æˆåŠŸè¿è¡Œå•ä¸ªå®éªŒåï¼Œæ‰©å±•è‡³æ‰€æœ‰è®ºæ–‡ä¸­æŠ¥å‘Šçš„æ•°æ®é›†å’Œé¢„æµ‹é•¿åº¦ï¼ˆhorizonï¼‰ã€‚ç¼–å†™æˆ–ä¿®æ”¹è„šæœ¬ä»¥æ‰¹é‡è¿è¡Œå®éªŒï¼Œæˆ–æ‰‹åŠ¨ä¾æ¬¡è¿è¡Œä¸åŒé…ç½®ã€‚å¯¹äºæ¯ä¸ªå®éªŒï¼Œè®°å½•æœ€ç»ˆçš„æµ‹\n  7. å¯¹æ¯”å’Œåˆ†æå¤ç°ç»“æœ: å°†ä¸Šä¸€æ­¥å¾—åˆ°çš„å®éªŒç»“æœä¸è®ºæ–‡ä¸­æŠ¥å‘Šçš„ç»“æœè¿›è¡Œé€é¡¹å¯¹æ¯”ã€‚è®¡ç®—ç›¸å¯¹å·®å¼‚ï¼Œå¹¶åˆ†ææ˜¯å¦å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼ˆä¾‹å¦‚ï¼ŒæŒ‡æ ‡è¯¯å·®åœ¨5%ä»¥å†…é€šå¸¸è®¤ä¸ºå¤ç°æˆåŠŸï¼‰ã€‚å¦‚æœå·®å¼‚è¾ƒå¤§ï¼Œæ£€æŸ¥å¯èƒ½çš„\n  8. å¯è§†åŒ–å’Œç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š: ä½¿ç”¨ matplotlib æˆ–ç±»ä¼¼åº“ç»˜åˆ¶å…³é”®å›¾è¡¨ï¼šè®­ç»ƒæŸå¤±æ›²çº¿ã€æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„é¢„æµ‹å€¼ä¸çœŸå®å€¼å¯¹æ¯”å›¾ï¼ˆé’ˆå¯¹ä¸åŒé¢„æµ‹é•¿åº¦ï¼‰ã€‚æ•´ç†æ•´ä¸ªå¤ç°è¿‡ç¨‹çš„æ­¥éª¤ã€ç¯å¢ƒé…ç½®ã€å®éªŒ\n\n---\n\n# é¡¹ç›®ä¸Šä¸‹æ–‡\n\n# DLinearè®ºæ–‡å¤ç°é¡¹ç›®ä¸Šä¸‹æ–‡\n\n## é¡¹ç›®ç›®æ ‡ä¸æ„¿æ™¯\n- **æ ¸å¿ƒç›®æ ‡**ï¼šå®Œæ•´å¤ç°è®ºæ–‡ã€ŠAre Transformers Effective for Time Series Forecasting?ã€‹ï¼ˆAAAI 2023 Oral, arXiv:2205.13504ï¼‰ä¸­æå‡ºçš„DLinearæ¨¡å‹å®éªŒä¸ç»“æœ\n- **å­¦æœ¯ä»·å€¼**ï¼šéªŒè¯DLinearä½œä¸ºæ—¶é—´åºåˆ—é¢„æµ‹ç®€å•æœ‰æ•ˆåŸºå‡†æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼ŒæŒ‘æˆ˜Transformeråœ¨è¯¥é¢†åŸŸçš„æœ‰æ•ˆæ€§å‡è®¾ï¼Œä¸ºåç»­ç ”ç©¶æä¾›å¯é å‚è€ƒ\n- **å·¥ç¨‹äº¤ä»˜**ï¼šäº§å‡ºå¯å¤ç°çš„ä»£ç åº“ã€å®éªŒè®°å½•ä¸å¯¹æ¯”åˆ†ææŠ¥å‘Š\n\n## æŠ€æœ¯æ ˆä¸æ¶æ„æ¦‚è¿°\n- **æ ¸å¿ƒæ¡†æ¶**ï¼šPyTorch\n- **æ•°æ®å¤„ç†**ï¼špandas, numpy\n- **å®éªŒç®¡ç†**ï¼šè‡ªå®šä¹‰é…ç½®è„šæœ¬ï¼ˆåŸºäºargparseæˆ–hydraï¼‰\n- **è¯„ä¼°æŒ‡æ ‡**ï¼šMSE, MAE\n- **æ¨¡å‹æ¶æ„**ï¼šDLinearï¼ˆåˆ†è§£+çº¿æ€§å±‚ï¼‰\n- **æ•°æ®é›†**ï¼šETTï¼ˆç”µåŠ›å˜å‹å™¨æ¸©åº¦ï¼‰ã€Weatherã€Electricityã€Traffic\n\n## å½“å‰è¿›åº¦æ‘˜è¦\n1. **è®ºæ–‡ç†è§£é˜¶æ®µ** âœ… å·²å®Œæˆè®ºæ–‡çš„å®šä½ã€æ£€ç´¢å’ŒåŸºæœ¬ä¿¡æ¯ç¡®è®¤ï¼ˆæ ‡é¢˜ã€æ ¸å¿ƒè´¡çŒ®ã€æ•°æ®é›†ã€è¯„ä¼°æŒ‡æ ‡ï¼‰\n2. **ä»£ç å®¡æŸ¥é˜¶æ®µ** âœ… å·²åˆ†æåŸå§‹ä»“åº“ç»“æ„ã€ä¾èµ–ä¸è¿è¡Œé€»è¾‘\n3. **ç¯å¢ƒé…ç½®é˜¶æ®µ** âœ… å·²å®Œæˆè™šæ‹Ÿç¯å¢ƒæ­å»ºä¸ä¾èµ–å®‰è£…\n4. **æ•°æ®å‡†å¤‡é˜¶æ®µ** âœ… å·²ä¸‹è½½ETTç­‰å…³é”®æ•°æ®é›†å¹¶å®Œæˆé¢„å¤„ç†\n5. **å•å®éªŒéªŒè¯é˜¶æ®µ** â³ æ­£åœ¨è¿›è¡Œç¬¬ä¸€ä¸ªæ•°æ®é›†ï¼ˆETTï¼‰çš„è®­ç»ƒä¸è°ƒè¯•\n6. **å…¨é¢å¤ç°é˜¶æ®µ** ğŸ“… è®¡åˆ’ä¸­\n7. **ç»“æœå¯¹æ¯”é˜¶æ®µ** ğŸ“… è®¡åˆ’ä¸­\n8. **æŠ¥å‘Šç”Ÿæˆé˜¶æ®µ** ğŸ“… è®¡åˆ’ä¸­\n\n## å…³é”®è®¾è®¡å†³ç­–\n1. **å¿ å®å¤ç°**ï¼šä¸¥æ ¼éµå¾ªè®ºæ–‡è¶…å‚æ•°ä¸å®éªŒè®¾ç½®ï¼Œé¿å…å¼•å…¥æœªç»éªŒè¯çš„ä¼˜åŒ–\n2. **æ¨¡å—åŒ–ä»£ç **ï¼šåˆ†ç¦»æ•°æ®åŠ è½½ã€æ¨¡å‹å®šä¹‰ã€è®­ç»ƒå¾ªç¯ä¸è¯„ä¼°é€»è¾‘ï¼Œä¾¿äºè°ƒè¯•ä¸æ‰©å±•\n3. **å¯å¤ç°æ€§**ï¼šå›ºå®šéšæœºç§å­ï¼Œè®°å½•å®Œæ•´ç¯å¢ƒé…ç½®ï¼ˆPythonç‰ˆæœ¬ã€åº“ç‰ˆæœ¬ï¼‰\n4. **æ¸è¿›éªŒè¯**ï¼šä»å•ä¸ªæ•°æ®é›†/é…ç½®å¼€å§‹ï¼ŒéªŒè¯æˆåŠŸåå†æ‰©å±•è‡³å…¨å®éªŒ\n5. **ç»“æœå¯¹æ¯”åŸºå‡†**ï¼šä»¥è®ºæ–‡æŠ¥å‘Šç»“æœä¸ºæ ‡å‡†ï¼Œæ¥å—Â±5%å†…çš„è¯¯å·®ä¸ºå¤ç°æˆåŠŸ\n\n---\n\n# å½“å‰ä»»åŠ¡ä¸Šä¸‹æ–‡\n\n### ä»»åŠ¡ä¸Šä¸‹æ–‡ï¼šæ­¥éª¤ 1/8 - ç¡®å®šæ•°æ®ç»“æ„å’Œå­˜å‚¨æ–¹æ¡ˆ\n\n**1. å½“å‰æ­¥éª¤åœ¨æ•´ä½“è®¡åˆ’ä¸­çš„ä½ç½®**\n*   è¿™æ˜¯é¡¹ç›®çš„ç¬¬ä¸€æ­¥ï¼Œå±äº**åˆå§‹åŒ–ä¸è®¾è®¡é˜¶æ®µ**ã€‚æœ¬æ­¥éª¤å°†ä¸ºåç»­çš„æ•°æ®é‡‡é›†ã€å¤„ç†ã€æ¨¡å‹éƒ¨ç½²ç­‰æ‰€æœ‰ç¯èŠ‚å¥ å®šæ•°æ®å±‚é¢çš„åŸºç¡€ã€‚\n\n**2. å‰ç½®æ­¥éª¤çš„å®Œæˆæƒ…å†µ**\n*   æ— ã€‚æœ¬é¡¹ç›®ä»æ­¤æ­¥éª¤å¼€å§‹ã€‚\n\n**3. æœ¬æ­¥éª¤çš„å…·ä½“ç›®æ ‡ä¸çº¦æŸ**\n*   **ç›®æ ‡**ï¼šè®¾è®¡å¹¶ç¡®å®šç”¨äºå­˜å‚¨ç³»ç»Ÿç›‘æ§æ•°æ®ï¼ˆå¦‚CPUã€å†…å­˜ã€ç£ç›˜I/Oã€ç½‘ç»œæµé‡ç­‰æ—¶é—´åºåˆ—ï¼‰çš„æ ¸å¿ƒæ•°æ®ç»“æ„å’Œå­˜å‚¨æ–¹æ¡ˆã€‚\n*   **å…³é”®è¦æ±‚**ï¼š\n    *   æ”¯æŒé«˜æ•ˆçš„æ—¶é—´åºåˆ—æ•°æ®å†™å…¥ä¸æŸ¥è¯¢ã€‚\n    *   æ•°æ®ç»“æ„éœ€å…·å¤‡çµæ´»æ€§ï¼Œä¾¿äºæœªæ¥æ·»åŠ æ–°çš„ç›‘æ§æŒ‡æ ‡ã€‚\n    *   æ–¹æ¡ˆéœ€ä¸ºåç»­å¯èƒ½çš„å®æ—¶åˆ†æä¸æ¨¡å‹é¢„æµ‹é¢„ç•™æ¥å£ã€‚\n*   **ä¸»è¦çº¦æŸ**ï¼š\n    *   å¿…é¡»æ»¡è¶³å¿«é€Ÿæ•°æ®å†™å…¥å’Œé«˜å¹¶å‘æŸ¥è¯¢çš„æ€§èƒ½éœ€æ±‚ã€‚\n    *   æ•°æ®ç»“æ„è®¾è®¡åº”é¿å…åœ¨æŒ‡æ ‡æ‰©å±•æ—¶è¿›è¡Œå¤§è§„æ¨¡é‡æ„ã€‚\n\n**4. ä¸å…¶ä»–æ­¥éª¤çš„ä¾èµ–å…³ç³»**\n*   **åç»­æ­¥éª¤å‡ä¾èµ–äºæ­¤**ï¼šæ­¥éª¤2ï¼ˆå®ç°æ•°æ®é‡‡é›†å™¨ï¼‰å°†ä¾æ®æœ¬æ­¥éª¤å®šä¹‰çš„æ ¼å¼å†™å…¥æ•°æ®ï¼›æ­¥éª¤3åŠä¹‹åçš„æŸ¥è¯¢ã€åˆ†æä¸é¢„æµ‹åŠŸèƒ½å°†ç›´æ¥ä¾èµ–æœ¬æ­¥éª¤ç¡®å®šçš„å­˜å‚¨æ–¹æ¡ˆè¿›è¡Œæ•°æ®è¯»å–ã€‚æ­¤æ­¥éª¤çš„å†³ç­–å°†ç›´æ¥å½±å“æ•´ä¸ªç³»ç»Ÿçš„æ€§èƒ½å’Œå¯ç»´æŠ¤æ€§ã€‚\n\n## ä»»åŠ¡æè¿°\n\nè®¿é—® arXiv æˆ–å…¶ä»–è®ºæ–‡å‘å¸ƒå¹³å°ï¼Œæœç´¢æ ‡é¢˜ä¸º 'DLinear: A Simple Yet Effective Baseline for Time Series Forecasting' çš„è®ºæ–‡å¹¶ä¸‹è½½ã€‚ä»”ç»†é˜…è¯»è®ºæ–‡ï¼Œé‡ç‚¹å…³æ³¨æ¨¡å‹æ¶æ„ã€æ•°å­¦åŸç†ã€å®éªŒè®¾ç½®ã€æ•°æ®é›†ã€è¯„ä¼°æŒ‡æ ‡å’Œæ ¸å¿ƒç»“è®ºã€‚åœ¨ç¬”è®°ä¸­è®°å½•å…³é”®ä¿¡æ¯ï¼ŒåŒ…æ‹¬æ¨¡å‹ç»“æ„å›¾ã€å…¬å¼ã€è¶…å‚æ•°å’ŒåŸºå‡†ç»“æœã€‚\n\n## çº¦æŸæ¡ä»¶\n\nç¡®ä¿å¯¹æ—¶é—´åºåˆ—é¢„æµ‹å’Œæ·±åº¦å­¦ä¹ æœ‰åŸºæœ¬ç†è§£ã€‚å¦‚æœé‡åˆ°ä¸ç†Ÿæ‚‰çš„æœ¯è¯­æˆ–æ¦‚å¿µï¼Œéœ€è¦è¿›è¡Œé¢å¤–æŸ¥é˜…ã€‚\n\n## é¢„æœŸè¾“å‡º\n\nä¸€ä»½åŒ…å«è®ºæ–‡æ ¸å¿ƒæ€æƒ³ã€æ¨¡å‹ç»†èŠ‚å’Œå®éªŒè®¾ç½®çš„é˜…è¯»ç¬”è®°ã€‚"},"uuid":"edee83de-a930-4666-b964-f229d5f27aad","timestamp":"2026-02-23T12:49:04.995Z","permissionMode":"bypassPermissions"}
{"parentUuid":"edee83de-a930-4666-b964-f229d5f27aad","isSidechain":false,"userType":"external","cwd":"/app/workspace","sessionId":"f068886d-0e50-4922-9128-2594a90f9c43","version":"2.1.50","gitBranch":"HEAD","message":{"id":"msg_2026022320490563f218c85ece4997","type":"message","role":"assistant","model":"glm-5","content":[{"type":"thinking","thinking":"ç”¨æˆ·è¦æ±‚æˆ‘å®Œæˆä¸€ä¸ªå¤ç°DLinearè®ºæ–‡çš„é¡¹ç›®ã€‚æ ¹æ®ä»»åŠ¡æè¿°ï¼Œå½“å‰éœ€è¦ï¼š\n\n1. è®¿é—®arXivæˆ–å…¶ä»–è®ºæ–‡å‘å¸ƒå¹³å°\n2. æœç´¢æ ‡é¢˜ä¸º 'DLinear: A Simple Yet Effective Baseline for Time Series Forecasting' çš„è®ºæ–‡\n3. ä¸‹è½½å¹¶é˜…è¯»è®ºæ–‡\n4. è®°å½•å…³é”®ä¿¡æ¯\n\nè®©æˆ‘é¦–å…ˆä½¿ç”¨webæœç´¢æ¥æ‰¾åˆ°è¿™ç¯‡è®ºæ–‡ã€‚æ ¹æ®é¡¹ç›®ä¸Šä¸‹æ–‡ä¸­æåˆ°çš„ä¿¡æ¯ï¼Œè¿™ç¯‡è®ºæ–‡çš„å…¨åæ˜¯ã€ŠAre Transformers Effective for Time Series Forecasting?ã€‹ï¼ˆAAAI 2023 Oral, arXiv:2205.13504ï¼‰ï¼ŒDLinearæ˜¯è®ºæ–‡ä¸­æå‡ºçš„æ¨¡å‹ã€‚\n\nè®©æˆ‘å¼€å§‹æœç´¢è¿™ç¯‡è®ºæ–‡ã€‚","signature":""}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"output_tokens":0}},"type":"assistant","uuid":"24d6000d-517a-4f25-bb2c-4fc09222b6da","timestamp":"2026-02-23T12:49:11.982Z"}
{"parentUuid":"24d6000d-517a-4f25-bb2c-4fc09222b6da","isSidechain":false,"userType":"external","cwd":"/app/workspace","sessionId":"f068886d-0e50-4922-9128-2594a90f9c43","version":"2.1.50","gitBranch":"HEAD","message":{"id":"msg_2026022320490563f218c85ece4997","type":"message","role":"assistant","model":"glm-5","content":[{"type":"text","text":"æˆ‘æ¥å¸®ä½ å®ŒæˆDLinearè®ºæ–‡çš„é˜…è¯»å’Œç¬”è®°æ•´ç†ä»»åŠ¡ã€‚è®©æˆ‘å…ˆæœç´¢å¹¶è·å–è¿™ç¯‡è®ºæ–‡ã€‚"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"output_tokens":0}},"type":"assistant","uuid":"756d3e09-1fa0-4723-a2dc-25cbe62ff595","timestamp":"2026-02-23T12:49:17.520Z"}
{"parentUuid":"756d3e09-1fa0-4723-a2dc-25cbe62ff595","isSidechain":false,"userType":"external","cwd":"/app/workspace","sessionId":"f068886d-0e50-4922-9128-2594a90f9c43","version":"2.1.50","gitBranch":"HEAD","message":{"id":"msg_2026022320490563f218c85ece4997","type":"message","role":"assistant","model":"glm-5","content":[{"type":"tool_use","id":"call_6bcafae068794e5cb96d5001","name":"TodoWrite","input":{"todos":[{"content":"æœç´¢å¹¶è®¿é—®DLinearè®ºæ–‡","status":"in_progress","activeForm":"æ­£åœ¨æœç´¢DLinearè®ºæ–‡"},{"content":"é˜…è¯»å¹¶ç†è§£è®ºæ–‡æ ¸å¿ƒå†…å®¹","status":"pending","activeForm":"æ­£åœ¨é˜…è¯»è®ºæ–‡"},{"content":"æ•´ç†æ¨¡å‹æ¶æ„å’Œæ•°å­¦åŸç†","status":"pending","activeForm":"æ­£åœ¨æ•´ç†æ¨¡å‹æ¶æ„"},{"content":"è®°å½•å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡","status":"pending","activeForm":"æ­£åœ¨è®°å½•å®éªŒè®¾ç½®"},{"content":"æ’°å†™å®Œæ•´çš„è®ºæ–‡é˜…è¯»ç¬”è®°","status":"pending","activeForm":"æ­£åœ¨æ’°å†™ç¬”è®°"}]}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"output_tokens":0}},"type":"assistant","uuid":"b68333ab-3389-4734-8fed-f4e5e0a5aa07","timestamp":"2026-02-23T12:49:17.520Z"}
{"parentUuid":"b68333ab-3389-4734-8fed-f4e5e0a5aa07","isSidechain":false,"userType":"external","cwd":"/app/workspace","sessionId":"f068886d-0e50-4922-9128-2594a90f9c43","version":"2.1.50","gitBranch":"HEAD","type":"user","message":{"role":"user","content":[{"tool_use_id":"call_6bcafae068794e5cb96d5001","type":"tool_result","content":"Todos have been modified successfully. Ensure that you continue to use the todo list to track your progress. Please proceed with the current tasks if applicable"}]},"uuid":"364363e5-4005-46fb-9cf9-ba5cc1d10eda","timestamp":"2026-02-23T12:49:17.830Z","toolUseResult":{"oldTodos":[],"newTodos":[{"content":"æœç´¢å¹¶è®¿é—®DLinearè®ºæ–‡","status":"in_progress","activeForm":"æ­£åœ¨æœç´¢DLinearè®ºæ–‡"},{"content":"é˜…è¯»å¹¶ç†è§£è®ºæ–‡æ ¸å¿ƒå†…å®¹","status":"pending","activeForm":"æ­£åœ¨é˜…è¯»è®ºæ–‡"},{"content":"æ•´ç†æ¨¡å‹æ¶æ„å’Œæ•°å­¦åŸç†","status":"pending","activeForm":"æ­£åœ¨æ•´ç†æ¨¡å‹æ¶æ„"},{"content":"è®°å½•å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡","status":"pending","activeForm":"æ­£åœ¨è®°å½•å®éªŒè®¾ç½®"},{"content":"æ’°å†™å®Œæ•´çš„è®ºæ–‡é˜…è¯»ç¬”è®°","status":"pending","activeForm":"æ­£åœ¨æ’°å†™ç¬”è®°"}]},"sourceToolAssistantUUID":"b68333ab-3389-4734-8fed-f4e5e0a5aa07"}
{"parentUuid":"364363e5-4005-46fb-9cf9-ba5cc1d10eda","isSidechain":false,"userType":"external","cwd":"/app/workspace","sessionId":"f068886d-0e50-4922-9128-2594a90f9c43","version":"2.1.50","gitBranch":"HEAD","message":{"id":"msg_202602232049186b4e4d9428d247ef","type":"message","role":"assistant","model":"glm-5","content":[{"type":"thinking","thinking":"ç°åœ¨è®©æˆ‘æœç´¢è¿™ç¯‡è®ºæ–‡ã€‚æ ¹æ®é¡¹ç›®ä¸Šä¸‹æ–‡ï¼Œè¿™ç¯‡è®ºæ–‡çš„arXiv IDæ˜¯2205.13504ã€‚","signature":""}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"output_tokens":0}},"type":"assistant","uuid":"e3f74ccd-5e25-47c0-9be9-b584b379a3d6","timestamp":"2026-02-23T12:49:21.373Z"}
{"parentUuid":"e3f74ccd-5e25-47c0-9be9-b584b379a3d6","isSidechain":false,"userType":"external","cwd":"/app/workspace","sessionId":"f068886d-0e50-4922-9128-2594a90f9c43","version":"2.1.50","gitBranch":"HEAD","message":{"id":"msg_202602232049186b4e4d9428d247ef","type":"message","role":"assistant","model":"glm-5","content":[{"type":"tool_use","id":"call_0dc35026c3134b7dad12fadc","name":"WebSearch","input":{"query":"DLinear \"Are Transformers Effective for Time Series Forecasting\" arXiv 2205.13504"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"output_tokens":0}},"type":"assistant","uuid":"884ebb79-b283-4080-b3e4-7127ae5a4e1f","timestamp":"2026-02-23T12:49:21.373Z"}
{"parentUuid":"884ebb79-b283-4080-b3e4-7127ae5a4e1f","isSidechain":false,"userType":"external","cwd":"/app/workspace","sessionId":"f068886d-0e50-4922-9128-2594a90f9c43","version":"2.1.50","gitBranch":"HEAD","type":"user","message":{"role":"user","content":[{"tool_use_id":"call_0dc35026c3134b7dad12fadc","type":"tool_result","content":"Web search results for query: \"DLinear \"Are Transformers Effective for Time Series Forecasting\" arXiv 2205.13504\"\n\nWeb search error: undefined\n\nWeb search error: undefined\n\nWeb search error: undefined\n\nWeb search error: undefined\n\n## Search Results: DLinear - \"Are Transformers Effective for Time Series Forecasting?\" (arXiv:2205.13504)\n\nBased on my web search, here's a comprehensive summary of this influential paper and the DLinear model:\n\n### ğŸ“„ Paper Information\n\n| Detail | Information |\n|--------|-------------|\n| **Title** | Are Transformers Effective for Time Series Forecasting? |\n| **arXiv ID** | 2205.13504 |\n| **Conference** | AAAI 2023 |\n| **Authors** | Ailing Zeng, Muxi Chen, Lei Zhang, Qiang Xu |\n| **Institutions** | The Chinese University of Hong Kong (CUHK), International Digital Economy Academy (IDEA) |\n\n---\n\n### ğŸ”‘ Key Contributions\n\n#### 1. **Core Question**\nThe paper questions whether Transformer-based models are truly effective for long-term time series forecasting (LTSF). The authors argue that:\n- **Self-attention is permutation-invariant** - it doesn't inherently capture temporal order\n- While positional encoding preserves some ordering information, the inherent permutation-invariance of attention leads to **temporal information loss**\n- Time series data lacks point-to-point semantic correlation (unlike NLP), requiring models to extract temporal relationships between consecutive points\n\n#### 2. **LTSF-Linear Family**\nTo validate their hypothesis, the authors proposed simple linear baseline models:\n\n| Model | Description |\n|-------|-------------|\n| **Vanilla Linear** | Basic linear model using weighted summation for direct prediction |\n| **DLinear** | Combines time series decomposition (trend + seasonal/remainder) with separate linear layers for each component |\n| **NLinear** | Handles distribution shift by subtracting the last value, predicting, then adding it back |\n\n#### 3. **DLinear Architecture**\n\n```\nInput â†’ Moving Average â†’ Trend Component â†’ Linear Layer â†’\n                       â†˜                                 â†± â†’ Sum â†’ Output\n                        Remainder (Seasonal) â†’ Linear Layer â†’\n```\n\n**Key steps:**\n1. **Decomposition**: Uses moving average to separate trend from seasonal/remainder components\n   - Trend: X_trend = MovingAverage(X)\n   - Remainder: X_seasonal = X - X_trend\n\n2. **Independent Linear Mapping**: Two separate single-layer linear networks process each component\n\n3. **Combination**: Results are summed for the final forecast\n\n---\n\n### ğŸ“Š Key Findings\n\n1. **DLinear outperformed complex Transformer models** (Informer, Autoformer, FEDformer, etc.) on 9 major benchmarks including ETTh1, ETTh2, ETTm1, ETTm2, Traffic, and others\n\n2. **Computational Efficiency**: \n   - DLinear has O(L) time complexity\n   - Much smaller and faster than Transformer alternatives\n\n3. **Performance Gains**: Many Transformer models' improvements came from the **direct multi-step (DMS) forecasting strategy** rather than the Transformer architecture itself\n\n---\n\n### ğŸŒŸ Impact & Legacy\n\nThe paper sparked significant discussion in the time series community:\n\n1. **Linear Model Renaissance**: Prompted researchers to reconsider the necessity of complex architectures\n\n2. **Transformer Improvements**: Led to the development of better Transformer variants like:\n   - **PatchTST** (ICLR 2023): Uses patching + channel independence\n   - **iTransformer** (ICLR 2024): Inverted transformer architecture\n   - **TimesNet**\n\n3. **Follow-up Research**: Multiple papers analyzing linear models and their effectiveness\n\n---\n\n### ğŸ”— Resources\n\n- **arXiv Paper**: https://arxiv.org/abs/2205.13504\n- **Code**: Available on GitHub (cure-lab/LTSF-Linear)\n- **Papers with Code**: Listed among top time series forecasting models\n\nThis paper represents a pivotal moment in time series forecasting research, challenging the assumption that increasingly complex neural architectures necessarily lead to better performance on temporal data.\n\n\nREMINDER: You MUST include the sources above in your response to the user using markdown hyperlinks."}]},"uuid":"dc42db3f-af01-4357-b63e-13e54d157e0e","timestamp":"2026-02-23T12:50:23.217Z","toolUseResult":{"query":"DLinear \"Are Transformers Effective for Time Series Forecasting\" arXiv 2205.13504","results":["Web search error: undefined","Web search error: undefined","Web search error: undefined","Web search error: undefined","## Search Results: DLinear - \"Are Transformers Effective for Time Series Forecasting?\" (arXiv:2205.13504)\n\nBased on my web search, here's a comprehensive summary of this influential paper and the DLinear model:\n\n### ğŸ“„ Paper Information\n\n| Detail | Information |\n|--------|-------------|\n| **Title** | Are Transformers Effective for Time Series Forecasting? |\n| **arXiv ID** | 2205.13504 |\n| **Conference** | AAAI 2023 |\n| **Authors** | Ailing Zeng, Muxi Chen, Lei Zhang, Qiang Xu |\n| **Institutions** | The Chinese University of Hong Kong (CUHK), International Digital Economy Academy (IDEA) |\n\n---\n\n### ğŸ”‘ Key Contributions\n\n#### 1. **Core Question**\nThe paper questions whether Transformer-based models are truly effective for long-term time series forecasting (LTSF). The authors argue that:\n- **Self-attention is permutation-invariant** - it doesn't inherently capture temporal order\n- While positional encoding preserves some ordering information, the inherent permutation-invariance of attention leads to **temporal information loss**\n- Time series data lacks point-to-point semantic correlation (unlike NLP), requiring models to extract temporal relationships between consecutive points\n\n#### 2. **LTSF-Linear Family**\nTo validate their hypothesis, the authors proposed simple linear baseline models:\n\n| Model | Description |\n|-------|-------------|\n| **Vanilla Linear** | Basic linear model using weighted summation for direct prediction |\n| **DLinear** | Combines time series decomposition (trend + seasonal/remainder) with separate linear layers for each component |\n| **NLinear** | Handles distribution shift by subtracting the last value, predicting, then adding it back |\n\n#### 3. **DLinear Architecture**\n\n```\nInput â†’ Moving Average â†’ Trend Component â†’ Linear Layer â†’\n                       â†˜                                 â†± â†’ Sum â†’ Output\n                        Remainder (Seasonal) â†’ Linear Layer â†’\n```\n\n**Key steps:**\n1. **Decomposition**: Uses moving average to separate trend from seasonal/remainder components\n   - Trend: X_trend = MovingAverage(X)\n   - Remainder: X_seasonal = X - X_trend\n\n2. **Independent Linear Mapping**: Two separate single-layer linear networks process each component\n\n3. **Combination**: Results are summed for the final forecast\n\n---\n\n### ğŸ“Š Key Findings\n\n1. **DLinear outperformed complex Transformer models** (Informer, Autoformer, FEDformer, etc.) on 9 major benchmarks including ETTh1, ETTh2, ETTm1, ETTm2, Traffic, and others\n\n2. **Computational Efficiency**: \n   - DLinear has O(L) time complexity\n   - Much smaller and faster than Transformer alternatives\n\n3. **Performance Gains**: Many Transformer models' improvements came from the **direct multi-step (DMS) forecasting strategy** rather than the Transformer architecture itself\n\n---\n\n### ğŸŒŸ Impact & Legacy\n\nThe paper sparked significant discussion in the time series community:\n\n1. **Linear Model Renaissance**: Prompted researchers to reconsider the necessity of complex architectures\n\n2. **Transformer Improvements**: Led to the development of better Transformer variants like:\n   - **PatchTST** (ICLR 2023): Uses patching + channel independence\n   - **iTransformer** (ICLR 2024): Inverted transformer architecture\n   - **TimesNet**\n\n3. **Follow-up Research**: Multiple papers analyzing linear models and their effectiveness\n\n---\n\n### ğŸ”— Resources\n\n- **arXiv Paper**: https://arxiv.org/abs/2205.13504\n- **Code**: Available on GitHub (cure-lab/LTSF-Linear)\n- **Papers with Code**: Listed among top time series forecasting models\n\nThis paper represents a pivotal moment in time series forecasting research, challenging the assumption that increasingly complex neural architectures necessarily lead to better performance on temporal data."],"durationSeconds":61.800430586999994},"sourceToolAssistantUUID":"884ebb79-b283-4080-b3e4-7127ae5a4e1f"}
{"parentUuid":"dc42db3f-af01-4357-b63e-13e54d157e0e","isSidechain":false,"userType":"external","cwd":"/app/workspace","sessionId":"f068886d-0e50-4922-9128-2594a90f9c43","version":"2.1.50","gitBranch":"HEAD","message":{"id":"msg_202602232050238884e1c5766d45c6","type":"message","role":"assistant","model":"glm-5","content":[{"type":"thinking","thinking":"æˆ‘å·²ç»æ‰¾åˆ°äº†è®ºæ–‡çš„åŸºæœ¬ä¿¡æ¯ã€‚ç°åœ¨è®©æˆ‘è®¿é—®arXivè·å–æ›´è¯¦ç»†çš„å†…å®¹ï¼ŒåŒ…æ‹¬è®ºæ–‡çš„PDFæˆ–è¯¦ç»†ä¿¡æ¯é¡µé¢ã€‚","signature":""}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"output_tokens":0}},"type":"assistant","uuid":"cca7c8d0-86af-411c-9cf8-38a147a3bb9d","timestamp":"2026-02-23T12:50:25.513Z"}
{"parentUuid":"cca7c8d0-86af-411c-9cf8-38a147a3bb9d","isSidechain":false,"userType":"external","cwd":"/app/workspace","sessionId":"f068886d-0e50-4922-9128-2594a90f9c43","version":"2.1.50","gitBranch":"HEAD","message":{"id":"msg_202602232050238884e1c5766d45c6","type":"message","role":"assistant","model":"glm-5","content":[{"type":"text","text":"è®©æˆ‘è·å–è®ºæ–‡çš„è¯¦ç»†å†…å®¹ã€‚"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"output_tokens":0}},"type":"assistant","uuid":"8f6e9965-5ffe-4f1d-bf50-7dc67e1efae2","timestamp":"2026-02-23T12:50:26.832Z"}
{"parentUuid":"8f6e9965-5ffe-4f1d-bf50-7dc67e1efae2","isSidechain":false,"userType":"external","cwd":"/app/workspace","sessionId":"f068886d-0e50-4922-9128-2594a90f9c43","version":"2.1.50","gitBranch":"HEAD","message":{"id":"msg_202602232050238884e1c5766d45c6","type":"message","role":"assistant","model":"glm-5","content":[{"type":"text","text":"**ğŸŒ Z.ai Built-in Tool: webReader**\n\n**Input:**\n```json\n{\"url\":\"https://arxiv.org/abs/2205.13504\",\"return_format\":\"markdown\"}\n```\n\n*Executing on server...*\n                                            "}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"output_tokens":0}},"type":"assistant","uuid":"04b1f9da-1c4e-4248-beb4-9de61617ec3b","timestamp":"2026-02-23T12:50:26.833Z"}
{"parentUuid":"04b1f9da-1c4e-4248-beb4-9de61617ec3b","isSidechain":false,"userType":"external","cwd":"/app/workspace","sessionId":"f068886d-0e50-4922-9128-2594a90f9c43","version":"2.1.50","gitBranch":"HEAD","message":{"id":"msg_20260223205026c34b2e7439a0491b","type":"message","role":"assistant","model":"glm-5","content":[{"type":"thinking","thinking":"å¾ˆå¥½ï¼Œæˆ‘æ‰¾åˆ°äº†è®ºæ–‡çš„åŸºæœ¬ä¿¡æ¯ã€‚ç°åœ¨è®©æˆ‘è®¿é—®arXivè·å–æ›´è¯¦ç»†çš„è®ºæ–‡å†…å®¹ã€‚","signature":"0e49449b6d274c2ca1ee6817"},{"type":"server_tool_use","id":"call_3407d6f1bf354ee0ae9a904b","name":"webReader","input":{"url":"https://arxiv.org/abs/2205.13504","return_format":"markdown"}},{"type":"tool_result","tool_use_id":"call_3407d6f1bf354ee0ae9a904b","content":"[{'text': {'title': '[2205.13504] Are Transformers Effective for Time Series Forecasting?', 'description': 'Abstract page for arXiv paper 2205.13504: Are Transformers Effective for Time Series Forecasting?', 'url': 'https://arxiv.org/abs/2205.13504', 'content': \"[2205.13504] Are Transformers Effective for Time Series Forecasting?\\n\\nSkip to main content\\n\\n![Image 1: Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)\\n\\nWe gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.\\nDonate\\n\\n![Image 2: arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg) > cs > arXiv:2205.13504\\n\\nHelp | Advanced Search\\n\\nAll fields\\nTitle\\nAuthor\\nAbstract\\nComments\\nJournal reference\\nACM classification\\nMSC classification\\nReport number\\narXiv identifier\\nDOI\\nORCID\\narXiv author ID\\nHelp pages\\nFull text\\n\\nSearch\\n\\n![Image 3: arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)\\n\\n![Image 4: Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)\\n\\nGO\\n\\n## quick links\\n\\n- Login\\n- Help Pages\\n- About\\n\\n# Computer Science > Artificial Intelligence\\n\\n__arXiv:2205.13504__ (cs)\\n\\n[Submitted on 26 May 2022 (v1), last revised 17 Aug 2022 (this version, v3)]\\n\\n# Title:Are Transformers Effective for Time Series Forecasting?\\n\\nAuthors:Ailing Zeng, Muxi Chen, Lei Zhang, Qiang Xu\\n\\nView a PDF of the paper titled Are Transformers Effective for Time Series Forecasting?, by Ailing Zeng and 3 other authors\\n\\nView PDF\\n> Abstract:Recently, there has been a surge of Transformer-based solutions for the long-term time series forecasting (LTSF) task. Despite the growing performance over the past few years, we question the validity of this line of research in this work. Specifically, Transformers is arguably the most successful solution to extract the semantic correlations among the elements in a long sequence. However, in time series modeling, we are to extract the temporal relations in an ordered set of continuous points. While employing positional encoding and using tokens to embed sub-series in Transformers facilitate preserving some ordering information, the nature of the \\\\emph{permutation-invariant} self-attention mechanism inevitably results in temporal information loss. To validate our claim, we introduce a set of embarrassingly simple one-layer linear models named LTSF-Linear for comparison. Experimental results on nine real-life datasets show that LTSF-Linear surprisingly outperforms existing sophisticated Transformer-based LTSF models in all cases, and often by a large margin. Moreover, we conduct comprehensive empirical studies to explore the impacts of various design elements of LTSF models on their temporal relation extraction capability. We hope this surprising finding opens up new research directions for the LTSF task. We also advocate revisiting the validity of Transformer-based solutions for other time series analysis tasks (e.g., anomaly detection) in the future. Code is available at: \\\\url{this https URL}.\\n\\n|  |  |\\n| --- | --- |\\n| Comments: | Code is available at this https URL |\\n| Subjects: | Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |\\n| Cite as: | arXiv:2205.13504 [cs.AI] |\\n|  | (or  arXiv:2205.13504v3 [cs.AI] for this version) |\\n|  | https://doi.org/10.48550/arXiv.2205.13504 Focus to learn more  arXiv-issued DOI via DataCite |\\n\\n## Submission history\\n\\nFrom: Ailing Zeng [view email]   \\n __[v1]__\\nThu, 26 May 2022 17:17:08 UTC (1,440 KB)  \\n__[v2]__\\nWed, 1 Jun 2022 15:43:23 UTC (1,441 KB)  \\n__[v3]__\\nWed, 17 Aug 2022 17:09:38 UTC (1,543 KB)\\n\\nFull-text links:\\n\\n## Access Paper:\\n\\nView a PDF of the paper titled Are Transformers Effective for Time Series Forecasting?, by Ailing Zeng and 3 other authors\\n\\n- View PDF\\n- TeX Source\\n\\n![Image 5: license icon](https://arxiv.org/icons/licenses/by-4.0.png)\\nview license\\n\\nCurrent browse context:\\n\\ncs.AI\\n\\n<\\xa0prev\\n\\xa0 | \\xa0 \\nnext\\xa0>\\n\\nnew\\n | \\nrecent\\n | 2022-05\\n\\nChange to browse by:\\n\\ncs  \\ncs.LG\\n\\n### References & Citations\\n\\n- NASA ADS\\n- Google Scholar\\n- Semantic Scholar\\n\\n### 3 blog links\\n\\n(what is this?)\\n\\nexport BibTeX citation\\nLoading...\\n\\n## BibTeX formatted citation\\n\\nÃ—\\n\\nloading...\\n\\nData provided by:\\n\\n### Bookmark\\n\\n![Image 6: BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)\\n![Image 7: Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)\\n\\nBibliographic Tools\\n\\n# Bibliographic and Citation Tools\\n\\nBibliographic Explorer Toggle\\n\\nBibliographic Explorer _(What is the Explorer?)_\\n\\nConnected Papers Toggle\\n\\nConnected Papers _(What is Connected Papers?)_\\n\\nLitmaps Toggle\\n\\nLitmaps _(What is Litmaps?)_\\n\\nscite.ai Toggle\\n\\nscite Smart Citations _(What are Smart Citations?)_\\n\\nCode, Data, Media\\n\\n# Code, Data and Media Associated with this Article\\n\\nalphaXiv Toggle\\n\\nalphaXiv _(What is alphaXiv?)_\\n\\nLinks to Code Toggle\\n\\nCatalyzeX Code Finder for Papers _(What is CatalyzeX?)_\\n\\nDagsHub Toggle\\n\\nDagsHub _(What is DagsHub?)_\\n\\nGotitPub Toggle\\n\\nGotit.pub _(What is GotitPub?)_\\n\\nHuggingface Toggle\\n\\nHugging Face _(What is Huggingface?)_\\n\\nLinks to Code Toggle\\n\\nPapers with Code _(What is Papers with Code?)_\\n\\nScienceCast Toggle\\n\\nScienceCast _(What is ScienceCast?)_\\n\\nDemos\\n\\n# Demos\\n\\nReplicate Toggle\\n\\nReplicate _(What is Replicate?)_\\n\\nSpaces Toggle\\n\\nHugging Face Spaces _(What is Spaces?)_\\n\\nSpaces Toggle\\n\\nTXYZ.AI _(What is TXYZ.AI?)_\\n\\nRelated Papers\\n\\n# Recommenders and Search Tools\\n\\nLink to Influence Flower\\n\\nInfluence Flower _(What are Influence Flowers?)_\\n\\nCore recommender toggle\\n\\nCORE Recommender _(What is CORE?)_\\n\\n- Author\\n- Venue\\n- Institution\\n- Topic\\n\\nAbout arXivLabs\\n\\n# arXivLabs: experimental projects with community collaborators\\n\\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\\n\\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\\n\\nHave an idea for a project that will add value for arXiv's community? __Learn more about arXivLabs__.\\n\\nWhich authors of this paper are endorsers? |\\nDisable MathJax (What is MathJax?)\\n\\n- About\\n- Help\\n\\n- Contact\\n- Subscribe\\n\\n- Copyright\\n- Privacy Policy\\n\\n- Web Accessibility Assistance\\n- arXiv Operational Status\", 'metadata': {'og:image': '/static/browse/0.3.4/images/arxiv-logo-fb.png', 'citation_title': 'Are Transformers Effective for Time Series Forecasting?', 'og:image:width': '1200', 'theme-color': '#ffffff', 'twitter:card': 'summary', 'og:site_name': 'arXiv.org', 'description': 'Abstract page for arXiv paper 2205.13504: Are Transformers Effective for Time Series Forecasting?', 'citation_date': '2022/05/26', 'og:description': 'Recently, there has been a surge of Transformer-based solutions for the long-term time series forecasting (LTSF) task. Despite the growing performance over the past few years, we question the validity of this line of research in this work. Specifically, Transformers is arguably the most successful solution to extract the semantic correlations among the elements in a long sequence. However, in time series modeling, we are to extract the temporal relations in an ordered set of continuous points. While employing positional encoding and using tokens to embed sub-series in Transformers facilitate preserving some ordering information, the nature of the \\\\emph{permutation-invariant} self-attention mechanism inevitably results in temporal information loss. To validate our claim, we introduce a set of embarrassingly simple one-layer linear models named LTSF-Linear for comparison. Experimental results on nine real-life datasets show that LTSF-Linear surprisingly outperforms existing sophisticated Transformer-based LTSF models in all cases, and often by a large margin. Moreover, we conduct comprehensive empirical studies to explore the impacts of various design elements of LTSF models on their temporal relation extraction capability. We hope this surprising finding opens up new research directions for the LTSF task. We also advocate revisiting the validity of Transformer-based solutions for other time series analysis tasks (e.g., anomaly detection) in the future. Code is available at: \\\\url{https://github.com/cure-lab/LTSF-Linear}.', 'citation_arxiv_id': '2205.13504', 'og:image:secure_url': '/static/browse/0.3.4/images/arxiv-logo-fb.png', 'twitter:image': 'https://static.arxiv.org/icons/twitter/arxiv-logo-twitter-square.png', 'citation_online_date': '2022/08/17', 'twitter:image:alt': 'arXiv logo', 'twitter:site': '@arxiv', 'citation_pdf_url': 'https://arxiv.org/pdf/2205.13504', 'lang': 'en', 'og:image:alt': 'arXiv logo', 'og:type': 'website', 'twitter:title': 'Are Transformers Effective for Time Series Forecasting?', 'citation_abstract': 'Recently, there has been a surge of Transformer-based solutions for the long-term time series forecasting (LTSF) task. Despite the growing performance over the past few years, we question the validity of this line of research in this work. Specifically, Transformers is arguably the most successful solution to extract the semantic correlations among the elements in a long sequence. However, in time series modeling, we are to extract the temporal relations in an ordered set of continuous points. While employing positional encoding and using tokens to embed sub-series in Transformers facilitate preserving some ordering information, the nature of the \\\\emph{permutation-invariant} self-attention mechanism inevitably results in temporal information loss. To validate our claim, we introduce a set of embarrassingly simple one-layer linear models named LTSF-Linear for comparison. Experimental results on nine real-life datasets show that LTSF-Linear surprisingly outperforms existing sophisticated Transformer-based LTSF models in all cases, and often by a large margin. Moreover, we conduct comprehensive empirical studies to explore the impacts of various design elements of LTSF models on their temporal relation extraction capability. We hope this surprising finding opens up new research directions for the LTSF task. We also advocate revisiting the validity of Transformer-based solutions for other time series analysis tasks (e.g., anomaly detection) in the future. Code is available at: \\\\url{https://github.com/cure-lab/LTSF-Linear}.', 'og:title': 'Are Transformers Effective for Time Series Forecasting?', 'citation_author': ['Zeng, Ailing', 'Chen, Muxi', 'Zhang, Lei', 'Xu, Qiang'], 'og:image:height': '700', 'msapplication-TileColor': '#da532c', 'viewport': 'width=device-width, initial-scale=1', 'twitter:description': 'Recently, there has been a surge of Transformer-based solutions for the long-term time series forecasting (LTSF) task. Despite the growing performance over the past few years, we question the...', 'og:url': 'https://arxiv.org/abs/2205.13504v3'}, 'external': {'stylesheet': {'/static/browse/0.3.4/css/arXiv.css?v=20241206': {'media': 'screen', 'type': 'text/css'}, '/static/browse/0.3.4/css/browse_search.css': {'media': 'screen', 'type': 'text/css'}, '/static/browse/0.3.4/css/tooltip.css': {'media': 'screen', 'type': 'text/css'}, '/static/browse/0.3.4/css/arXiv-print.css?v=20200611': {'media': 'print', 'type': 'text/css'}, '/static/base/1.0.1/css/abs.css': {'type': 'text/css'}, 'https://static.arxiv.org/js/bibex-dev/bibex.css?20200709': {'media': 'screen', 'type': 'text/css'}}, 'manifest': {'/static/browse/0.3.4/images/icons/site.webmanifest': {}}, 'icon': {'/static/browse/0.3.4/images/icons/favicon-16x16.png': {'sizes': '16x16', 'type': 'image/png'}, '/static/browse/0.3.4/images/icons/favicon-32x32.png': {'sizes': '32x32', 'type': 'image/png'}}, 'mask-icon': {'/static/browse/0.3.4/images/icons/safari-pinned-tab.svg': {}}, 'canonical': {'https://arxiv.org/abs/2205.13504': {}}, 'apple-touch-icon': {'/static/browse/0.3.4/images/icons/apple-touch-icon.png': {'sizes': '180x180'}}}}, 'type': 'text'}][{\"title\": \"[2205.13504] Are Transformers Effective for Time Series Forecasting?\", \"description\": \"Abstract page for arXiv paper 2205.13504: Are Transformers Effective for Time Series Forecasting?\", \"url\": \"https://arxiv.org/abs/2205.13504\", \"content\": \"[2205.13504] Are Transformers Effective for Time Series Forecasting?\\n\\nSkip to main content\\n\\n![Image 1: Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)\\n\\nWe gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.\\nDonate\\n\\n![Image 2: arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg) > cs > arXiv:2205.13504\\n\\nHelp | Advanced Search\\n\\nAll fields\\nTitle\\nAuthor\\nAbstract\\nComments\\nJournal reference\\nACM classification\\nMSC classification\\nReport number\\narXiv identifier\\nDOI\\nORCID\\narXiv author ID\\nHelp pages\\nFull text\\n\\nSearch\\n\\n![Image 3: arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)\\n\\n![Image 4: Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)\\n\\nGO\\n\\n## quick links\\n\\n- Login\\n- Help Pages\\n- About\\n\\n# Computer Science > Artificial Intelligence\\n\\n__arXiv:2205.13504__ (cs)\\n\\n[Submitted on 26 May 2022 (v1), last revised 17 Aug 2022 (this version, v3)]\\n\\n# Title:Are Transformers Effective for Time Series Forecasting?\\n\\nAuthors:Ailing Zeng, Muxi Chen, Lei Zhang, Qiang Xu\\n\\nView a PDF of the paper titled Are Transformers Effective for Time Series Forecasting?, by Ailing Zeng and 3 other authors\\n\\nView PDF\\n> Abstract:Recently, there has been a surge of Transformer-based solutions for the long-term time series forecasting (LTSF) task. Despite the growing performance over the past few years, we question the validity of this line of research in this work. Specifically, Transformers is arguably the most successful solution to extract the semantic correlations among the elements in a long sequence. However, in time series modeling, we are to extract the temporal relations in an ordered set of continuous points. While employing positional encoding and using tokens to embed sub-series in Transformers facilitate preserving some ordering information, the nature of the \\\\emph{permutation-invariant} self-attention mechanism inevitably results in temporal information loss. To validate our claim, we introduce a set of embarrassingly simple one-layer linear models named LTSF-Linear for comparison. Experimental results on nine real-life datasets show that LTSF-Linear surprisingly outperforms existing sophisticated Transformer-based LTSF models in all cases, and often by a large margin. Moreover, we conduct comprehensive empirical studies to explore the impacts of various design elements of LTSF models on their temporal relation extraction capability. We hope this surprising finding opens up new research directions for the LTSF task. We also advocate revisiting the validity of Transformer-based solutions for other time series analysis tasks (e.g., anomaly detection) in the future. Code is available at: \\\\url{this https URL}.\\n\\n|  |  |\\n| --- | --- |\\n| Comments: | Code is available at this https URL |\\n| Subjects: | Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |\\n| Cite as: | arXiv:2205.13504 [cs.AI] |\\n|  | (or  arXiv:2205.13504v3 [cs.AI] for this version) |\\n|  | https://doi.org/10.48550/arXiv.2205.13504 Focus to learn more  arXiv-issued DOI via DataCite |\\n\\n## Submission history\\n\\nFrom: Ailing Zeng [view email]   \\n __[v1]__\\nThu, 26 May 2022 17:17:08 UTC (1,440 KB)  \\n__[v2]__\\nWed, 1 Jun 2022 15:43:23 UTC (1,441 KB)  \\n__[v3]__\\nWed, 17 Aug 2022 17:09:38 UTC (1,543 KB)\\n\\nFull-text links:\\n\\n## Access Paper:\\n\\nView a PDF of the paper titled Are Transformers Effective for Time Series Forecasting?, by Ailing Zeng and 3 other authors\\n\\n- View PDF\\n- TeX Source\\n\\n![Image 5: license icon](https://arxiv.org/icons/licenses/by-4.0.png)\\nview license\\n\\nCurrent browse context:\\n\\ncs.AI\\n\\n<Â prev\\nÂ  | Â  \\nnextÂ >\\n\\nnew\\n | \\nrecent\\n | 2022-05\\n\\nChange to browse by:\\n\\ncs  \\ncs.LG\\n\\n### References & Citations\\n\\n- NASA ADS\\n- Google Scholar\\n- Semantic Scholar\\n\\n### 3 blog links\\n\\n(what is this?)\\n\\nexport BibTeX citation\\nLoading...\\n\\n## BibTeX formatted citation\\n\\nÃ—\\n\\nloading...\\n\\nData provided by:\\n\\n### Bookmark\\n\\n![Image 6: BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)\\n![Image 7: Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)\\n\\nBibliographic Tools\\n\\n# Bibliographic and Citation Tools\\n\\nBibliographic Explorer Toggle\\n\\nBibliographic Explorer _(What is the Explorer?)_\\n\\nConnected Papers Toggle\\n\\nConnected Papers _(What is Connected Papers?)_\\n\\nLitmaps Toggle\\n\\nLitmaps _(What is Litmaps?)_\\n\\nscite.ai Toggle\\n\\nscite Smart Citations _(What are Smart Citations?)_\\n\\nCode, Data, Media\\n\\n# Code, Data and Media Associated with this Article\\n\\nalphaXiv Toggle\\n\\nalphaXiv _(What is alphaXiv?)_\\n\\nLinks to Code Toggle\\n\\nCatalyzeX Code Finder for Papers _(What is CatalyzeX?)_\\n\\nDagsHub Toggle\\n\\nDagsHub _(What is DagsHub?)_\\n\\nGotitPub Toggle\\n\\nGotit.pub _(What is GotitPub?)_\\n\\nHuggingface Toggle\\n\\nHugging Face _(What is Huggingface?)_\\n\\nLinks to Code Toggle\\n\\nPapers with Code _(What is Papers with Code?)_\\n\\nScienceCast Toggle\\n\\nScienceCast _(What is ScienceCast?)_\\n\\nDemos\\n\\n# Demos\\n\\nReplicate Toggle\\n\\nReplicate _(What is Replicate?)_\\n\\nSpaces Toggle\\n\\nHugging Face Spaces _(What is Spaces?)_\\n\\nSpaces Toggle\\n\\nTXYZ.AI _(What is TXYZ.AI?)_\\n\\nRelated Papers\\n\\n# Recommenders and Search Tools\\n\\nLink to Influence Flower\\n\\nInfluence Flower _(What are Influence Flowers?)_\\n\\nCore recommender toggle\\n\\nCORE Recommender _(What is CORE?)_\\n\\n- Author\\n- Venue\\n- Institution\\n- Topic\\n\\nAbout arXivLabs\\n\\n# arXivLabs: experimental projects with community collaborators\\n\\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\\n\\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\\n\\nHave an idea for a project that will add value for arXiv's community? __Learn more about arXivLabs__.\\n\\nWhich authors of this paper are endorsers? |\\nDisable MathJax (What is MathJax?)\\n\\n- About\\n- Help\\n\\n- Contact\\n- Subscribe\\n\\n- Copyright\\n- Privacy Policy\\n\\n- Web Accessibility Assistance\\n- arXiv Operational Status\", \"metadata\": {\"og:image\": \"/static/browse/0.3.4/images/arxiv-logo-fb.png\", \"citation_title\": \"Are Transformers Effective for Time Series Forecasting?\", \"og:image:width\": \"1200\", \"theme-color\": \"#ffffff\", \"twitter:card\": \"summary\", \"og:site_name\": \"arXiv.org\", \"description\": \"Abstract page for arXiv paper 2205.13504: Are Transformers Effective for Time Series Forecasting?\", \"citation_date\": \"2022/05/26\", \"og:description\": \"Recently, there has been a surge of Transformer-based solutions for the long-term time series forecasting (LTSF) task. Despite the growing performance over the past few years, we question the validity of this line of research in this work. Specifically, Transformers is arguably the most successful solution to extract the semantic correlations among the elements in a long sequence. However, in time series modeling, we are to extract the temporal relations in an ordered set of continuous points. While employing positional encoding and using tokens to embed sub-series in Transformers facilitate preserving some ordering information, the nature of the \\\\emph{permutation-invariant} self-attention mechanism inevitably results in temporal information loss. To validate our claim, we introduce a set of embarrassingly simple one-layer linear models named LTSF-Linear for comparison. Experimental results on nine real-life datasets show that LTSF-Linear surprisingly outperforms existing sophisticated Transformer-based LTSF models in all cases, and often by a large margin. Moreover, we conduct comprehensive empirical studies to explore the impacts of various design elements of LTSF models on their temporal relation extraction capability. We hope this surprising finding opens up new research directions for the LTSF task. We also advocate revisiting the validity of Transformer-based solutions for other time series analysis tasks (e.g., anomaly detection) in the future. Code is available at: \\\\url{https://github.com/cure-lab/LTSF-Linear}.\", \"citation_arxiv_id\": \"2205.13504\", \"og:image:secure_url\": \"/static/browse/0.3.4/images/arxiv-logo-fb.png\", \"twitter:image\": \"https://static.arxiv.org/icons/twitter/arxiv-logo-twitter-square.png\", \"citation_online_date\": \"2022/08/17\", \"twitter:image:alt\": \"arXiv logo\", \"twitter:site\": \"@arxiv\", \"citation_pdf_url\": \"https://arxiv.org/pdf/2205.13504\", \"lang\": \"en\", \"og:image:alt\": \"arXiv logo\", \"og:type\": \"website\", \"twitter:title\": \"Are Transformers Effective for Time Series Forecasting?\", \"citation_abstract\": \"Recently, there has been a surge of Transformer-based solutions for the long-term time series forecasting (LTSF) task. Despite the growing performance over the past few years, we question the validity of this line of research in this work. Specifically, Transformers is arguably the most successful solution to extract the semantic correlations among the elements in a long sequence. However, in time series modeling, we are to extract the temporal relations in an ordered set of continuous points. While employing positional encoding and using tokens to embed sub-series in Transformers facilitate preserving some ordering information, the nature of the \\\\emph{permutation-invariant} self-attention mechanism inevitably results in temporal information loss. To validate our claim, we introduce a set of embarrassingly simple one-layer linear models named LTSF-Linear for comparison. Experimental results on nine real-life datasets show that LTSF-Linear surprisingly outperforms existing sophisticated Transformer-based LTSF models in all cases, and often by a large margin. Moreover, we conduct comprehensive empirical studies to explore the impacts of various design elements of LTSF models on their temporal relation extraction capability. We hope this surprising finding opens up new research directions for the LTSF task. We also advocate revisiting the validity of Transformer-based solutions for other time series analysis tasks (e.g., anomaly detection) in the future. Code is available at: \\\\url{https://github.com/cure-lab/LTSF-Linear}.\", \"og:title\": \"Are Transformers Effective for Time Series Forecasting?\", \"citation_author\": [\"Zeng, Ailing\", \"Chen, Muxi\", \"Zhang, Lei\", \"Xu, Qiang\"], \"og:image:height\": \"700\", \"msapplication-TileColor\": \"#da532c\", \"viewport\": \"width=device-width, initial-scale=1\", \"twitter:description\": \"Recently, there has been a surge of Transformer-based solutions for the long-term time series forecasting (LTSF) task. Despite the growing performance over the past few years, we question the...\", \"og:url\": \"https://arxiv.org/abs/2205.13504v3\"}, \"external\": {\"stylesheet\": {\"/static/browse/0.3.4/css/arXiv.css?v=20241206\": {\"media\": \"screen\", \"type\": \"text/css\"}, \"/static/browse/0.3.4/css/browse_search.css\": {\"media\": \"screen\", \"type\": \"text/css\"}, \"/static/browse/0.3.4/css/tooltip.css\": {\"media\": \"screen\", \"type\": \"text/css\"}, \"/static/browse/0.3.4/css/arXiv-print.css?v=20200611\": {\"media\": \"print\", \"type\": \"text/css\"}, \"/static/base/1.0.1/css/abs.css\": {\"type\": \"text/css\"}, \"https://static.arxiv.org/js/bibex-dev/bibex.css?20200709\": {\"media\": \"screen\", \"type\": \"text/css\"}}, \"manifest\": {\"/static/browse/0.3.4/images/icons/site.webmanifest\": {}}, \"icon\": {\"/static/browse/0.3.4/images/icons/favicon-16x16.png\": {\"sizes\": \"16x16\", \"type\": \"image/png\"}, \"/static/browse/0.3.4/images/icons/favicon-32x32.png\": {\"sizes\": \"32x32\", \"type\": \"image/png\"}}, \"mask-icon\": {\"/static/browse/0.3.4/images/icons/safari-pinned-tab.svg\": {}}, \"canonical\": {\"https://arxiv.org/abs/2205.13504\": {}}, \"apple-touch-icon\": {\"/static/browse/0.3.4/images/icons/apple-touch-icon.png\": {\"sizes\": \"180x180\"}}}}]"},{"type":"thinking","thinking":"ç°åœ¨æˆ‘è·å–äº†è®ºæ–‡çš„æ‘˜è¦é¡µé¢ä¿¡æ¯ã€‚è®©æˆ‘ç»§ç»­è·å–è®ºæ–‡çš„PDFå†…å®¹æˆ–æ›´è¯¦ç»†çš„ä¿¡æ¯ã€‚æˆ‘è¿˜éœ€è¦è®¿é—®GitHubä»“åº“æ¥äº†è§£ä»£ç å®ç°ç»†èŠ‚ã€‚","signature":"6af1fe6a3dbc48daae1e5bec"},{"type":"text","text":"è®©æˆ‘è·å–è®ºæ–‡PDFçš„æ›´å¤šè¯¦ç»†å†…å®¹ï¼Œä»¥åŠè®¿é—®GitHubä»“åº“äº†è§£ä»£ç å®ç°ã€‚"},{"type":"server_tool_use","id":"call_3e1cc4c14af741c0b101e3bc","name":"webReader","input":{"url":"https://arxiv.org/pdf/2205.13504","return_format":"markdown"}},{"type":"server_tool_use","id":"call_d5d53186c04848e4b0d46949","name":"webReader","input":{"url":"https://github.com/cure-lab/LTSF-Linear","return_format":"markdown"}},{"type":"tool_result","tool_use_id":"call_d5d53186c04848e4b0d46949","content":"[{'text': {'title': 'GitHub - cure-lab/LTSF-Linear: [AAAI-23 Oral] Official implementation of the paper \"Are Transformers Effective for Time Series Forecasting?\"', 'description': '[AAAI-23 Oral] Official implementation of the paper \"Are Transformers Effective for Time Series Forecasting?\" - cure-lab/LTSF-Linear', 'url': 'https://github.com/cure-lab/LTSF-Linear', 'content': '## Are Transformers Effective for Time Series Forecasting? (AAAI 2023)\\n\\nThis repo is the official Pytorch implementation of LTSF-Linear: \"Are Transformers Effective for Time Series Forecasting?\".\\n\\n## Updates\\n\\n- [2024/01/28] Our model has been included in NeuralForecast. Special thanks to the contributor @cchallu!\\n- [2022/11/23] Accepted to AAAI 2023 with three strong accept! We also release a __benchmark for long-term time series forecasting__ for further research.\\n- [2022/08/25] We update our paper with comprehensive analyses on why existing LTSF-Transformers do not work well on the LTSF problem!\\n- [2022/08/25] Besides DLinear, we\\'re excited to add two Linear models to the paper and this repo. Now we have an LTSF-Linear family!\\n\\n  - Linear: Just one linear layer.\\n  - DLinear: Decomposition Linear to handle data with trend and seasonality patterns.\\n  - NLinear: A Normalized Linear to deal with train-test set distribution shifts. See section \\'LTSF-Linear\\' for more details.\\n- [2022/08/25] We update some scripts of LTSF-Linear.\\n\\n  - Linear, NLinear, and DLinear use the same scripts.\\n  - Some results of DLinear are slightly different now.\\n\\n## Features\\n\\n- Add a benchmark for long-term time series forecasting.\\n- Support both Univariate and Multivariate long-term time series forecasting.\\n- Support visualization of weights.\\n- Support scripts on different look-back window size.\\n\\nBesides LTSF-Linear, we provide five significant forecasting Transformers to re-implement the results in the paper.\\n\\n- Transformer (NeuIPS 2017)\\n- Informer (AAAI 2021 Best paper)\\n- Autoformer (NeuIPS 2021)\\n- Pyraformer (ICLR 2022 Oral)\\n- FEDformer (ICML 2022)\\n\\n## Detailed Description\\n\\nWe provide all experiment script files in `./scripts`:\\n\\n| Files | Interpretation |\\n| --- | --- |\\n| EXP-LongForecasting | Long-term Time Series Forecasting Task |\\n| EXP-LookBackWindow | Study the impact of different look-back window sizes |\\n| EXP-Embedding | Study the effects of different embedding strategies |\\n\\nThis code is simply built on the code base of Autoformer. We appreciate the following GitHub repos a lot for their valuable code base or datasets:\\n\\nThe implementation of Autoformer, Informer, and Transformer is from https://github.com/thuml/Autoformer\\n\\nThe implementation of FEDformer is from https://github.com/MAZiqing/FEDformer\\n\\nThe implementation of Pyraformer is from https://github.com/alipay/Pyraformer\\n\\n## LTSF-Linear\\n\\n### LTSF-Linear family\\n\\n![Image 1: image](https://github.com/cure-lab/LTSF-Linear/raw/main/pics/Linear.png)\\nLTSF-Linear is a set of linear models.\\n\\n- Linear: It is just a one-layer linear model, but it outperforms Transformers.\\n- NLinear: __To boost the performance of Linear when there is a distribution shift in the dataset__, NLinear first subtracts the input by the last value of the sequence. Then, the input goes through a linear layer, and the subtracted part is added back before making the final prediction. The subtraction and addition in NLinear are a simple normalization for the input sequence.\\n- DLinear: It is a combination of a Decomposition scheme used in Autoformer and FEDformer with linear layers. It first decomposes a raw data input into a trend component by a moving average kernel and a remainder (seasonal) component. Then, two one-layer linear layers are applied to each component and we sum up the two features to get the final prediction. By explicitly handling trend, __DLinear enhances the performance of a vanilla linear when there is a clear trend in the data.__\\n\\nAlthough LTSF-Linear is simple, it has some compelling characteristics:\\n\\n- An O(1) maximum signal traversing path length: The shorter the path, the better the dependencies are captured, making LTSF-Linear capable of capturing both short-range and long-range temporal relations.\\n- High-efficiency: As each branch has only one linear layer, it costs much lower memory and fewer parameters and has a faster inference speed than existing Transformers.\\n- Interpretability: After training, we can visualize weights to have some insights into the predicted values.\\n- Easy-to-use: LTSF-Linear can be obtained easily without tuning model hyper-parameters.\\n\\n### Comparison with Transformers\\n\\nUnivariate Forecasting:\\n![Image 2: image](https://github.com/cure-lab/LTSF-Linear/raw/main/pics/Uni-results.png)\\nMultivariate Forecasting:\\n![Image 3: image](https://github.com/cure-lab/LTSF-Linear/raw/main/pics/Mul-results.png)\\nLTSF-Linear outperforms all transformer-based methods by a large margin.\\n\\n### Efficiency\\n\\n![Image 4: image](https://github.com/cure-lab/LTSF-Linear/raw/main/pics/efficiency.png)\\nComparison of method efficiency with Look-back window size 96 and Forecasting steps 720 on Electricity. MACs are the number of multiply-accumulate operations. We use DLinear for comparison since it has the double cost in LTSF-Linear. The inference time averages 5 runs.\\n\\n## Getting Started\\n\\n### Environment Requirements\\n\\nFirst, please make sure you have installed Conda. Then, our environment can be installed by:\\n\\n```\\nconda create -n LTSF_Linear python=3.6.9\\nconda activate LTSF_Linear\\npip install -r requirements.txt\\n```\\n\\n### Data Preparation\\n\\nYou can obtain all the nine benchmarks from Google Drive provided in Autoformer. All the datasets are well pre-processed and can be used easily.\\n\\n__Please put them in the `./dataset` directory__\\n\\n### Training Example\\n\\n- In `scripts/` , we provide the model implementation _Dlinear/Autoformer/Informer/Transformer_\\n- In `FEDformer/scripts/`, we provide the _FEDformer_ implementation\\n- In `Pyraformer/scripts/`, we provide the _Pyraformer_ implementation\\n\\nFor example:\\n\\nTo train the __LTSF-Linear__ on __Exchange-Rate dataset__, you can use the script `scripts/EXP-LongForecasting/Linear/exchange_rate.sh`:\\n\\n```\\nsh scripts/EXP-LongForecasting/Linear/exchange_rate.sh\\n```\\n\\nIt will start to train DLinear by default, the results will be shown in `logs/LongForecasting`. You can specify the name of the model in the script. (Linear, DLinear, NLinear)\\n\\nAll scripts about using LTSF-Linear on long forecasting task is in `scripts/EXP-LongForecasting/Linear/`, you can run them in a similar way. The default look-back window in scripts is 336, LTSF-Linear generally achieves better results with longer look-back window as dicussed in the paper.\\n\\nScripts about look-back window size and long forecasting of FEDformer and Pyraformer are in `FEDformer/scripts` and `Pyraformer/scripts`, respectively. To run them, you need to first `cd FEDformer` or `cd Pyraformer`. Then, you can use sh to run them in a similar way. Logs will be stored in `logs/`.\\n\\nEach experiment in `scripts/EXP-LongForecasting/Linear/` takes 5min-20min. For other Transformer scripts, since we put all related experiments in one script file, directly running them will take 8 hours per day. You can keep the experiments you are interested in and comment on the others.\\n\\n### Weights Visualization\\n\\nAs shown in our paper, the weights of LTSF-Linear can reveal some characteristics of the data, i.e., the periodicity. As an example, we provide the weight visualization of DLinear in `weight_plot.py`. To run the visualization, you need to input the model path (model_name) of DLinear (the model directory in `./checkpoint` by default). To obtain smooth and clear patterns, you can use the initialization we provided in the file of linear models.\\n\\n![Image 5: image](https://github.com/cure-lab/LTSF-Linear/raw/main/pics/Visualization_DLinear.png)\\n\\n## Citing\\n\\nIf you find this repository useful for your work, please consider citing it as follows:\\n\\n```\\n@inproceedings{Zeng2022AreTE,\\n  title={Are Transformers Effective for Time Series Forecasting?},\\n  author={Ailing Zeng and Muxi Chen and Lei Zhang and Qiang Xu},\\n  journal={Proceedings of the AAAI Conference on Artificial Intelligence},\\n  year={2023}\\n}\\n```\\n\\nPlease remember to cite all the datasets and compared methods if you use them in your experiments.', 'metadata': {'og:image': 'https://opengraph.githubassets.com/c1cee1fda9bc86180561e25df037c4346c1abdbd4690b68cc55c9c822bf06b54/cure-lab/LTSF-Linear', 'og:image:width': '1200', 'twitter:card': 'summary_large_image', 'og:site_name': 'GitHub', 'release': '3d444f0a47beeeac94cddbb51c91ab408befe8d4', 'html-safe-nonce': '25f15a645c6fef15752b5aceee50698a8c0baa5b5b820eeeac137e444341d74e', 'expected-hostname': 'github.com', 'og:description': '[AAAI-23 Oral] Official implementation of the paper \"Are Transformers Effective for Time Series Forecasting?\" - cure-lab/LTSF-Linear', 'browser-errors-url': 'https://api.github.com/_private/browser/errors', 'octolytics-dimension-user_login': 'cure-lab', 'hostname': 'github.com', 'browser-stats-url': 'https://api.github.com/_private/browser/stats', 'route-pattern': '/:user_id/:repository', 'octolytics-dimension-repository_id': '496687161', 'octolytics-dimension-repository_network_root_nwo': 'cure-lab/LTSF-Linear', 'og:image:alt': '[AAAI-23 Oral] Official implementation of the paper \"Are Transformers Effective for Time Series Forecasting?\" - cure-lab/LTSF-Linear', 'google-site-verification': 'Apib7-x98H0j5cPqHWwSMm6dNU4GmODRoqxLiDzdx9I', 'visitor-hmac': 'af8d0207a5cc0907be8b5e925a31118d95ac0540308a88ad35809bdc8244d411', 'turbo-cache-control': 'no-preview', 'request-id': '2E03:1574:48B6E17:3B5BFD6:698BE875', 'octolytics-dimension-repository_is_fork': 'false', 'go-import': 'github.com/cure-lab/LTSF-Linear git https://github.com/cure-lab/LTSF-Linear.git', 'octolytics-dimension-user_id': '89723741', 'octolytics-dimension-repository_network_root_id': '496687161', 'route-controller': 'files', 'msapplication-TileImage': '/windows-tile.png', 'octolytics-url': 'https://collector.github.com/github/collect', 'apple-itunes-app': 'app-id=1477376905, app-argument=https://github.com/cure-lab/LTSF-Linear', 'theme-color': '#1e2327', 'hovercard-subject-tag': 'repository:496687161', 'turbo-body-classes': 'logged-out env-production page-responsive', 'description': '[AAAI-23 Oral] Official implementation of the paper \"Are Transformers Effective for Time Series Forecasting?\" - cure-lab/LTSF-Linear', 'twitter:image': 'https://opengraph.githubassets.com/c1cee1fda9bc86180561e25df037c4346c1abdbd4690b68cc55c9c822bf06b54/cure-lab/LTSF-Linear', 'twitter:site': '@github', 'visitor-payload': 'eyJyZWZlcnJlciI6IiIsInJlcXVlc3RfaWQiOiIyRTAzOjE1NzQ6NDhCNkUxNzozQjVCRkQ2OjY5OEJFODc1IiwidmlzaXRvcl9pZCI6Ijg0MTE5MzA5NDk3OTMzNDM2MDUiLCJyZWdpb25fZWRnZSI6ImZyYSIsInJlZ2lvbl9yZW5kZXIiOiJmcmEifQ==', 'github-keyboard-shortcuts': 'repository,copilot', 'lang': 'en', 'fetch-nonce': 'v2:3d5a8841-37b4-7f64-8dee-a46467f33b70', 'og:type': 'object', 'twitter:title': 'GitHub - cure-lab/LTSF-Linear: [AAAI-23 Oral] Official implementation of the paper \"Are Transformers Effective for Time Series Forecasting?\"', 'og:title': 'GitHub - cure-lab/LTSF-Linear: [AAAI-23 Oral] Official implementation of the paper \"Are Transformers Effective for Time Series Forecasting?\"', 'og:image:height': '600', 'analytics-location': '/<user-name>/<repo-name>', 'color-scheme': 'light dark', 'route-action': 'disambiguate', 'msapplication-TileColor': '#ffffff', 'octolytics-dimension-repository_public': 'true', 'fb:app_id': '1401488693436528', 'ui-target': 'full', 'octolytics-dimension-repository_nwo': 'cure-lab/LTSF-Linear', 'viewport': 'width=device-width', 'twitter:description': '[AAAI-23 Oral] Official implementation of the paper \"Are Transformers Effective for Time Series Forecasting?\" - cure-lab/LTSF-Linear', 'current-catalog-service-hash': 'f3abb0cc802f3d7b95fc8762b94bdcb13bf39634c40c357301c4aa1d67a256fb', 'disable-turbo': 'false', 'og:url': 'https://github.com/cure-lab/LTSF-Linear'}, 'external': {'stylesheet': {'https://github.githubassets.com/assets/light-7a548c73f231168f.css': {'media': 'all'}, 'https://github.githubassets.com/assets/primer-primitives-2c5f58bae956fa96.css': {'media': 'all'}, 'https://github.githubassets.com/assets/primer-react.7bdab33cf4d9c38d.module.css': {'media': 'all'}, 'https://github.githubassets.com/assets/keyboard-shortcuts-dialog.124a139fc586638e.module.css': {'media': 'all'}, 'https://github.githubassets.com/assets/primer-b97936554294d7a3.css': {'media': 'all'}, 'https://github.githubassets.com/assets/global-5d300701a37a830a.css': {'media': 'all'}, 'https://github.githubassets.com/assets/dark-a037c5198a3956d3.css': {'media': 'all'}, 'https://github.githubassets.com/assets/dark_high_contrast-c4cd957551b6a79f.css': {'media': 'all'}, 'https://github.githubassets.com/assets/light_high_contrast-6d5205ba83e64657.css': {'media': 'all'}, 'https://github.githubassets.com/assets/repository-c2a686ef3f37f3e5.css': {'media': 'all'}, 'https://github.githubassets.com/assets/code-67953c77b47fec3b.css': {'media': 'all'}, 'https://github.githubassets.com/assets/github-294399c8d5b652c5.css': {'media': 'all'}, 'https://github.githubassets.com/assets/94077.2a3ef11f8e0edae3.module.css': {'media': 'all'}, 'https://github.githubassets.com/assets/notifications-subscriptions-menu.72e14da340cf5788.module.css': {'media': 'all'}, 'https://github.githubassets.com/assets/marketing-navigation.6981f452ada322e1.module.css': {'media': 'all'}, 'https://github.githubassets.com/assets/appearance-settings.51d12175097b565e.module.css': {'media': 'all'}}, 'preconnect': {'https://github.githubassets.com': {}, 'https://avatars.githubusercontent.com': {}}, 'search': {'/opensearch.xml': {'title': 'GitHub', 'type': 'application/opensearchdescription+xml'}}, 'assets': {'https://github.githubassets.com/': {}}, 'dns-prefetch': {'https://user-images.githubusercontent.com/': {}, 'https://github-cloud.s3.amazonaws.com': {}, 'https://github.githubassets.com': {}, 'https://avatars.githubusercontent.com': {}}, 'fluid-icon': {'https://github.com/fluidicon.png': {'title': 'GitHub'}}, 'manifest': {'/manifest.json': {}}, 'icon': {'https://github.githubassets.com/favicons/favicon.svg': {'type': 'image/svg+xml'}}, 'alternate': {'https://github.githubassets.com/favicons/favicon.png': {'type': 'image/png'}}, 'mask-icon': {'https://github.githubassets.com/assets/pinned-octocat-093da3e6fa40.svg': {}}, 'canonical': {'https://github.com/cure-lab/LTSF-Linear': {}}}}, 'type': 'text'}][{\"title\": \"GitHub - cure-lab/LTSF-Linear: [AAAI-23 Oral] Official implementation of the paper \\\"Are Transformers Effective for Time Series Forecasting?\\\"\", \"description\": \"[AAAI-23 Oral] Official implementation of the paper \\\"Are Transformers Effective for Time Series Forecasting?\\\" - cure-lab/LTSF-Linear\", \"url\": \"https://github.com/cure-lab/LTSF-Linear\", \"content\": \"## Are Transformers Effective for Time Series Forecasting? (AAAI 2023)\\n\\nThis repo is the official Pytorch implementation of LTSF-Linear: \\\"Are Transformers Effective for Time Series Forecasting?\\\".\\n\\n## Updates\\n\\n- [2024/01/28] Our model has been included in NeuralForecast. Special thanks to the contributor @cchallu!\\n- [2022/11/23] Accepted to AAAI 2023 with three strong accept! We also release a __benchmark for long-term time series forecasting__ for further research.\\n- [2022/08/25] We update our paper with comprehensive analyses on why existing LTSF-Transformers do not work well on the LTSF problem!\\n- [2022/08/25] Besides DLinear, we're excited to add two Linear models to the paper and this repo. Now we have an LTSF-Linear family!\\n\\n  - Linear: Just one linear layer.\\n  - DLinear: Decomposition Linear to handle data with trend and seasonality patterns.\\n  - NLinear: A Normalized Linear to deal with train-test set distribution shifts. See section 'LTSF-Linear' for more details.\\n- [2022/08/25] We update some scripts of LTSF-Linear.\\n\\n  - Linear, NLinear, and DLinear use the same scripts.\\n  - Some results of DLinear are slightly different now.\\n\\n## Features\\n\\n- Add a benchmark for long-term time series forecasting.\\n- Support both Univariate and Multivariate long-term time series forecasting.\\n- Support visualization of weights.\\n- Support scripts on different look-back window size.\\n\\nBesides LTSF-Linear, we provide five significant forecasting Transformers to re-implement the results in the paper.\\n\\n- Transformer (NeuIPS 2017)\\n- Informer (AAAI 2021 Best paper)\\n- Autoformer (NeuIPS 2021)\\n- Pyraformer (ICLR 2022 Oral)\\n- FEDformer (ICML 2022)\\n\\n## Detailed Description\\n\\nWe provide all experiment script files in `./scripts`:\\n\\n| Files | Interpretation |\\n| --- | --- |\\n| EXP-LongForecasting | Long-term Time Series Forecasting Task |\\n| EXP-LookBackWindow | Study the impact of different look-back window sizes |\\n| EXP-Embedding | Study the effects of different embedding strategies |\\n\\nThis code is simply built on the code base of Autoformer. We appreciate the following GitHub repos a lot for their valuable code base or datasets:\\n\\nThe implementation of Autoformer, Informer, and Transformer is from https://github.com/thuml/Autoformer\\n\\nThe implementation of FEDformer is from https://github.com/MAZiqing/FEDformer\\n\\nThe implementation of Pyraformer is from https://github.com/alipay/Pyraformer\\n\\n## LTSF-Linear\\n\\n### LTSF-Linear family\\n\\n![Image 1: image](https://github.com/cure-lab/LTSF-Linear/raw/main/pics/Linear.png)\\nLTSF-Linear is a set of linear models.\\n\\n- Linear: It is just a one-layer linear model, but it outperforms Transformers.\\n- NLinear: __To boost the performance of Linear when there is a distribution shift in the dataset__, NLinear first subtracts the input by the last value of the sequence. Then, the input goes through a linear layer, and the subtracted part is added back before making the final prediction. The subtraction and addition in NLinear are a simple normalization for the input sequence.\\n- DLinear: It is a combination of a Decomposition scheme used in Autoformer and FEDformer with linear layers. It first decomposes a raw data input into a trend component by a moving average kernel and a remainder (seasonal) component. Then, two one-layer linear layers are applied to each component and we sum up the two features to get the final prediction. By explicitly handling trend, __DLinear enhances the performance of a vanilla linear when there is a clear trend in the data.__\\n\\nAlthough LTSF-Linear is simple, it has some compelling characteristics:\\n\\n- An O(1) maximum signal traversing path length: The shorter the path, the better the dependencies are captured, making LTSF-Linear capable of capturing both short-range and long-range temporal relations.\\n- High-efficiency: As each branch has only one linear layer, it costs much lower memory and fewer parameters and has a faster inference speed than existing Transformers.\\n- Interpretability: After training, we can visualize weights to have some insights into the predicted values.\\n- Easy-to-use: LTSF-Linear can be obtained easily without tuning model hyper-parameters.\\n\\n### Comparison with Transformers\\n\\nUnivariate Forecasting:\\n![Image 2: image](https://github.com/cure-lab/LTSF-Linear/raw/main/pics/Uni-results.png)\\nMultivariate Forecasting:\\n![Image 3: image](https://github.com/cure-lab/LTSF-Linear/raw/main/pics/Mul-results.png)\\nLTSF-Linear outperforms all transformer-based methods by a large margin.\\n\\n### Efficiency\\n\\n![Image 4: image](https://github.com/cure-lab/LTSF-Linear/raw/main/pics/efficiency.png)\\nComparison of method efficiency with Look-back window size 96 and Forecasting steps 720 on Electricity. MACs are the number of multiply-accumulate operations. We use DLinear for comparison since it has the double cost in LTSF-Linear. The inference time averages 5 runs.\\n\\n## Getting Started\\n\\n### Environment Requirements\\n\\nFirst, please make sure you have installed Conda. Then, our environment can be installed by:\\n\\n```\\nconda create -n LTSF_Linear python=3.6.9\\nconda activate LTSF_Linear\\npip install -r requirements.txt\\n```\\n\\n### Data Preparation\\n\\nYou can obtain all the nine benchmarks from Google Drive provided in Autoformer. All the datasets are well pre-processed and can be used easily.\\n\\n__Please put them in the `./dataset` directory__\\n\\n### Training Example\\n\\n- In `scripts/` , we provide the model implementation _Dlinear/Autoformer/Informer/Transformer_\\n- In `FEDformer/scripts/`, we provide the _FEDformer_ implementation\\n- In `Pyraformer/scripts/`, we provide the _Pyraformer_ implementation\\n\\nFor example:\\n\\nTo train the __LTSF-Linear__ on __Exchange-Rate dataset__, you can use the script `scripts/EXP-LongForecasting/Linear/exchange_rate.sh`:\\n\\n```\\nsh scripts/EXP-LongForecasting/Linear/exchange_rate.sh\\n```\\n\\nIt will start to train DLinear by default, the results will be shown in `logs/LongForecasting`. You can specify the name of the model in the script. (Linear, DLinear, NLinear)\\n\\nAll scripts about using LTSF-Linear on long forecasting task is in `scripts/EXP-LongForecasting/Linear/`, you can run them in a similar way. The default look-back window in scripts is 336, LTSF-Linear generally achieves better results with longer look-back window as dicussed in the paper.\\n\\nScripts about look-back window size and long forecasting of FEDformer and Pyraformer are in `FEDformer/scripts` and `Pyraformer/scripts`, respectively. To run them, you need to first `cd FEDformer` or `cd Pyraformer`. Then, you can use sh to run them in a similar way. Logs will be stored in `logs/`.\\n\\nEach experiment in `scripts/EXP-LongForecasting/Linear/` takes 5min-20min. For other Transformer scripts, since we put all related experiments in one script file, directly running them will take 8 hours per day. You can keep the experiments you are interested in and comment on the others.\\n\\n### Weights Visualization\\n\\nAs shown in our paper, the weights of LTSF-Linear can reveal some characteristics of the data, i.e., the periodicity. As an example, we provide the weight visualization of DLinear in `weight_plot.py`. To run the visualization, you need to input the model path (model_name) of DLinear (the model directory in `./checkpoint` by default). To obtain smooth and clear patterns, you can use the initialization we provided in the file of linear models.\\n\\n![Image 5: image](https://github.com/cure-lab/LTSF-Linear/raw/main/pics/Visualization_DLinear.png)\\n\\n## Citing\\n\\nIf you find this repository useful for your work, please consider citing it as follows:\\n\\n```\\n@inproceedings{Zeng2022AreTE,\\n  title={Are Transformers Effective for Time Series Forecasting?},\\n  author={Ailing Zeng and Muxi Chen and Lei Zhang and Qiang Xu},\\n  journal={Proceedings of the AAAI Conference on Artificial Intelligence},\\n  year={2023}\\n}\\n```\\n\\nPlease remember to cite all the datasets and compared methods if you use them in your experiments.\", \"metadata\": {\"og:image\": \"https://opengraph.githubassets.com/c1cee1fda9bc86180561e25df037c4346c1abdbd4690b68cc55c9c822bf06b54/cure-lab/LTSF-Linear\", \"og:image:width\": \"1200\", \"twitter:card\": \"summary_large_image\", \"og:site_name\": \"GitHub\", \"release\": \"3d444f0a47beeeac94cddbb51c91ab408befe8d4\", \"html-safe-nonce\": \"25f15a645c6fef15752b5aceee50698a8c0baa5b5b820eeeac137e444341d74e\", \"expected-hostname\": \"github.com\", \"og:description\": \"[AAAI-23 Oral] Official implementation of the paper \\\"Are Transformers Effective for Time Series Forecasting?\\\" - cure-lab/LTSF-Linear\", \"browser-errors-url\": \"https://api.github.com/_private/browser/errors\", \"octolytics-dimension-user_login\": \"cure-lab\", \"hostname\": \"github.com\", \"browser-stats-url\": \"https://api.github.com/_private/browser/stats\", \"route-pattern\": \"/:user_id/:repository\", \"octolytics-dimension-repository_id\": \"496687161\", \"octolytics-dimension-repository_network_root_nwo\": \"cure-lab/LTSF-Linear\", \"og:image:alt\": \"[AAAI-23 Oral] Official implementation of the paper \\\"Are Transformers Effective for Time Series Forecasting?\\\" - cure-lab/LTSF-Linear\", \"google-site-verification\": \"Apib7-x98H0j5cPqHWwSMm6dNU4GmODRoqxLiDzdx9I\", \"visitor-hmac\": \"af8d0207a5cc0907be8b5e925a31118d95ac0540308a88ad35809bdc8244d411\", \"turbo-cache-control\": \"no-preview\", \"request-id\": \"2E03:1574:48B6E17:3B5BFD6:698BE875\", \"octolytics-dimension-repository_is_fork\": \"false\", \"go-import\": \"github.com/cure-lab/LTSF-Linear git https://github.com/cure-lab/LTSF-Linear.git\", \"octolytics-dimension-user_id\": \"89723741\", \"octolytics-dimension-repository_network_root_id\": \"496687161\", \"route-controller\": \"files\", \"msapplication-TileImage\": \"/windows-tile.png\", \"octolytics-url\": \"https://collector.github.com/github/collect\", \"apple-itunes-app\": \"app-id=1477376905, app-argument=https://github.com/cure-lab/LTSF-Linear\", \"theme-color\": \"#1e2327\", \"hovercard-subject-tag\": \"repository:496687161\", \"turbo-body-classes\": \"logged-out env-production page-responsive\", \"description\": \"[AAAI-23 Oral] Official implementation of the paper \\\"Are Transformers Effective for Time Series Forecasting?\\\" - cure-lab/LTSF-Linear\", \"twitter:image\": \"https://opengraph.githubassets.com/c1cee1fda9bc86180561e25df037c4346c1abdbd4690b68cc55c9c822bf06b54/cure-lab/LTSF-Linear\", \"twitter:site\": \"@github\", \"visitor-payload\": \"eyJyZWZlcnJlciI6IiIsInJlcXVlc3RfaWQiOiIyRTAzOjE1NzQ6NDhCNkUxNzozQjVCRkQ2OjY5OEJFODc1IiwidmlzaXRvcl9pZCI6Ijg0MTE5MzA5NDk3OTMzNDM2MDUiLCJyZWdpb25fZWRnZSI6ImZyYSIsInJlZ2lvbl9yZW5kZXIiOiJmcmEifQ==\", \"github-keyboard-shortcuts\": \"repository,copilot\", \"lang\": \"en\", \"fetch-nonce\": \"v2:3d5a8841-37b4-7f64-8dee-a46467f33b70\", \"og:type\": \"object\", \"twitter:title\": \"GitHub - cure-lab/LTSF-Linear: [AAAI-23 Oral] Official implementation of the paper \\\"Are Transformers Effective for Time Series Forecasting?\\\"\", \"og:title\": \"GitHub - cure-lab/LTSF-Linear: [AAAI-23 Oral] Official implementation of the paper \\\"Are Transformers Effective for Time Series Forecasting?\\\"\", \"og:image:height\": \"600\", \"analytics-location\": \"/<user-name>/<repo-name>\", \"color-scheme\": \"light dark\", \"route-action\": \"disambiguate\", \"msapplication-TileColor\": \"#ffffff\", \"octolytics-dimension-repository_public\": \"true\", \"fb:app_id\": \"1401488693436528\", \"ui-target\": \"full\", \"octolytics-dimension-repository_nwo\": \"cure-lab/LTSF-Linear\", \"viewport\": \"width=device-width\", \"twitter:description\": \"[AAAI-23 Oral] Official implementation of the paper \\\"Are Transformers Effective for Time Series Forecasting?\\\" - cure-lab/LTSF-Linear\", \"current-catalog-service-hash\": \"f3abb0cc802f3d7b95fc8762b94bdcb13bf39634c40c357301c4aa1d67a256fb\", \"disable-turbo\": \"false\", \"og:url\": \"https://github.com/cure-lab/LTSF-Linear\"}, \"external\": {\"stylesheet\": {\"https://github.githubassets.com/assets/light-7a548c73f231168f.css\": {\"media\": \"all\"}, \"https://github.githubassets.com/assets/primer-primitives-2c5f58bae956fa96.css\": {\"media\": \"all\"}, \"https://github.githubassets.com/assets/primer-react.7bdab33cf4d9c38d.module.css\": {\"media\": \"all\"}, \"https://github.githubassets.com/assets/keyboard-shortcuts-dialog.124a139fc586638e.module.css\": {\"media\": \"all\"}, \"https://github.githubassets.com/assets/primer-b97936554294d7a3.css\": {\"media\": \"all\"}, \"https://github.githubassets.com/assets/global-5d300701a37a830a.css\": {\"media\": \"all\"}, \"https://github.githubassets.com/assets/dark-a037c5198a3956d3.css\": {\"media\": \"all\"}, \"https://github.githubassets.com/assets/dark_high_contrast-c4cd957551b6a79f.css\": {\"media\": \"all\"}, \"https://github.githubassets.com/assets/light_high_contrast-6d5205ba83e64657.css\": {\"media\": \"all\"}, \"https://github.githubassets.com/assets/repository-c2a686ef3f37f3e5.css\": {\"media\": \"all\"}, \"https://github.githubassets.com/assets/code-67953c77b47fec3b.css\": {\"media\": \"all\"}, \"https://github.githubassets.com/assets/github-294399c8d5b652c5.css\": {\"media\": \"all\"}, \"https://github.githubassets.com/assets/94077.2a3ef11f8e0edae3.module.css\": {\"media\": \"all\"}, \"https://github.githubassets.com/assets/notifications-subscriptions-menu.72e14da340cf5788.module.css\": {\"media\": \"all\"}, \"https://github.githubassets.com/assets/marketing-navigation.6981f452ada322e1.module.css\": {\"media\": \"all\"}, \"https://github.githubassets.com/assets/appearance-settings.51d12175097b565e.module.css\": {\"media\": \"all\"}}, \"preconnect\": {\"https://github.githubassets.com\": {}, \"https://avatars.githubusercontent.com\": {}}, \"search\": {\"/opensearch.xml\": {\"title\": \"GitHub\", \"type\": \"application/opensearchdescription+xml\"}}, \"assets\": {\"https://github.githubassets.com/\": {}}, \"dns-prefetch\": {\"https://user-images.githubusercontent.com/\": {}, \"https://github-cloud.s3.amazonaws.com\": {}, \"https://github.githubassets.com\": {}, \"https://avatars.githubusercontent.com\": {}}, \"fluid-icon\": {\"https://github.com/fluidicon.png\": {\"title\": \"GitHub\"}}, \"manifest\": {\"/manifest.json\": {}}, \"icon\": {\"https://github.githubassets.com/favicons/favicon.svg\": {\"type\": \"image/svg+xml\"}}, \"alternate\": {\"https://github.githubassets.com/favicons/favicon.png\": {\"type\": \"image/png\"}}, \"mask-icon\": {\"https://github.githubassets.com/assets/pinned-octocat-093da3e6fa40.svg\": {}}, \"canonical\": {\"https://github.com/cure-lab/LTSF-Linear\": {}}}}]"},{"type":"tool_result","tool_use_id":"call_3e1cc4c14af741c0b101e3bc","content":"[{'text': {'url': 'https://arxiv.org/pdf/2205.13504', 'content': 'Are Transformers Effective for Time Series Forecasting?\\nAiling Zeng1*, Muxi Chen1*, Lei Zhang2, Qiang Xu1\\n1The Chinese University of Hong Kong\\n2International Digital Economy Academy (IDEA)\\n{alzeng, mxchen21, qxu}@cse.cuhk.edu.hk\\n{leizhang}@idea.edu.cn\\nAbstract\\nRecently, there has been a surge of Transformer-based\\nsolutions for the long-term time series forecasting (LTSF)\\ntask. Despite the growing performance over the past few\\nyears, we question the validity of this line of research in this\\nwork . Speciï¬cally, Transformers is arguably the most suc-\\ncessful solution to extract the semantic correlations among\\nthe elements in a long sequence. However, in time series\\nmodeling, we are to extract the temporal relations in an\\nordered set of continuous points . While employing posi-\\ntional encoding and using tokens to embed sub-series in\\nTransformers facilitate preserving some ordering informa-\\ntion, the nature of the permutation-invariant self-attention\\nmechanism inevitably results in temporal information loss.\\nTo validate our claim, we introduce a set of embarrass-\\ningly simple one-layer linear models named LTSF-Linear\\nfor comparison. Experimental results on nine real-life\\ndatasets show that LTSF-Linear surprisingly outperforms\\nexisting sophisticated Transformer-based LTSF models in\\nall cases, and often by a large margin. Moreover, we con-\\nduct comprehensive empirical studies to explore the im-\\npacts of various design elements of LTSF models on their\\ntemporal relation extraction capability. We hope this sur-\\nprising ï¬nding opens up new research directions for the\\nLTSF task. We also advocate revisiting the validity of\\nTransformer-based solutions for other time series analysis\\ntasks (e.g., anomaly detection) in the future. Code is avail-\\nable at: https://github.com/cure-lab/LTSF-\\nLinear .\\n1. Introduction\\nTime series are ubiquitous in todayâ€™s data-driven world.\\nGiven historical data, time series forecasting (TSF) is a\\nlong-standing task that has a wide range of applications,\\nincluding but not limited to trafï¬c ï¬‚ow estimation, en-\\n*Equal contributionergy management, and ï¬nancial investment. Over the past\\nseveral decades, TSF solutions have undergone a progres-\\nsion from traditional statistical methods (e.g., ARIMA [1])\\nand machine learning techniques (e.g., GBRT [11]) to\\ndeep learning-based solutions, e.g., Recurrent Neural Net-\\nworks [15] and Temporal Convolutional Networks [3, 17].\\nTransformer [26] is arguably the most successful se-\\nquence modeling architecture, demonstrating unparalleled\\nperformances in various applications, such as natural lan-\\nguage processing (NLP) [7], speech recognition [8], and\\ncomputer vision [19, 29]. Recently, there has also been a\\nsurge of Transformer-based solutions for time series anal-\\nysis, as surveyed in [27]. Most notable models, which\\nfocus on the less explored and challenging long-term time\\nseries forecasting (LTSF) problem, include LogTrans [16]\\n(NeurIPS 2019), Informer [30] (AAAI 2021 Best paper),\\nAutoformer [28] (NeurIPS 2021), Pyraformer [18] (ICLR\\n2022 Oral), Triformer [5] (IJCAI 2022) and the recent FED-\\nformer [31] (ICML 2022).\\nThe main working power of Transformers is from its\\nmulti-head self-attention mechanism, which has a remark-\\nable capability of extracting semantic correlations among\\nelements in a long sequence (e.g., words in texts or 2D\\npatches in images). However, self-attention is permutation-\\ninvariant and â€œanti-orderâ€ to some extent. While using var-\\nious types of positional encoding techniques can preserve\\nsome ordering information, it is still inevitable to have tem-\\nporal information loss after applying self-attention on top\\nof them. This is usually not a serious concern for semantic-\\nrich applications such as NLP, e.g., the semantic meaning\\nof a sentence is largely preserved even if we reorder some\\nwords in it. However, when analyzing time series data,\\nthere is usually a lack of semantics in the numerical data\\nitself, and we are mainly interested in modeling the tempo-\\nral changes among a continuous set of points . That is, the\\norder itself plays the most crucial role. Consequently, we\\npose the following intriguing question: Are Transformers\\nreally effective for long-term time series forecasting?\\nMoreover, while existing Transformer-based LTSF so-\\n1arXiv:2205.13504v3 [cs.AI] 17 Aug 2022\\nlutions have demonstrated considerable prediction accu-\\nracy improvements over traditional methods, in their exper-\\niments, all the compared (non-Transformer) baselines per-\\nform autoregressive or iterated multi-step (IMS) forecast-\\ning [1,2,22,24], which are known to suffer from signiï¬cant\\nerror accumulation effects for the LTSF problem. There-\\nfore, in this work, we challenge Transformer-based LTSF\\nsolutions with direct multi-step (DMS) forecasting strate-\\ngies to validate their real performance.\\nNot all time series are predictable, let alone long-term\\nforecasting (e.g., for chaotic systems). We hypothesize that\\nlong-term forecasting is only feasible for those time series\\nwith a relatively clear trend and periodicity. As linear mod-\\nels can already extract such information, we introduce a set\\nof embarrassingly simple models named LTSF-Linear as a\\nnew baseline for comparison. LTSF-Linear regresses histor-\\nical time series with a one-layer linear model to forecast fu-\\nture time series directly. We conduct extensive experiments\\non nine widely-used benchmark datasets that cover various\\nreal-life applications: trafï¬c, energy, economics, weather,\\nand disease predictions. Surprisingly, our results show that\\nLTSF-Linear outperforms existing complex Transformer-\\nbased models in all cases, and often by a large margin (20%\\n50%) . Moreover, we ï¬nd that, in contrast to the claims in\\nexisting Transformers, most of them fail to extract temporal\\nrelations from long sequences, i.e., the forecasting errors are\\nnot reduced (sometimes even increased) with the increase of\\nlook-back window sizes. Finally, we conduct various abla-\\ntion studies on existing Transformer-based TSF solutions to\\nstudy the impact of various design elements in them.\\nTo sum up, the contributions of this work include:\\nâ€¢ To the best of our knowledge, this is the ï¬rst work to\\nchallenge the effectiveness of the booming Transform-\\ners for the long-term time series forecasting task.\\nâ€¢ To validate our claims, we introduce a set of em-\\nbarrassingly simple one-layer linear models, named\\nLTSF-Linear , and compare them with existing\\nTransformer-based LTSF solutions on nine bench-\\nmarks. LTSF-Linear can be a new baseline for the\\nLTSF problem.\\nâ€¢ We conduct comprehensive empirical studies on var-\\nious aspects of existing Transformer-based solutions,\\nincluding the capability of modeling long inputs, the\\nsensitivity to time series order, the impact of posi-\\ntional encoding and sub-series embedding, and efï¬-\\nciency comparisons. Our ï¬ndings would beneï¬t future\\nresearch in this area.\\nWith the above, we conclude that the temporal model-\\ning capabilities of Transformers for time series are exag-\\ngerated, at least for the existing LTSF benchmarks . At the\\nsame time, while LTSF-Linear achieves a better predictionaccuracy compared to existing works, it merely serves as a\\nsimple baseline for future research on the challenging long-\\nterm TSF problem. With our ï¬ndings, we also advocate\\nrevisiting the validity of Transformer-based solutions for\\nother time series analysis tasks in the future.\\n2. Preliminaries: TSF Problem Formulation\\nFor time series containing Cvariates, given historical\\ndataX=fXt\\n1; :::; Xt\\nCgL\\nt=1, wherein Lis the look-back\\nwindow size and Xt\\niis the value of the ithvariate at the tth\\ntime step. The time series forecasting task is to predict the\\nvalues ^X=f^Xt\\n1; :::;^Xt\\nCgL+T\\nt=L+1at the Tfuture time steps.\\nWhen T > 1, iterated multi-step (IMS) forecasting [23]\\nlearns a single-step forecaster and iteratively applies it to\\nobtain multi-step predictions. Alternatively, direct multi-\\nstep (DMS) forecasting [4] directly optimizes the multi-step\\nforecasting objective at once.\\nCompared to DMS forecasting results, IMS predictions\\nhave smaller variance thanks to the autoregressive estima-\\ntion procedure, but they inevitably suffer from error accu-\\nmulation effects. Consequently, IMS forecasting is prefer-\\nable when there is a highly-accurate single-step forecaster,\\nandTis relatively small. In contrast, DMS forecasting gen-\\nerates more accurate predictions when it is hard to obtain an\\nunbiased single-step forecasting model, or Tis large.\\n3. Transformer-Based LTSF Solutions\\nTransformer-based models [26] have achieved unparal-\\nleled performances in many long-standing AI tasks in natu-\\nral language processing and computer vision ï¬elds, thanks\\nto the effectiveness of the multi-head self-attention mech-\\nanism. This has also triggered lots of research interest\\nin Transformer-based time series modeling techniques [20,\\n27]. In particular, a large amount of research works are ded-\\nicated to the LTSF task (e.g., [16, 18, 28, 30, 31]). Con-\\nsidering the ability to capture long-range dependencies\\nwith Transformer models, most of them focus on the less-\\nexplored long-term forecasting problem ( T1)1.\\nWhen applying the vanilla Transformer model to the\\nLTSF problem, it has some limitations, including the\\nquadratic time/memory complexity with the original self-\\nattention scheme and error accumulation caused by the au-\\ntoregressive decoder design. Informer [30] addresses these\\nissues and proposes a novel Transformer architecture with\\nreduced complexity and a DMS forecasting strategy. Later,\\nmore Transformer variants introduce various time series\\nfeatures into their models for performance or efï¬ciency im-\\nprovements [18,28,31]. We summarize the design elements\\nof existing Transformer-based LTSF solutions as follows\\n(see Figure 1).\\n1Due to page limit, we leave the discussion of non-Transformer fore-\\ncasting solutions in the Appendix.\\n2\\n(d) Decoder (c) Encoder (b) Embedding (a) Preprocessing\\nForecasting Output\\n(d) Decoder (c) Encoder (b) Embedding (a) Preprocessing\\nOutput\\nInput\\nSeasonal-trend\\ndecompositionNormalization\\nTimestamp\\npreparationChannel projection\\nFixed position\\nLocal timestamp\\nGlobal timestampProbSparseand distilling\\nself-attention @ Informer\\nSeries auto-correlation with\\ndecomposition @ Autoformer\\nMulti-resolution pyramidal\\nattention @ Pyraformer\\nFrequency enhanced block with\\ndecomposition @ FEDformerLogSparse and convolutional\\nself-attention @ LogTrans\\nDirect Multi-Step\\n(DMS) @ Informer\\nDMS with auto-correlation and\\ndecomposition @ Autoformer\\nDMS along spatio-temporal\\ndimension @ Pyraformer\\nDMS with frequency attention\\nanddecomposition@ FEDformerIterated Multi-Step\\n(IMS) @LogTransFigure 1. The pipeline of existing Transformer-based TSF solutions. In (a) and (b), the solid boxes are essential operations, and the dotted\\nboxes are applied optionally. (c) and (d) are distinct for different methods [16, 18, 28, 30, 31].\\nTime series decomposition: For data preprocessing, nor-\\nmalization with zero-mean is common in TSF. Besides,\\nAutoformer [28] ï¬rst applies seasonal-trend decomposition\\nbehind each neural block, which is a standard method in\\ntime series analysis to make raw data more predictable [6,\\n13]. Speciï¬cally, they use a moving average kernel on the\\ninput sequence to extract the trend-cyclical component of\\nthe time series. The difference between the original se-\\nquence and the trend component is regarded as the seasonal\\ncomponent. On top of the decomposition scheme of Aut-\\noformer, FEDformer [31] further proposes the mixture of\\nexpertsâ€™ strategies to mix the trend components extracted\\nby moving average kernels with various kernel sizes.\\nInput embedding strategies: The self-attention layer in\\nthe Transformer architecture cannot preserve the positional\\ninformation of the time series. However, local positional\\ninformation, i.e. the ordering of time series, is important.\\nBesides, global temporal information, such as hierarchical\\ntimestamps (week, month, year) and agnostic timestamps\\n(holidays and events), is also informative [30]. To enhance\\nthe temporal context of time-series inputs, a practical design\\nin the SOTA Transformer-based methods is injecting sev-\\neral embeddings, like a ï¬xed positional encoding, a channel\\nprojection embedding, and learnable temporal embeddings\\ninto the input sequence. Moreover, temporal embeddings\\nwith a temporal convolution layer [16] or learnable times-\\ntamps [28] are introduced.\\nSelf-attention schemes: Transformers rely on the self-\\nattention mechanism to extract the semantic dependen-\\ncies between paired elements. Motivated by reducing\\ntheO\\nL2\\ntime and memory complexity of the vanilla\\nTransformer, recent works propose two strategies for ef-\\nï¬ciency. On the one hand, LogTrans and Pyraformer\\nexplicitly introduce a sparsity bias into the self-attention\\nscheme. Speciï¬cally, LogTrans uses a Logsparse mask to\\nreduce the computational complexity to O(LlogL )while\\nPyraformer adopts pyramidal attention that captures hierar-\\nchically multi-scale temporal dependencies with an O(L)\\ntime and memory complexity. On the other hand, In-\\nformer and FEDformer use the low-rank property in the\\nself-attention matrix. Informer proposes a ProbSparse self-attention mechanism and a self-attention distilling operation\\nto decrease the complexity to O(LlogL ), and FEDformer\\ndesigns a Fourier enhanced block and a wavelet enhanced\\nblock with random selection to obtain O(L)complexity.\\nLastly, Autoformer designs a series-wise auto-correlation\\nmechanism to replace the original self-attention layer.\\nDecoders: The vanilla Transformer decoder outputs se-\\nquences in an autoregressive manner, resulting in a slow in-\\nference speed and error accumulation effects, especially for\\nlong-term predictions. Informer designs a generative-style\\ndecoder for DMS forecasting. Other Transformer variants\\nemploy similar DMS strategies. For instance, Pyraformer\\nuses a fully-connected layer concatenating Spatio-temporal\\naxes as the decoder. Autoformer sums up two reï¬ned de-\\ncomposed features from trend-cyclical components and the\\nstacked auto-correlation mechanism for seasonal compo-\\nnents to get the ï¬nal prediction. FEDformer also uses a\\ndecomposition scheme with the proposed frequency atten-\\ntion block to decode the ï¬nal results.\\nThe premise of Transformer models is the semantic cor-\\nrelations between paired elements, while the self-attention\\nmechanism itself is permutation-invariant, and its capabil-\\nity of modeling temporal relations largely depends on posi-\\ntional encodings associated with input tokens. Considering\\nthe raw numerical data in time series (e.g., stock prices or\\nelectricity values), there are hardly any point-wise semantic\\ncorrelations between them. In time series modeling, we are\\nmainly interested in the temporal relations among a contin-\\nuous set of points, and the order of these elements instead\\nof the paired relationship plays the most crucial role. While\\nemploying positional encoding and using tokens to embed\\nsub-series facilitate preserving some ordering information,\\nthe nature of the permutation-invariant self-attention mech-\\nanism inevitably results in temporal information loss. Due\\nto the above observations, we are interested in revisiting the\\neffectiveness of Transformer-based LTSF solutions.\\n4. An Embarrassingly Simple Baseline\\nIn the experiments of existing Transformer-based LTSF\\nsolutions ( T1), all the compared (non-Transformer)\\n3\\nbaselines are IMS forecasting techniques, which are known\\nto suffer from signiï¬cant error accumulation effects. We\\nhypothesize that the performance improvements in these\\nworks are largely due to the DMS strategy used in them.\\nLinear ğ‘Š ï¿½\\nLinear ğ‘Š ï¿½Look-back WindowRemainder\\nTrendForecasting Output\\nğ‘‹ï¿½âˆˆ â„ï¿½Ã—ï¿½\\nğ‘‹ï¿½âˆˆ â„ï¿½Ã—ï¿½ğ»ï¿½âˆˆ â„ï¿½Ã—ï¿½ğ»ï¿½âˆˆ â„ï¿½Ã—ï¿½\\nğ‘‹ï¿½âˆˆ â„ï¿½Ã—ï¿½ ğ‘‹ âˆˆ â„ï¿½Ã—ï¿½\\nHistory ğ¿timestepsFuture ğ‘‡timesteps\\n(a) The whole structure of DLinear (b) One Linear Layer\\nFigure 2. Illustration of the basic linear model.\\nTo validate this hypothesis, we present the simplest DMS\\nmodel via a temporal linear layer, named LTSF-Linear , as a\\nbaseline for comparison. The basic formulation of LTSF-\\nLinear directly regresses historical time series for future\\nprediction via a weighted sum operation (as illustrated in\\nFigure 2). The mathematical expression is ^Xi=WX i,\\nwhere W2RTLis a linear layer along the temporal axis.\\n^XiandXiare the prediction and input for each ithvari-\\nate. Note that LTSF-Linear shares weights across different\\nvariates and does not model any spatial correlations.\\nLTSF-Linear is a set of linear models. Vanilla Linear is\\na one-layer linear model. To handle time series across dif-\\nferent domains (e.g., ï¬nance, trafï¬c, and energy domains),\\nwe further introduce two variants with two preprocessing\\nmethods, named DLinear andNLinear .\\nâ€¢ Speciï¬cally, DLinear is a combination of a Decom-\\nposition scheme used in Autoformer and FEDformer\\nwith linear layers. It ï¬rst decomposes a raw data in-\\nput into a trend component by a moving average ker-\\nnel and a remainder (seasonal) component. Then, two\\none-layer linear layers are applied to each component,\\nand we sum up the two features to get the ï¬nal predic-\\ntion. By explicitly handling trend, DLinear enhances\\nthe performance of a vanilla linear when there is a clear\\ntrend in the data.\\nâ€¢ Meanwhile, to boost the performance of LTSF-Linear\\nwhen there is a distribution shift in the dataset, NLin-\\nearï¬rst subtracts the input by the last value of the se-\\nquence. Then, the input goes through a linear layer,\\nand the subtracted part is added back before making\\nthe ï¬nal prediction. The subtraction and addition in\\nNLinear are a simple normalization for the input se-\\nquence.5. Experiments\\n5.1. Experimental Settings\\nDataset. We conduct extensive experiments on nine\\nwidely-used real-world datasets, including ETT (Electricity\\nTransformer Temperature) [30] (ETTh1, ETTh2, ETTm1,\\nETTm2), Trafï¬c, Electricity, Weather, ILI, Exchange-\\nRate [15]. All of them are multivariate time series. We\\nleave data descriptions in the Appendix.\\nEvaluation metric. Following previous works [28, 30,\\n31], we use Mean Squared Error (MSE) and Mean Absolute\\nError (MAE) as the core metrics to compare performance.\\nCompared methods. We include ï¬ve recent\\nTransformer-based methods: FEDformer [31], Aut-\\noformer [28], Informer [30], Pyraformer [18], and\\nLogTrans [16]. Besides, we include a naive DMS method:\\nClosest Repeat ( Repeat ), which repeats the last value in the\\nlook-back window, as another simple baseline. Since there\\nare two variants of FEDformer, we compare the one with\\nbetter accuracy (FEDformer-f via Fourier transform).\\n5.2. Comparison with Transformers\\nQuantitative results. In Table 2, we extensively eval-\\nuate all mentioned Transformers on nine benchmarks, fol-\\nlowing the experimental setting of previous work [28, 30,\\n31]. Surprisingly, the performance of LTSF-Linear sur-\\npasses the SOTA FEDformer in most cases by 20% 50%\\nimprovements on the multivariate forecasting , where LTSF-\\nLinear even does not model correlations among variates.\\nFor different time series benchmarks, NLinear andDLin-\\nearshow the superiority to handle the distribution shift and\\ntrend-seasonality features. We also provide results for uni-\\nvariate forecasting of ETT datasets in the Appendix, where\\nLTSF-Linear still consistently outperforms Transformer-\\nbased LTSF solutions by a large margin.\\nFEDformer achieves competitive forecasting accuracy\\non ETTh1. This because FEDformer employs classical time\\nseries analysis techniques such as frequency processing,\\nwhich brings in time series inductive bias and beneï¬ts the\\nability of temporal feature extraction. In summary, these re-\\nsults reveal that existing complex Transformer-based LTSF\\nsolutions are not seemingly effective on the existing nine\\nbenchmarks while LTSF-Linear can be a powerful baseline.\\nAnother interesting observation is that even though the\\nnaive Repeat method shows worse results when predict-\\ning long-term seasonal data (e.g., Electricity and Trafï¬c), it\\nsurprisingly outperforms all Transformer-based methods on\\nExchange-Rate (around 45%). This is mainly caused by the\\nwrong prediction of trends in Transformer-based solutions,\\nwhich may overï¬t toward sudden change noises in the train-\\ning data, resulting in signiï¬cant accuracy degradation (see\\nFigure 3(b)). Instead, Repeat does not have the bias.\\nQualitative results. As shown in Figure 3, we plot\\n4\\nDatasets ETTh1 &ETTh2 ETTm1 &ETTm2 Trafï¬c Electricity Exchange-Rate Weather ILI\\nVariates 7 7 862 321 8 21 7\\nTimesteps 17,420 69,680 17,544 26,304 7,588 52,696 966\\nGranularity 1hour 5min 1hour 1hour 1day 10min 1week\\nTable 1. The statistics of the nine popular datasets for the LTSF problem.\\nMethods IMP. Linear* NLinear* DLinear* FEDformer Autoformer Informer Pyraformer* LogTrans Repeat *\\nMetric MSE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAEElectricity96 27.40% 0.140 0.237 0.141 0.237 0.140 0.237 0.193 0.308 0.201 0.317 0.274 0.368 0.386 0.449 0.258 0.357 1.588 0.946\\n192 23.88% 0.153 0.250 0.154 0.248 0.153 0.249 0.201 0.315 0.222 0.334 0.296 0.386 0.386 0.443 0.266 0.368 1.595 0.950\\n336 21.02% 0.169 0.268 0.171 0.265 0.169 0.267 0.214 0.329 0.231 0.338 0.300 0.394 0.378 0.443 0.280 0.380 1.617 0.961\\n720 17.47% 0.203 0.301 0.210 0.297 0.203 0.301 0.246 0.355 0.254 0.361 0.373 0.439 0.376 0.445 0.283 0.376 1.647 0.975Exchange96 45.27% 0.082 0.207 0.089 0.208 0.081 0.203 0.148 0.278 0.197 0.323 0.847 0.752 0.376 1.105 0.968 0.812 0.081 0.196\\n192 42.06% 0.167 0.304 0.180 0.300 0.157 0.293 0.271 0.380 0.300 0.369 1.204 0.895 1.748 1.151 1.040 0.851 0.167 0.289\\n336 33.69% 0.328 0.432 0.331 0.415 0.305 0.414 0.460 0.500 0.509 0.524 1.672 1.036 1.874 1.172 1.659 1.081 0.305 0.396\\n720 46.19% 0.964 0.750 1.033 0.780 0.643 0.601 1.195 0.841 1.447 0.941 2.478 1.310 1.943 1.206 1.941 1.127 0.823 0.681Trafï¬c96 30.15% 0.410 0.282 0.410 0.279 0.410 0.282 0.587 0.366 0.613 0.388 0.719 0.391 2.085 0.468 0.684 0.384 2.723 1.079\\n192 29.96% 0.423 0.287 0.423 0.284 0.423 0.287 0.604 0.373 0.616 0.382 0.696 0.379 0.867 0.467 0.685 0.390 2.756 1.087\\n336 29.95% 0.436 0.295 0.435 0.290 0.436 0.296 0.621 0.383 0.622 0.337 0.777 0.420 0.869 0.469 0.734 0.408 2.791 1.095\\n720 25.87% 0.466 0.315 0.464 0.307 0.466 0.315 0.626 0.382 0.660 0.408 0.864 0.472 0.881 0.473 0.717 0.396 2.811 1.097Weather96 18.89% 0.176 0.236 0.182 0.232 0.176 0.237 0.217 0.296 0.266 0.336 0.300 0.384 0.896 0.556 0.458 0.490 0.259 0.254\\n192 21.01% 0.218 0.276 0.225 0.269 0.220 0.282 0.276 0.336 0.307 0.367 0.598 0.544 0.622 0.624 0.658 0.589 0.309 0.292\\n336 22.71% 0.262 0.312 0.271 0.301 0.265 0.319 0.339 0.380 0.359 0.395 0.578 0.523 0.739 0.753 0.797 0.652 0.377 0.338\\n720 19.85% 0.326 0.365 0.338 0.348 0.323 0.362 0.403 0.428 0.419 0.428 1.059 0.741 1.004 0.934 0.869 0.675 0.465 0.394ILI24 47.86% 1.947 0.985 1.683 0.858 2.215 1.081 3.228 1.260 3.483 1.287 5.764 1.677 1.420 2.012 4.480 1.444 6.587 1.701\\n36 36.43% 2.182 1.036 1.703 0.859 1.963 0.963 2.679 1.080 3.103 1.148 4.755 1.467 7.394 2.031 4.799 1.467 7.130 1.884\\n48 34.43% 2.256 1.060 1.719 0.884 2.130 1.024 2.622 1.078 2.669 1.085 4.763 1.469 7.551 2.057 4.800 1.468 6.575 1.798\\n60 34.33% 2.390 1.104 1.819 0.917 2.368 1.096 2.857 1.157 2.770 1.125 5.264 1.564 7.662 2.100 5.278 1.560 5.893 1.677ETTh196 0.80% 0.375 0.397 0.374 0.394 0.375 0.399 0.376 0.419 0.449 0.459 0.865 0.713 0.664 0.612 0.878 0.740 1.295 0.713\\n192 3.57% 0.418 0.429 0.408 0.415 0.405 0.416 0.420 0.448 0.500 0.482 1.008 0.792 0.790 0.681 1.037 0.824 1.325 0.733\\n336 6.54% 0.479 0.476 0.429 0.427 0.439 0.443 0.459 0.465 0.521 0.496 1.107 0.809 0.891 0.738 1.238 0.932 1.323 0.744\\n720 13.04% 0.624 0.592 0.440 0.453 0.472 0.490 0.506 0.507 0.514 0.512 1.181 0.865 0.963 0.782 1.135 0.852 1.339 0.756ETTh296 19.94% 0.288 0.352 0.277 0.338 0.289 0.353 0.346 0.388 0.358 0.397 3.755 1.525 0.645 0.597 2.116 1.197 0.432 0.422\\n192 19.81% 0.377 0.413 0.344 0.381 0.383 0.418 0.429 0.439 0.456 0.452 5.602 1.931 0.788 0.683 4.315 1.635 0.534 0.473\\n336 25.93% 0.452 0.461 0.357 0.400 0.448 0.465 0.496 0.487 0.482 0.486 4.721 1.835 0.907 0.747 1.124 1.604 0.591 0.508\\n720 14.25% 0.698 0.595 0.394 0.436 0.605 0.551 0.463 0.474 0.515 0.511 3.647 1.625 0.963 0.783 3.188 1.540 0.588 0.517ETTm196 21.10% 0.308 0.352 0.306 0.348 0.299 0.343 0.379 0.419 0.505 0.475 0.672 0.571 0.543 0.510 0.600 0.546 1.214 0.665\\n192 21.36% 0.340 0.369 0.349 0.375 0.335 0.365 0.426 0.441 0.553 0.496 0.795 0.669 0.557 0.537 0.837 0.700 1.261 0.690\\n336 17.07% 0.376 0.393 0.375 0.388 0.369 0.386 0.445 0.459 0.621 0.537 1.212 0.871 0.754 0.655 1.124 0.832 1.283 0.707\\n720 21.73% 0.440 0.435 0.433 0.422 0.425 0.421 0.543 0.490 0.671 0.561 1.166 0.823 0.908 0.724 1.153 0.820 1.319 0.729ETTm296 17.73% 0.168 0.262 0.167 0.255 0.167 0.260 0.203 0.287 0.255 0.339 0.365 0.453 0.435 0.507 0.768 0.642 0.266 0.328\\n192 17.84% 0.232 0.308 0.221 0.293 0.224 0.303 0.269 0.328 0.281 0.340 0.533 0.563 0.730 0.673 0.989 0.757 0.340 0.371\\n336 15.69% 0.320 0.373 0.274 0.327 0.281 0.342 0.325 0.366 0.339 0.372 1.363 0.887 1.201 0.845 1.334 0.872 0.412 0.410\\n720 12.58% 0.413 0.435 0.368 0.384 0.397 0.421 0.421 0.415 0.433 0.432 3.379 1.338 3.625 1.451 3.048 1.328 0.521 0.465\\n- Methods* are implemented by us; Other results are from FEDformer [31].\\nTable 2. Multivariate long-term forecasting errors in terms of MSE and MAE, the lower the better. Among them, ILI dataset is with\\nforecasting horizon T2 f24;36;48;60g. For the others, T2 f96;192;336;720g.Repeat repeats the last value in the look-back window.\\nThebest results are highlighted in bold and the best results of Transformers are highlighted with a underline . Accordingly, IMP. is the\\nbest result of linear models compared to the results of Transformer-based solutions.\\nthe prediction results on three selected time series datasets\\nwith Transformer-based solutions and LTSF-Linear : Elec-\\ntricity (Sequence 1951, Variate 36), Exchange-Rate (Se-\\nquence 676, Variate 3), and ETTh2 ( Sequence 1241, Vari-\\nate 2), where these datasets have different temporal patterns.\\nWhen the input length is 96 steps, and the output horizon\\nis 336 steps, Transformers [28, 30, 31] fail to capture the\\nscale and bias of the future data on Electricity and ETTh2.\\nMoreover, they can hardly predict a proper trend on aperi-\\nodic data such as Exchange-Rate. These phenomena further\\nindicate the inadequacy of existing Transformer-based solu-\\ntions for the LTSF task.\\n5.3. More Analyses on LTSF-Transformers\\nCan existing LTSF-Transformers extract temporal re-\\nlations well from longer input sequences? The size of the\\nlook-back window greatly impacts forecasting accuracy asit determines how much we can learn from historical data.\\nGenerally speaking, a powerful TSF model with a strong\\ntemporal relation extraction capability should be able to\\nachieve better results with larger look-back window sizes.\\nTo study the impact of input look-back win-\\ndow sizes, we conduct experiments with L2\\nf24;48;72;96;120;144;168;192;336;504;672;720g\\nfor long-term forecasting (T=720). Figure 4 demonstrates\\nthe MSE results on two datasets. Similar to the observations\\nfrom previous studies [27, 30], existing Transformer-based\\nmodelsâ€™ performance deteriorates or stays stable when\\nthe look-back window size increases. In contrast, the\\nperformances of all LTSF-Linear are signiï¬cantly boosted\\nwith the increase of look-back window size. Thus, existing\\nsolutions tend to overï¬t temporal noises instead of extract-\\ning temporal information if given a longer sequence, and\\nthe input size 96 is exactly suitable for most Transformers.\\n5\\n0 50 100 150 200 250 3001.5\\n1.0\\n0.5\\n0.00.51.0GrouthTruth Autoformer Informer FEDformer DLinear(a) Electricity\\n0 50 100 150 200 250 3001\\n01234GrouthTruth Autoformer Informer FEDformer DLinear (b) Exchange-Rate\\n0 50 100 150 200 250 3001.5\\n1.0\\n0.5\\n0.00.51.0GrouthTruth Autoformer Informer FEDformer DLinear (c) ETTh2\\nFigure 3. Illustration of the long-term forecasting output (Y-axis) of ï¬ve models with an input length L=96 and output length T=192\\n(X-axis) on Electricity, Exchange-Rate, and ETTh2, respectively.\\nAdditionally, we provide more quantitative results in the\\nAppendix, and our conclusion holds in almost all cases.\\n24 48 72 96 120 144 168 192 336 504 672 7200.40.60.81.01.21.4\\nTransformer\\nInformerAutoformer\\nFEDformerPyraformer\\nLinearNLinear\\nDLinear\\n(a)720steps- Trafï¬c\\n24 48 72 96 120 144 168 192 336 504 672 7200.200.250.300.350.40\\nTransformer\\nInformerAutoformer\\nFEDformerPyraformer\\nLinearNLinear\\nDLinear (b)720steps- Electricity\\nFigure 4. The MSE results (Y-axis) of models with different look-\\nback window sizes (X-axis) of long-term forecasting (T= 720) on\\nthe Trafï¬c and Electricity datasets.\\nWhat can be learned for long-term forecasting? While\\nthe temporal dynamics in the look-back window signiï¬-\\ncantly impact the forecasting accuracy of short-term time\\nseries forecasting, we hypothesize that long-term forecast-\\ning depends on whether models can capture the trend and\\nperiodicity well only. That is, the farther the forecasting\\nhorizon, the less impact the look-back window itself has.\\nMethods FEDformer Autoformer\\nInput Close Far Close Far\\nElectricity 0.251 0.265 0.255 0.287\\nTrafï¬c 0.631 0.645 0.677 0.675\\nTable 3. Comparison of different input sequences under the MSE\\nmetric to explore what LTSF-Transformers depend on. If the in-\\nput is Close , we use the 96th;:::;191thtime steps as the input\\nsequence. If the input is Far, we use the 0th;:::;95thtime steps.\\nBoth of them forecast the 192th;:::;(192 + 720) thtime steps.\\nTo validate the above hypothesis, in Table 3, we compare\\nthe forecasting accuracy for the same future 720 time steps\\nwith data from two different look-back windows: (i). the\\noriginal input L=96 setting (called Close ) and (ii). the far\\ninput L=96 setting (called Far) that is before the original96 time steps. From the experimental results, the perfor-\\nmance of the SOTA Transformers drops slightly, indicat-\\ning these models only capture similar temporal information\\nfrom the adjacent time series sequence. Since capturing the\\nintrinsic characteristics of the dataset generally does not re-\\nquire a large number of parameters, i,e. one parameter can\\nrepresent the periodicity. Using too many parameters will\\neven cause overï¬tting, which partially explains why LTSF-\\nLinear performs better than Transformer-based methods.\\nAre the self-attention scheme effective for LTSF? We\\nverify whether these complex designs in the existing Trans-\\nformer (e.g., Informer) are essential. In Table 4, we gradu-\\nally transform Informer to Linear. First, we replace each\\nself-attention layer by a linear layer, called Att.-Linear ,\\nsince a self-attention layer can be regarded as a fully-\\nconnected layer where weights are dynamically changed.\\nFurthermore, we discard other auxiliary designs (e.g., FFN)\\nin Informer to leave embedding layers and linear layers,\\nnamed Embed + Linear . Finally, we simplify the model to\\none linear layer. Surprisingly, the performance of Informer\\ngrows with the gradual simpliï¬cation, indicating the unnec-\\nessary of the self-attention scheme and other complex mod-\\nules at least for existing LTSF benchmarks.\\nMethods Informer Att.-Linear Embed + Linear LinearExchange96 0.847 1.003 0.173 0.084\\n192 1.204 0.979 0.443 0.155\\n336 1.672 1.498 1.288 0.301\\n720 2.478 2.102 2.026 0.763ETTh196 0.865 0.613 0.454 0.400\\n192 1.008 0.759 0.686 0.438\\n336 1.107 0.921 0.821 0.479\\n720 1.181 0.902 1.051 0.515\\nTable 4. The MSE comparisons of gradually transforming In-\\nformer to a Linear from the left to right columns. Att.-Linear is\\na structure that replaces each attention layer with a linear layer.\\nEmbed + Linear is to drop other designs and only keeps embed-\\nding layers and a linear layer. The look-back window size is 96.\\nCan existing LTSF-Transformers preserve temporal\\norder well? Self-attention is inherently permutation-\\n6\\nMethods Linear FEDformer Autoformer Informer\\nPredict Length Ori. Shuf. Half-Ex. Ori. Shuf. Half-Ex. Ori. Shuf. Half-Ex. Ori. Shuf. Half-Ex.Exchange96 0.080 0.133 0.169 0.161 0.160 0.162 0.152 0.158 0.160 0.952 1.004 0.959\\n192 0.162 0.208 0.243 0.274 0.275 0.275 0.278 0.271 0.277 1.012 1.023 1.014\\n336 0.286 0.320 0.345 0.439 0.439 0.439 0.435 0.430 0.435 1.177 1.181 1.177\\n720 0.806 0.819 0.836 1.122 1.122 1.122 1.113 1.113 1.113 1.198 1.210 1.196\\nAverage Drop N/A 27.26% 46.81% N/A -0.09% 0.20% N/A 0.09% 1.12% N/A -0.12% -0.18%ETTh196 0.395 0.824 0.431 0.376 0.753 0.405 0.455 0.838 0.458 0.974 0.971 0.971\\n192 0.447 0.824 0.471 0.419 0.730 0.436 0.486 0.774 0.491 1.233 1.232 1.231\\n336 0.490 0.825 0.505 0.447 0.736 0.453 0.496 0.752 0.497 1.693 1.693 1.691\\n720 0.520 0.846 0.528 0.468 0.720 0.470 0.525 0.696 0.524 2.720 2.716 2.715\\nAverage Drop N/A 81.06% 4.78% N/A 73.28% 3.44% N/A 56.91% 0.46% N/A 1.98% 0.18%\\nTable 5. The MSE comparisons of models when shufï¬‚ing the raw input sequence. Shuf. randomly shufï¬‚es the input sequence. Half-EX.\\nrandomly exchanges the ï¬rst half of the input sequences with the second half. Average Drop is the average performance drop under all\\nforecasting lengths after shufï¬‚ing. All results are the average test MSE of ï¬ve runs.\\ninvariant, i.e., regardless of the order. However, in time-\\nseries forecasting, the sequence order often plays a crucial\\nrole. We argue that even with positional and temporal em-\\nbeddings, existing Transformer-based methods still suffer\\nfrom temporal information loss. In Table 5, we shufï¬‚e the\\nraw input before the embedding strategies. Two shufï¬‚ing\\nstrategies are presented: Shuf. randomly shufï¬‚es the whole\\ninput sequences and Half-Ex. exchanges the ï¬rst half of\\nthe input sequence with the second half. Interestingly, com-\\npared with the original setting ( Ori.) on the Exchange Rate,\\nthe performance of all Transformer-based methods does not\\nï¬‚uctuate even when the input sequence is randomly shuf-\\nï¬‚ed. By contrary, the performance of LTSF-Linear is dam-\\naged signiï¬cantly. These indicate that LTSF-Transformers\\nwith different positional and temporal embeddings preserve\\nquite limited temporal relations and are prone to overï¬t on\\nnoisy ï¬nancial data, while the LTSF-Linear can model the\\norder naturally and avoid overï¬tting with fewer parameters.\\nFor the ETTh1 dataset, FEDformer and Autoformer in-\\ntroduce time series inductive bias into their models, mak-\\ning them can extract certain temporal information when the\\ndataset has more clear temporal patterns (e.g., periodicity)\\nthan the Exchange Rate. Therefore, the average drops of\\nthe two Transformers are 73.28% and 56.91% under the\\nShuf. setting, where it loses the whole order information.\\nMoreover, Informer still suffers less from both Shuf. and\\nHalf-Ex. settings due to its no such temporal inductive bias.\\nOverall, the average drops of LTSF-Linear are larger than\\nTransformer-based methods for all cases, indicating the ex-\\nisting Transformers do not preserve temporal order well.\\nHow effective are different embedding strategies? We\\nstudy the beneï¬ts of position and timestamp embeddings\\nused in Transformer-based methods. In Table 6, the fore-\\ncasting errors of Informer largely increase without posi-\\ntional embeddings (wo/Pos.). Without timestamp embed-\\ndings (wo/Temp.) will gradually damage the performance\\nof Informer as the forecasting lengths increase. Since In-\\nformer uses a single time step for each token, it is necessary\\nto introduce temporal information in tokens.Methods EmbeddingTrafï¬c\\n96 192 336 720\\nFEDformerAll 0.597 0.606 0.627 0.649\\nwo/Pos. 0.587 0.604 0.621 0.626\\nwo/Temp. 0.613 0.623 0.650 0.677\\nwo/Pos.-Temp. 0.613 0.622 0.648 0.663\\nAutoformerAll 0.629 0.647 0.676 0.638\\nwo/Pos. 0.613 0.616 0.622 0.660\\nwo/Temp. 0.681 0.665 0.908 0.769\\nwo/Pos.-Temp. 0.672 0.811 1.133 1.300\\nInformerAll 0.719 0.696 0.777 0.864\\nwo/Pos. 1.035 1.186 1.307 1.472\\nwo/Temp. 0.754 0.780 0.903 1.259\\nwo/Pos.-Temp. 1.038 1.351 1.491 1.512\\nTable 6. The MSE comparisons of different embedding strate-\\ngies on Transformer-based methods with look-back window size\\n96and forecasting lengths f96;192;336;720g.\\nRather than using a single time step in each token, FED-\\nformer and Autoformer input a sequence of timestamps to\\nembed the temporal information. Hence, they can achieve\\ncomparable or even better performance without ï¬xed po-\\nsitional embeddings. However, without timestamp embed-\\ndings, the performance of Autoformer declines rapidly be-\\ncause of the loss of global temporal information. Instead,\\nthanks to the frequency-enhanced module proposed in FED-\\nformer to introduce temporal inductive bias, it suffers less\\nfrom removing any position/timestamp embeddings.\\nIs training data size a limiting factor for existing LTSF-\\nTransformers? Some may argue that the poor performance\\nof Transformer-based solutions is due to the small sizes of\\nthe benchmark datasets. Unlike computer vision or nat-\\nural language processing tasks, TSF is performed on col-\\nlected time series, and it is difï¬cult to scale up the training\\ndata size. In fact, the size of the training data would in-\\ndeed have a signiï¬cant impact on the model performance.\\nAccordingly, we conduct experiments on Trafï¬c, compar-\\ning the performance of the model trained on a full dataset\\n(17,544*0.7 hours), named Ori., with that trained on a\\nshortened dataset (8,760 hours, i.e., 1 year), called Short .\\nUnexpectedly, Table 7 presents that the prediction errors\\n7\\nwith reduced training data are lower in most cases. This\\nmight because the whole-year data maintains more clear\\ntemporal features than a longer but incomplete data size.\\nWhile we cannot conclude that we should use less data for\\ntraining, it demonstrates that the training data scale is not\\nthe limiting reason for the performances of Autoformer and\\nFEDformer.\\nMethods FEDformer Autoformer\\nDataset Ori. Short Ori. Short\\n96 0.587 0.568 0.613 0.594\\n192 0.604 0.584 0.616 0.621\\n336 0.621 0.601 0.622 0.621\\n720 0.626 0.608 0.660 0.650\\nTable 7. The MSE comparison of two training data sizes.\\nIs efï¬ciency really a top-level priority? Existing LTSF-\\nTransformers claim that the O\\nL2\\ncomplexity of the\\nvanilla Transformer is unaffordable for the LTSF problem.\\nAlthough they prove to be able to improve the theoretical\\ntime and memory complexity from O\\nL2\\ntoO(L), it is\\nunclear whether 1) the actual inference time and memory\\ncost on devices are improved, and 2) the memory issue is\\nunacceptable and urgent for todayâ€™s GPU (e.g., an NVIDIA\\nTitan XP here). In Table 8, we compare the average prac-\\ntical efï¬ciencies with 5 runs. Interestingly, compared with\\nthe vanilla Transformer (with the same DMS decoder), most\\nTransformer variants incur similar or even worse inference\\ntime and parameters in practice. These follow-ups introduce\\nmore additional design elements to make practical costs\\nhigh. Moreover, the memory cost of the vanilla Transformer\\nis practically acceptable, even for output length L= 720 ,\\nwhich weakens the importance of developing a memory-\\nefï¬cient Transformers, at least for existing benchmarks.\\nMethod MACs Parameter Time Memory\\nDLinear 0.04G 139.7K 0.4ms 687MiB\\nTransformer 4.03G 13.61M 26.8ms 6091MiB\\nInformer 3.93G 14.39M 49.3ms 3869MiB\\nAutoformer 4.41G 14.91M 164.1ms 7607MiB\\nPyraformer 0.80G 241.4M3.4ms 7017MiB\\nFEDformer 4.41G 20.68M 40.5ms 4143MiB\\n-is modiï¬ed into the same one-step decoder, which is implemented in the source code from Autoformer.\\n-236.7M parameters of Pyraformer come from its linear decoder.\\nTable 8. Comparison of practical efï¬ciency of LTSF-Transformers\\nunder L=96 and T=720 on the Electricity. MACs are the number of\\nmultiply-accumulate operations. We use Dlinear for comparison\\nsince it has the double cost in LTSF-Linear . The inference time\\naverages 5 runs.\\n6. Conclusion and Future Work\\nConclusion. This work questions the effectiveness of\\nemerging favored Transformer-based solutions for the long-\\nterm time series forecasting problem. We use an em-\\nbarrassingly simple linear model LTSF-Linear as a DMS\\nforecasting baseline to verify our claims. Note that ourcontributions do not come from proposing a linear model\\nbut rather from throwing out an important question, show-\\ning surprising comparisons, and demonstrating why LTSF-\\nTransformers are not as effective as claimed in these works\\nthrough various perspectives. We sincerely hope our com-\\nprehensive studies can beneï¬t future work in this area.\\nFuture work. LTSF-Linear has a limited model ca-\\npacity, and it merely serves a simple yet competitive base-\\nline with strong interpretability for future research. For ex-\\nample, the one-layer linear network is hard to capture the\\ntemporal dynamics caused by change points [25]. Conse-\\nquently, we believe there is a great potential for new model\\ndesigns, data processing, and benchmarks to tackle the chal-\\nlenging LTSF problem.\\n8\\nAppendix:\\nAre Transformers Effective for Time Series Forecasting?\\nIn this Appendix, we provide descriptions of non-\\nTransformer-based TSF solutions, detailed experimental\\nsettings, more comparisons under different look-back win-\\ndow sizes, and the visualization of LTSF-Linear on all\\ndatasets. We also append our code to reproduce the results\\nshown in the paper.\\nA. Related Work: Non-Transformer-Based\\nTSF Solutions\\nAs a long-standing problem with a wide range of ap-\\nplications, statistical approaches (e.g., autoregressive inte-\\ngrated moving average (ARIMA) [1], exponential smooth-\\ning [12], and structural models [14]) for time series fore-\\ncasting have been used from the 1970s onward. Generally\\nspeaking, the parametric models used in statistical methods\\nrequire signiï¬cant domain expertise to build.\\nTo relieve this burden, many machine learning\\ntechniques such as gradient boosting regression tree\\n(GBRT) [10, 11] gain popularity, which learns the tempo-\\nral dynamics of time series in a data-driven manner. How-\\never, these methods still require manual feature engineer-\\ning and model designs. With the powerful representation\\nlearning capability of deep neural networks (DNNs) from\\nabundant data, various deep learning-based TSF solutions\\nare proposed in the literature, achieving better forecasting\\naccuracy than traditional techniques in many cases.\\nBesides Transformers, the other two popular DNN archi-\\ntectures are also applied for time series forecasting:\\nâ€¢ Recurrent neural networks (RNNs) based methods\\n(e.g., [21]) summarize the past information compactly\\nin internal memory states and recursively update them-\\nselves for forecasting.\\nâ€¢ Convolutional neural networks (CNNs) based meth-\\nods (e.g., [3]), wherein convolutional ï¬lters are used\\nto capture local temporal features.\\nRNN-based TSF methods belong to IMS forecasting\\ntechniques. Depending on whether the decoder is imple-\\nmented in an autoregressive manner, there are either IMS\\nor DMS forecasting techniques for CNN-based TSF meth-\\nods [3, 17].\\nB. Experimental Details\\nB.1. Data Descriptions\\nWe use nine wildly-used datasets in the main paper. The\\ndetails are listed in the following.â€¢ ETT (Electricity Transformer Temperature) [30]2con-\\nsists of two hourly-level datasets (ETTh) and two 15-\\nminute-level datasets (ETTm). Each of them contains\\nseven oil and load features of electricity transformers\\nfrom July 2016 to July 2018 .\\nâ€¢ Trafï¬c3describes the road occupancy rates. It con-\\ntains the hourly data recorded by the sensors of San\\nFrancisco freeways from 2015 to2016 .\\nâ€¢ Electricity4collects the hourly electricity consumption\\nof321clients from 2012 to2014 .\\nâ€¢ Exchange-Rate [15]5collects the daily exchange rates\\nof8countries from 1990 to2016 .\\nâ€¢ Weather6includes 21indicators of weather, such as air\\ntemperature, and humidity. Its data is recorded every\\n10 min for 2020 in Germany.\\nâ€¢ ILI7describes the ratio of patients seen with inï¬‚uenza-\\nlike illness and the number of patients. It includes\\nweekly data from the Centers for Disease Control and\\nPrevention of the United States from 2002 to2021 .\\nB.2. Implementation Details\\nFor existing Transformer-based TSF solutions: the im-\\nplementation of Autoformer [28], Informer [30], and the\\nvanilla Transformer [26] are all taken from the Autoformer\\nwork [28]; the implementation of FEDformer [31] and\\nPyraformer [18] are from their respective code repository.\\nWe also adopt their default hyper-parameters to train the\\nmodels. For DLinear , the moving average kernel size for\\ndecomposition is 25, which is the same as Autoformer. The\\ntotal parameters of a vanilla linear model and a NLinear\\nare TL. The total parameters of the DLinear are 2TL. Since\\nLTSF-Linear will be underï¬tting when the input length is\\nshort, and LTSF-Transformers tend to overï¬t on a long\\nlookback window size. To compare the best performance\\nof existing LTSF-Transformers with LTSF-Linear , we re-\\nport L=336 for LTSF-Linear and L=96 for Transformers by\\ndefault. For more hyper-parameters of LTSF-Linear , please\\nrefer to our code.\\n2https://github.com/zhouhaoyi/ETDataset\\n3http://pems.dot.ca.gov\\n4https : / / archive . ics . uci . edu / ml / datasets /\\nElectricityLoadDiagrams20112014\\n5https : / / github . com / laiguokun / multivariate -\\ntime-series-data\\n6https://www.bgc-jena.mpg.de/wetter/\\n7https : / / gis . cdc . gov / grasp / fluview /\\nfluportaldashboard.html\\n9\\nC. Additional Comparison with Transformers\\nWe further compare LTSF-Linear with LTSF-\\nTransformer for Univariate Forecasting on four ETT\\ndatasets. Moreover, in Figure 4of the main paper, we\\ndemonstrate that existing Transformers fail to exploit large\\nlook-back window sizes with two examples. Here, we give\\ncomprehensive comparisons between LTSF-Linear and\\nTransformer-based TSF solutions under various look-back\\nwindow sizes on all benchmarks .\\nC.1. Comparison of Univariate Forecasting\\nWe present the univariate forecasting results on the four\\nETT datasets in table 9. Similarly, LTSF-Linear , especially\\nforNLinear can consistently outperform all transformer-\\nbased methods by a large margin in most time. We ï¬nd\\nthat there are serious distribution shifts between training\\nand test sets (as shown in Fig. 5 (a), (b)) on ETTh1 and\\nETTh2 datasets. Simply normalization via the last value\\nfrom the lookback window can greatly relieve the distribu-\\ntion shift problem.\\nC.2. Comparison under Different Look-back Win-\\ndows\\nIn Figure 6, we provide the MSE comparisons of ï¬ve\\nLTSF-Transformers with LTSF-Linear under different look-\\nback window sizes to explore whether existing Transform-\\ners can extract temporal well from longer input sequences.\\nFor hourly granularity datasets (ETTh1, ETTh2, Trafï¬c,\\nand Electricity), the increasing look-back window sizes are\\n{24, 48, 72, 96, 120, 144, 168, 192, 336, 504, 672, 720},\\nwhich represent {1, 2, 3, 4, 5, 6, 7, 8, 14, 21, 28, 30}\\ndays. The forecasting steps are {24, 720}, which mean {1,\\n30} days. For 5-minute granularity datasets (ETTm1 and\\nETTm2), we set the look-back window size as {24, 36, 48,\\n60, 72, 144, 288}, which represent {2, 3, 4, 5, 6, 12, 24}\\nhours. For 10-minute granularity datasets (Weather), we set\\nthe look-back window size as {24, 48, 72, 96, 120, 144,\\n168, 192, 336, 504, 672, 720}, which mean {4, 8, 12, 16,\\n20, 24, 28, 32, 56, 84, 112, 120} hours. The forecasting\\nsteps are {24, 720} that are {4, 120} hours. For weekly\\ngranularity dataset (ILI), we set the look-back window size\\nas {26, 52, 78, 104, 130, 156, 208}, which represent {0.5, 1,\\n1.5, 2, 2.5, 3, 3.5, 4} years. The corresponding forecasting\\nsteps are {26, 208}, meaning {0.5, 4} years.\\nAs shown in Figure 6, with increased look-back win-\\ndow sizes, the performance of LTSF-Linear is signiï¬cantly\\nboosted for most datasets (e.g., ETTm1 and Trafï¬c), while\\nthis is not the case for Transformer-based TSF solutions.\\nMost of their performance ï¬‚uctuates or gets worse as\\nthe input lengths increase. To be speciï¬c, the results of\\nExchange-Rate do not show improved results with a long\\nlook-back window (from Figure 6(m) and (n)), and we at-tribute it to the low information-to-noise ratio in such ï¬nan-\\ncial data.\\nD. Ablation study on the LTSF-Linear\\nD.1. Motivation of NLinear\\nIf we normalize the test data by the mean and variance of\\ntrain data, there could be a distribution shift in testing data,\\ni.e, the mean value of testing data is not 0. If the model\\nmade a prediction that is out of the distribution of true value,\\na large error would occur. For example, there is a large er-\\nror between the true value and the true value minus/add one.\\nTherefore, in NLinear , we use the subtraction and addition\\nto shift the model prediction toward the distribution of true\\nvalue. Then, large errors are avoided, and the model perfor-\\nmances can be improved. Figure 5 illustrates histograms of\\nthe trainset-test set distributions, where each bar represents\\nthe number of data points. Clear distribution shifts between\\ntraining and testing data can be observed in ETTh1, ETTh2,\\nand ILI. Accordingly, from Table 9 and Table 2 in the main\\npaper, we can observe that there are great improvements\\nin the three datasets comparing the NLinear to the Linear ,\\nshowing the effectiveness of the NLinear in relieving dis-\\ntribution shifts. Moreover, for the datasets without obvi-\\nous distribution shifts, like Electricity in Figure 5(c), using\\nthe vanilla Linear can be enough, demonstrating the similar\\nperformance with NLinear andDLinear .\\nD.2. The Features of LTSF-Linear\\nAlthough LTSF-Linear is simple, it has some compelling\\ncharacteristics:\\nâ€¢AnO(1)maximum signal traversing path length :\\nThe shorter the path, the better the dependencies are\\ncaptured [18], making LTSF-Linear capable of cap-\\nturing both short-range and long-range temporal rela-\\ntions.\\nâ€¢High-efï¬ciency: AsLTSF-Linear is a linear model\\nwith two linear layers at most, it costs much lower\\nmemory and fewer parameters and has a faster infer-\\nence speed than existing Transformers (see Table 8in\\nmain paper).\\nâ€¢Interpretability: After training, we can visualize\\nweights from the seasonality and trend branches to\\nhave some insights on the predicted values [9].\\nâ€¢Easy-to-use: LTSF-Linear can be obtained easily\\nwithout tuning model hyper-parameters.\\nD.3. Interpretability of LTSF-Linear\\nBecause LTSF-Linear is a set of linear models, the\\nweights of linear layers can directly reveal how LTSF-\\nLinear works. The weight visualization of LTSF-Linear can\\n10\\nMethods Linear NLinear DLinear FEDformer-f FEDformer-w Autoformer Informer LogTrans\\nMetric MSE MAE MSE MSE MAE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAEETTh 196 0.189 0.359 0.053 0.177 0.056 0.180 0.079 0.215 0.080 0.214 0.071 0.206 0.193 0.377 0.283 0.468\\n192 0.078 0.212 0.069 0.204 0.071 0.204 0.104 0.245 0.105 0.256 0.114 0.262 0.217 0.395 0.234 0.409\\n336 0.091 0.237 0.081 0.226 0.098 0.244 0.119 0.270 0.120 0.269 0.107 0.258 0.202 0.381 0.386 0.546\\n720 0.172 0.340 0.080 0.226 0.189 0.359 0.142 0.299 0.127 0.280 0.126 0.283 0.183 0.355 0.475 0.629ETTh 296 0.133 0.283 0.129 0.278 0.131 0.279 0.128 0.271 0.156 0.306 0.153 0.306 0.213 0.373 0.217 0.379\\n192 0.176 0.330 0.169 0.324 0.176 0.329 0.185 0.330 0.238 0.380 0.204 0.351 0.227 0.387 0.281 0.429\\n336 0.213 0.371 0.194 0.355 0.209 0.367 0.231 0.378 0.271 0.412 0.246 0.389 0.242 0.401 0.293 0.437\\n720 0.292 0.440 0.225 0.381 0.276 0.426 0.278 0.420 0.288 0.438 0.268 0.409 0.291 0.439 0.218 0.387ETTm 196 0.028 0.125 0.026 0.122 0.028 0.123 0.033 0.140 0.036 0.149 0.056 0.183 0.109 0.277 0.049 0.171\\n192 0.043 0.154 0.039 0.149 0.045 0.156 0.058 0.186 0.069 0.206 0.081 0.216 0.151 0.310 0.157 0.317\\n336 0.059 0.180 0.052 0.172 0.061 0.182 0.084 0.231 0.071 0.209 0.076 0.218 0.427 0.591 0.289 0.459\\n720 0.080 0.211 0.073 0.207 0.080 0.210 0.102 0.250 0.105 0.248 0.110 0.267 0.438 0.586 0.430 0.579ETTm 296 0.066 0.189 0.063 0.182 0.063 0.183 0.067 0.198 0.063 0.189 0.065 0.189 0.088 0.225 0.075 0.208\\n192 0.094 0.230 0.090 0.223 0.092 0.227 0.102 0.245 0.110 0.252 0.118 0.256 0.132 0.283 0.129 0.275\\n336 0.120 0.263 0.117 0.259 0.119 0.261 0.130 0.279 0.147 0.301 0.154 0.305 0.180 0.336 0.154 0.302\\n720 0.175 0.320 0.170 0.318 0.175 0.320 0.178 0.325 0.219 0.368 0.182 0.335 0.300 0.435 0.160 0.321\\nTable 9. Univariate long sequence time-series forecasting results on ETT full benchmark. The best results are highlighted in bold and the\\nbest results of Transformers are highlighted with a underline .\\n(a) ETTh1 channel6\\n(b) ETTh2 channel3\\n(c) Electricity channel3\\n(d) ILI channel6\\nFigure 5. Distribution of ETTh1, ETTh2, Electricity, and ILI dataset. A clear distribution shift between training and testing data can be\\nobserved in ETTh1, ETTh2, and ILI.\\nalso reveal certain characteristics in the data used for fore-\\ncasting.\\nHere we take DLinear as an example. Accordingly, we\\nvisualize the trend and remainder weights of all datasets\\nwith a ï¬xed input length of 96 and four different forecasting\\nhorizons. To obtain a smooth weight with a clear pattern in\\nvisualization, we initialize the weights of the linear layers\\nin DLinear as 1=Lrather than random initialization. That\\nis, we use the same weight for every forecasting time step\\nin the look-back window at the start of training.\\nHow the model works: Figure 7(c) visualize the\\nweights of the trend and the remaining layers on the\\nExchange-Rate dataset. Due to the lack of periodicity and\\nseasonality in ï¬nancial data, it is hard to observe clear pat-\\nterns, but the trend layer reveals greater weights of informa-\\ntion closer to the outputs, representing their larger contribu-\\ntions to the predicted values.\\nPeriodicity of data: For Trafï¬c data, as shown in Fig-\\nure 7(d), the model gives high weights to the latest time\\nstep of the look-back window for the 0,23,47...719 forecast-ing steps. Among these forecasting time steps, the 0, 167,\\n335, 503, 671 time steps have higher weights. Note that 24\\ntime steps are a day, and 168 time steps are a week. This\\nindicates that Trafï¬c has a daily periodicity and a weekly\\nperiodicity.\\nReferences\\n[1] Adebiyi A Ariyo, Adewumi O Adewumi, and\\nCharles K Ayo. Stock price prediction using the arima\\nmodel. In 2014 UKSim-AMSS 16th International Con-\\nference on Computer Modelling and Simulation , pages\\n106â€“112. IEEE, 2014. 1, 2, 9\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua\\nBengio. Neural machine translation by jointly learn-\\ning to align and translate. arXiv: Computation and\\nLanguage , 2014. 2\\n[3] Shaojie Bai, J Zico Kolter, and Vladlen Koltun.\\nAn empirical evaluation of generic convolutional and\\n11\\n24 48 72 96 120 144 168 192 336 504 672 7200.30.40.50.60.70.80.91.0\\nTransformer\\nInformerAutoformer\\nFEDformerPyraformer\\nLinearNLinear\\nDLinear(a)24steps- ETTh1\\n24 48 72 96 120 144 168 192 336 504 672 7200.40.60.81.01.21.4\\nTransformer\\nInformerAutoformer\\nFEDformerPyraformer\\nLinearNLinear\\nDLinear (b)720steps- ETTh1\\n24 48 72 96 120 144 168 192 336 504 672 7200.20.40.60.81.01.2\\nTransformer\\nInformerAutoformer\\nFEDformerPyraformer\\nLinearNLinear\\nDLinear (c)24steps- ETTh2\\n24 48 72 96 120 144 168 192 336 504 672 72012345\\nTransformer\\nInformerAutoformer\\nFEDformerPyraformer\\nLinearNLinear\\nDLinear (d)720steps- ETTh2\\n24 36 48 60 72 144 2880.20.30.40.50.6\\nTransformer\\nInformerAutoformer\\nFEDformerPyraformer\\nLinearNLinear\\nDLinear\\n(e)24steps- ETTm1\\n24 36 48 60 72 144 2880.40.50.60.70.80.91.01.11.2\\nTransformer\\nInformerAutoformer\\nFEDformerPyraformer\\nLinearNLinear\\nDLinear (f)576steps- ETTm1\\n24 36 48 60 72 144 2880.100.150.200.250.30\\nTransformer\\nInformerAutoformer\\nFEDformerPyraformer\\nLinearNLinear\\nDLinear (g)24steps- ETTm2\\n24 36 48 60 72 144 2881234\\nTransformer\\nInformerAutoformer\\nFEDformerPyraformer\\nLinearNLinear\\nDLinear (h)576steps- ETTm2\\n24 48 72 96 120 144 168 192 336 504 672 7200.100.150.200.250.300.350.400.450.50\\nTransformer\\nInformerAutoformer\\nFEDformerPyraformer\\nLinearNLinear\\nDLinear\\n(i)24steps- Weather\\n24 48 72 96 120 144 168 192 336 504 672 7200.40.60.81.01.21.41.6\\nTransformer\\nInformerAutoformer\\nFEDformerPyraformer\\nLinearNLinear\\nDLinear (j)720steps- Weather\\n24 48 72 96 120 144 168 192 336 504 672 7200.40.50.60.70.80.9\\nTransformer\\nInformerAutoformer\\nFEDformerPyraformer\\nLinearNLinear\\nDLinear (k)24steps- Trafï¬c\\n24 48 72 96 120 144 168 192 336 504 672 7200.40.60.81.01.21.4\\nTransformer\\nInformerAutoformer\\nFEDformerPyraformer\\nLinearNLinear\\nDLinear (l)720steps- Trafï¬c\\n24 48 72 96 120 144 168 192 336 504 672 7200.00.20.40.60.81.01.21.4\\nTransformer\\nInformerAutoformer\\nFEDformerPyraformer\\nLinearNLinear\\nDLinear\\n(m)24steps- Exchange\\n24 48 72 96 120 144 168 192 336 504 672 7200.51.01.52.02.53.0\\nTransformer\\nInformerAutoformer\\nFEDformerPyraformer\\nLinearNLinear\\nDLinear (n)720steps- Exchange\\n26 52 78 104 130 156 208234567\\nTransformer\\nInformerAutoformer\\nFEDformerPyraformer\\nLinearNLinear\\nDLinear (o)24steps- ILI\\n26 52 78 104 130 156 2082345678\\nTransformer\\nInformerAutoformer\\nFEDformerPyraformer\\nLinearNLinear\\nDLinear (p)60steps- ILI\\nFigure 6. The MSE results (Y-axis) of models with different look-back window sizes (X-axis) of the long-term forecasting (e.g., 720-time\\nsteps) and the short-term forecasting (e.g., 24time steps) on different benchmarks.\\nrecurrent networks for sequence modeling. arXiv\\npreprint arXiv:1803.01271 , 2018. 1, 9\\n[4] Guillaume Chevillon. Direct multi-step estimation\\nand forecasting. Journal of Economic Surveys ,\\n21(4):746â€“785, 2007. 2\\n[5] Razvan-Gabriel Cirstea, Chenjuan Guo, Bin Yang,\\nTung Kieu, Xuanyi Dong, and Shirui Pan. Triformer:Triangular, variable-speciï¬c attentions for long se-\\nquence multivariate time series forecastingâ€“full ver-\\nsion. arXiv preprint arXiv:2204.13767 , 2022. 1\\n[6] R. B. Cleveland. Stl : A seasonal-trend decomposition\\nprocedure based on loess. Journal of Ofï¬ce Statistics ,\\n1990. 3\\n[7] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\\n12\\nIn-96, Out-96(b1) Remainder(b2) TrendIn-96, Out-192(b3) Remainder(b4) TrendIn-96, Out-336(b5) Remainder(b6) TrendIn-96, Out-720(b7) Remainder(b8) TrendElectricity\\nIn-96, Out-96(c1) Remainder(c2) TrendIn-96, Out-192(c3) Remainder(c4) TrendIn-96, Out-336(c5) Remainder(c6) Trend(c7) Remainder(c8) TrendExchange-Rate\\nIn-36, Out-24(f1) Remainder(f2) TrendIn-36, Out-36(f4) TrendIn-36, Out-48(f5) Remainder(f6) Trend(f7) Remainder(f8) TrendILI\\n(f3) RemainderIn-96, Out-96(d1) Remainder(d2) TrendIn-96, Out-192(d3) Remainder(d4) TrendIn-96, Out-336(d5) Remainder(d6) Trend(d7) Remainder(d8) TrendTraffic\\nIn-96, Out-96(e1) Remainder(e2) Trend\\nIn-96, Out-192(e3) Remainder(e4) TrendIn-96, Out-336(e5) Remainder(e6) Trend(e7) Remainder(e8) TrendWeatherIn-96, Out-720\\nIn-36, Out-60In-96, Out-720\\nIn-96, Out-720In-96, Out-96(a1) Remainder(a2) TrendIn-96, Out-168(a3) Remainder(a4) TrendIn-96, Out-336(a5) Remainder(a6) TrendIn-96, Out-720(a7) Remainder(a8) TrendETTh1\\nFigure 7. Visualization of the weights(T*L) of LTSF-Linear on several benchmarks. Models are trained with a look-back window L\\n(X-axis) and different forecasting time steps T (Y-axis). We show weights in the remainder and trend layer.\\nKristina Toutanova. Bert: Pre-training of deep bidirec- tional transformers for language understanding. arXiv\\n13\\npreprint arXiv:1810.04805 , 2018. 1\\n[8] Linhao Dong, Shuang Xu, and Bo Xu. Speech-\\ntransformer: a no-recurrence sequence-to-sequence\\nmodel for speech recognition. In 2018 IEEE Inter-\\nnational Conference on Acoustics, Speech and Signal\\nProcessing (ICASSP) , pages 5884â€“5888. IEEE, 2018.\\n1\\n[9] Ruijun Dong and Witold Pedrycz. A granular time se-\\nries approach to long-term forecasting and trend fore-\\ncasting. Physica A: Statistical Mechanics and its Ap-\\nplications , 387(13):3253â€“3270, 2008. 10\\n[10] Shereen Elsayed, Daniela Thyssens, Ahmed Rashed,\\nHadi Samer Jomaa, and Lars Schmidt-Thieme. Do\\nwe really need deep learning models for time series\\nforecasting? arXiv preprint arXiv:2101.02118 , 2021.\\n9\\n[11] Jerome H Friedman. Greedy function approximation:\\na gradient boosting machine. Annals of statistics ,\\npages 1189â€“1232, 2001. 1, 9\\n[12] Everette S Gardner Jr. Exponential smoothing: The\\nstate of the art. Journal of forecasting , 4(1):1â€“28,\\n1985. 9\\n[13] James Douglas Hamilton. Time series analysis .\\nPrinceton university press, 2020. 3\\n[14] Andrew C Harvey. Forecasting, structural time series\\nmodels and the kalman ï¬lter. 1990. 9\\n[15] Guokun Lai, Wei-Cheng Chang, Yiming Yang, and\\nHanxiao Liu. Modeling long- and short-term tempo-\\nral patterns with deep neural networks. international\\nacm sigir conference on research and development in\\ninformation retrieval , 2017. 1, 4, 9\\n[16] Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou,\\nWenhu Chen, Yu-Xiang Wang, and Xifeng Yan. En-\\nhancing the locality and breaking the memory bottle-\\nneck of transformer on time series forecasting. Ad-\\nvances in Neural Information Processing Systems , 32,\\n2019. 1, 2, 3, 4\\n[17] Minhao Liu, Ailing Zeng, Zhijian Xu, Qiuxia Lai, and\\nQiang Xu. Time series is a special sequence: Fore-\\ncasting with sample convolution and interaction. arXiv\\npreprint arXiv:2106.09305 , 2021. 1, 9\\n[18] Shizhan Liu, Hang Yu, Cong Liao, Jianguo Li, Weiyao\\nLin, Alex X Liu, and Schahram Dustdar. Pyraformer:\\nLow-complexity pyramidal attention for long-range\\ntime series modeling and forecasting. In International\\nConference on Learning Representations , 2021. 1, 2,\\n3, 4, 9, 10\\n[19] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei,\\nZheng Zhang, Stephen Lin, and Baining Guo. Swin\\ntransformer: Hierarchical vision transformer usingshifted windows. In Proceedings of the IEEE/CVF\\nInternational Conference on Computer Vision , pages\\n10012â€“10022, 2021. 1\\n[20] LIU Minhao, Ailing Zeng, LAI Qiuxia, Ruiyuan Gao,\\nMin Li, Jing Qin, and Qiang Xu. T-wavenet: A tree-\\nstructured wavelet neural network for time series sig-\\nnal analysis. In International Conference on Learning\\nRepresentations , 2021. 2\\n[21] GÃ¡bor PetnehÃ¡zi. Recurrent neural networks for time\\nseries forecasting. arXiv preprint arXiv:1901.00069 ,\\n2019. 9\\n[22] David Salinas, Valentin Flunkert, and Jan Gasthaus.\\nDeepar: Probabilistic forecasting with autoregressive\\nrecurrent networks. International Journal of Forecast-\\ning, 2017. 2\\n[23] Souhaib Ben Taieb, Rob J Hyndman, et al. Recur-\\nsive and direct multi-step forecasting: the best of both\\nworlds , volume 19. Citeseer, 2012. 2\\n[24] Sean J. Taylor and Benjamin Letham. Forecasting at\\nscale. PeerJ Prepr. , 2017. 2\\n[25] Gerrit JJ van den Burg and Christopher KI Williams.\\nAn evaluation of change point detection algorithms.\\narXiv preprint arXiv:2003.06222 , 2020. 8\\n[26] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\\nUszkoreit, Llion Jones, Aidan N Gomez, Åukasz\\nKaiser, and Illia Polosukhin. Attention is all you need.\\nAdvances in neural information processing systems ,\\n30, 2017. 1, 2, 9\\n[27] Qingsong Wen, Tian Zhou, Chaoli Zhang, Weiqi\\nChen, Ziqing Ma, Junchi Yan, and Liang Sun. Trans-\\nformers in time series: A survey. arXiv preprint\\narXiv:2202.07125 , 2022. 1, 2, 5\\n[28] Jiehui Xu, Jianmin Wang, Mingsheng Long, et al.\\nAutoformer: Decomposition transformers with auto-\\ncorrelation for long-term series forecasting. Advances\\nin Neural Information Processing Systems , 34, 2021.\\n1, 2, 3, 4, 5, 9\\n[29] Ailing Zeng, Xuan Ju, Lei Yang, Ruiyuan Gao,\\nXizhou Zhu, Bo Dai, and Qiang Xu. Deciwatch: A\\nsimple baseline for 10x efï¬cient 2d and 3d pose esti-\\nmation. arXiv preprint arXiv:2203.08713 , 2022. 1\\n[30] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai\\nZhang, Jianxin Li, Hui Xiong, and Wancai Zhang.\\nInformer: Beyond efï¬cient transformer for long se-\\nquence time-series forecasting. In The Thirty-Fifth\\nAAAI Conference on Artiï¬cial Intelligence, AAAI\\n2021, Virtual Conference , volume 35, pages 11106â€“\\n11115. AAAI Press, 2021. 1, 2, 3, 4, 5, 9\\n[31] Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang,\\nLiang Sun, and Rong Jin. Fedformer: Frequency en-\\nhanced decomposed transformer for long-term series\\n14\\nforecasting. In International Conference on Machine\\nLearning , 2022. 1, 2, 3, 4, 5, 9\\n15'}, 'type': 'text'}][{\"url\": \"https://arxiv.org/pdf/2205.13504\", \"content\": \"Are Transformers Effective for Time Series Forecasting?\\nAiling Zeng1*, Muxi Chen1*, Lei Zhang2, Qiang Xu1\\n1The Chinese University of Hong Kong\\n2International Digital Economy Academy (IDEA)\\n{alzeng, mxchen21, qxu}@cse.cuhk.edu.hk\\n{leizhang}@idea.edu.cn\\nAbstract\\nRecently, there has been a surge of Transformer-based\\nsolutions for the long-term time series forecasting (LTSF)\\ntask. Despite the growing performance over the past few\\nyears, we question the validity of this line of research in this\\nwork . Speciï¬cally, Transformers is arguably the most suc-\\ncessful solution to extract the semantic correlations among\\nthe elements in a long sequence. However, in time series\\nmodeling, we are to extract the temporal relations in an\\nordered set of continuous points . While employing posi-\\ntional encoding and using tokens to embed sub-series in\\nTransformers facilitate preserving some ordering informa-\\ntion, the nature of the permutation-invariant self-attention\\nmechanism inevitably results in temporal information loss.\\nTo validate our claim, we introduce a set of embarrass-\\ningly simple one-layer linear models named LTSF-Linear\\nfor comparison. Experimental results on nine real-life\\ndatasets show that LTSF-Linear surprisingly outperforms\\nexisting sophisticated Transformer-based LTSF models in\\nall cases, and often by a large margin. Moreover, we con-\\nduct comprehensive empirical studies to explore the im-\\npacts of various design elements of LTSF models on their\\ntemporal relation extraction capability. We hope this sur-\\nprising ï¬nding opens up new research directions for the\\nLTSF task. We also advocate revisiting the validity of\\nTransformer-based solutions for other time series analysis\\ntasks (e.g., anomaly detection) in the future. Code is avail-\\nable at: https://github.com/cure-lab/LTSF-\\nLinear .\\n1. Introduction\\nTime series are ubiquitous in todayâ€™s data-driven world.\\nGiven historical data, time series forecasting (TSF) is a\\nlong-standing task that has a wide range of applications,\\nincluding but not limited to trafï¬c ï¬‚ow estimation, en-\\n*Equal contributionergy management, and ï¬nancial investment. Over the past\\nseveral decades, TSF solutions have undergone a progres-\\nsion from traditional statistical methods (e.g., ARIMA [1])\\nand machine learning techniques (e.g., GBRT [11]) to\\ndeep learning-based solutions, e.g., Recurrent Neural Net-\\nworks [15] and Temporal Convolutional Networks [3, 17].\\nTransformer [26] is arguably the most successful se-\\nquence modeling architecture, demonstrating unparalleled\\nperformances in various applications, such as natural lan-\\nguage processing (NLP) [7], speech recognition [8], and\\ncomputer vision [19, 29]. Recently, there has also been a\\nsurge of Transformer-based solutions for time series anal-\\nysis, as surveyed in [27]. Most notable models, which\\nfocus on the less explored and challenging long-term time\\nseries forecasting (LTSF) problem, include LogTrans [16]\\n(NeurIPS 2019), Informer [30] (AAAI 2021 Best paper),\\nAutoformer [28] (NeurIPS 2021), Pyraformer [18] (ICLR\\n2022 Oral), Triformer [5] (IJCAI 2022) and the recent FED-\\nformer [31] (ICML 2022).\\nThe main working power of Transformers is from its\\nmulti-head self-attention mechanism, which has a remark-\\nable capability of extracting semantic correlations among\\nelements in a long sequence (e.g., words in texts or 2D\\npatches in images). However, self-attention is permutation-\\ninvariant and â€œanti-orderâ€ to some extent. While using var-\\nious types of positional encoding techniques can preserve\\nsome ordering information, it is still inevitable to have tem-\\nporal information loss after applying self-attention on top\\nof them. This is usually not a serious concern for semantic-\\nrich applications such as NLP, e.g., the semantic meaning\\nof a sentence is largely preserved even if we reorder some\\nwords in it. However, when analyzing time series data,\\nthere is usually a lack of semantics in the numerical data\\nitself, and we are mainly interested in modeling the tempo-\\nral changes among a continuous set of points . That is, the\\norder itself plays the most crucial role. Consequently, we\\npose the following intriguing question: Are Transformers\\nreally effective for long-term time series forecasting?\\nMoreover, while existing Transformer-based LTSF so-\\n1arXiv:2205.13504v3 [cs.AI] 17 Aug 2022\\nlutions have demonstrated considerable prediction accu-\\nracy improvements over traditional methods, in their exper-\\niments, all the compared (non-Transformer) baselines per-\\nform autoregressive or iterated multi-step (IMS) forecast-\\ning [1,2,22,24], which are known to suffer from signiï¬cant\\nerror accumulation effects for the LTSF problem. There-\\nfore, in this work, we challenge Transformer-based LTSF\\nsolutions with direct multi-step (DMS) forecasting strate-\\ngies to validate their real performance.\\nNot all time series are predictable, let alone long-term\\nforecasting (e.g., for chaotic systems). We hypothesize that\\nlong-term forecasting is only feasible for those time series\\nwith a relatively clear trend and periodicity. As linear mod-\\nels can already extract such information, we introduce a set\\nof embarrassingly simple models named LTSF-Linear as a\\nnew baseline for comparison. LTSF-Linear regresses histor-\\nical time series with a one-layer linear model to forecast fu-\\nture time series directly. We conduct extensive experiments\\non nine widely-used benchmark datasets that cover various\\nreal-life applications: trafï¬c, energy, economics, weather,\\nand disease predictions. Surprisingly, our results show that\\nLTSF-Linear outperforms existing complex Transformer-\\nbased models in all cases, and often by a large margin (20%\\n50%) . Moreover, we ï¬nd that, in contrast to the claims in\\nexisting Transformers, most of them fail to extract temporal\\nrelations from long sequences, i.e., the forecasting errors are\\nnot reduced (sometimes even increased) with the increase of\\nlook-back window sizes. Finally, we conduct various abla-\\ntion studies on existing Transformer-based TSF solutions to\\nstudy the impact of various design elements in them.\\nTo sum up, the contributions of this work include:\\nâ€¢ To the best of our knowledge, this is the ï¬rst work to\\nchallenge the effectiveness of the booming Transform-\\ners for the long-term time series forecasting task.\\nâ€¢ To validate our claims, we introduce a set of em-\\nbarrassingly simple one-layer linear models, named\\nLTSF-Linear , and compare them with existing\\nTransformer-based LTSF solutions on nine bench-\\nmarks. LTSF-Linear can be a new baseline for the\\nLTSF problem.\\nâ€¢ We conduct comprehensive empirical studies on var-\\nious aspects of existing Transformer-based solutions,\\nincluding the capability of modeling long inputs, the\\nsensitivity to time series order, the impact of posi-\\ntional encoding and sub-series embedding, and efï¬-\\nciency comparisons. Our ï¬ndings would beneï¬t future\\nresearch in this area.\\nWith the above, we conclude that the temporal model-\\ning capabilities of Transformers for time series are exag-\\ngerated, at least for the existing LTSF benchmarks . At the\\nsame time, while LTSF-Linear achieves a better predictionaccuracy compared to existing works, it merely serves as a\\nsimple baseline for future research on the challenging long-\\nterm TSF problem. With our ï¬ndings, we also advocate\\nrevisiting the validity of Transformer-based solutions for\\nother time series analysis tasks in the future.\\n2. Preliminaries: TSF Problem Formulation\\nFor time series containing Cvariates, given historical\\ndataX=fXt\\n1; :::; Xt\\nCgL\\nt=1, wherein Lis the look-back\\nwindow size and Xt\\niis the value of the ithvariate at the tth\\ntime step. The time series forecasting task is to predict the\\nvalues ^X=f^Xt\\n1; :::;^Xt\\nCgL+T\\nt=L+1at the Tfuture time steps.\\nWhen T > 1, iterated multi-step (IMS) forecasting [23]\\nlearns a single-step forecaster and iteratively applies it to\\nobtain multi-step predictions. Alternatively, direct multi-\\nstep (DMS) forecasting [4] directly optimizes the multi-step\\nforecasting objective at once.\\nCompared to DMS forecasting results, IMS predictions\\nhave smaller variance thanks to the autoregressive estima-\\ntion procedure, but they inevitably suffer from error accu-\\nmulation effects. Consequently, IMS forecasting is prefer-\\nable when there is a highly-accurate single-step forecaster,\\nandTis relatively small. In contrast, DMS forecasting gen-\\nerates more accurate predictions when it is hard to obtain an\\nunbiased single-step forecasting model, or Tis large.\\n3. Transformer-Based LTSF Solutions\\nTransformer-based models [26] have achieved unparal-\\nleled performances in many long-standing AI tasks in natu-\\nral language processing and computer vision ï¬elds, thanks\\nto the effectiveness of the multi-head self-attention mech-\\nanism. This has also triggered lots of research interest\\nin Transformer-based time series modeling techniques [20,\\n27]. In particular, a large amount of research works are ded-\\nicated to the LTSF task (e.g., [16, 18, 28, 30, 31]). Con-\\nsidering the ability to capture long-range dependencies\\nwith Transformer models, most of them focus on the less-\\nexplored long-term forecasting problem ( T1)1.\\nWhen applying the vanilla Transformer model to the\\nLTSF problem, it has some limitations, including the\\nquadratic time/memory complexity with the original self-\\nattention scheme and error accumulation caused by the au-\\ntoregressive decoder design. Informer [30] addresses these\\nissues and proposes a novel Transformer architecture with\\nreduced complexity and a DMS forecasting strategy. Later,\\nmore Transformer variants introduce various time series\\nfeatures into their models for performance or efï¬ciency im-\\nprovements [18,28,31]. We summarize the design elements\\nof existing Transformer-based LTSF solutions as follows\\n(see Figure 1).\\n1Due to page limit, we leave the discussion of non-Transformer fore-\\ncasting solutions in the Appendix.\\n2\\n(d) Decoder (c) Encoder (b) Embedding (a) Preprocessing\\nForecasting Output\\n(d) Decoder (c) Encoder (b) Embedding (a) Preprocessing\\nOutput\\nInput\\nSeasonal-trend\\ndecompositionNormalization\\nTimestamp\\npreparationChannel projection\\nFixed position\\nLocal timestamp\\nGlobal timestampProbSparseand distilling\\nself-attention @ Informer\\nSeries auto-correlation with\\ndecomposition @ Autoformer\\nMulti-resolution pyramidal\\nattention @ Pyraformer\\nFrequency enhanced block with\\ndecomposition @ FEDformerLogSparse and convolutional\\nself-attention @ LogTrans\\nDirect Multi-Step\\n(DMS) @ Informer\\nDMS with auto-correlation and\\ndecomposition @ Autoformer\\nDMS along spatio-temporal\\ndimension @ Pyraformer\\nDMS with frequency attention\\nanddecomposition@ FEDformerIterated Multi-Step\\n(IMS) @LogTransFigure 1. The pipeline of existing Transformer-based TSF solutions. In (a) and (b), the solid boxes are essential operations, and the dotted\\nboxes are applied optionally. (c) and (d) are distinct for different methods [16, 18, 28, 30, 31].\\nTime series decomposition: For data preprocessing, nor-\\nmalization with zero-mean is common in TSF. Besides,\\nAutoformer [28] ï¬rst applies seasonal-trend decomposition\\nbehind each neural block, which is a standard method in\\ntime series analysis to make raw data more predictable [6,\\n13]. Speciï¬cally, they use a moving average kernel on the\\ninput sequence to extract the trend-cyclical component of\\nthe time series. The difference between the original se-\\nquence and the trend component is regarded as the seasonal\\ncomponent. On top of the decomposition scheme of Aut-\\noformer, FEDformer [31] further proposes the mixture of\\nexpertsâ€™ strategies to mix the trend components extracted\\nby moving average kernels with various kernel sizes.\\nInput embedding strategies: The self-attention layer in\\nthe Transformer architecture cannot preserve the positional\\ninformation of the time series. However, local positional\\ninformation, i.e. the ordering of time series, is important.\\nBesides, global temporal information, such as hierarchical\\ntimestamps (week, month, year) and agnostic timestamps\\n(holidays and events), is also informative [30]. To enhance\\nthe temporal context of time-series inputs, a practical design\\nin the SOTA Transformer-based methods is injecting sev-\\neral embeddings, like a ï¬xed positional encoding, a channel\\nprojection embedding, and learnable temporal embeddings\\ninto the input sequence. Moreover, temporal embeddings\\nwith a temporal convolution layer [16] or learnable times-\\ntamps [28] are introduced.\\nSelf-attention schemes: Transformers rely on the self-\\nattention mechanism to extract the semantic dependen-\\ncies between paired elements. Motivated by reducing\\ntheO\\nL2\\ntime and memory complexity of the vanilla\\nTransformer, recent works propose two strategies for ef-\\nï¬ciency. On the one hand, LogTrans and Pyraformer\\nexplicitly introduce a sparsity bias into the self-attention\\nscheme. Speciï¬cally, LogTrans uses a Logsparse mask to\\nreduce the computational complexity to O(LlogL )while\\nPyraformer adopts pyramidal attention that captures hierar-\\nchically multi-scale temporal dependencies with an O(L)\\ntime and memory complexity. On the other hand, In-\\nformer and FEDformer use the low-rank property in the\\nself-attention matrix. Informer proposes a ProbSparse self-attention mechanism and a self-attention distilling operation\\nto decrease the complexity to O(LlogL ), and FEDformer\\ndesigns a Fourier enhanced block and a wavelet enhanced\\nblock with random selection to obtain O(L)complexity.\\nLastly, Autoformer designs a series-wise auto-correlation\\nmechanism to replace the original self-attention layer.\\nDecoders: The vanilla Transformer decoder outputs se-\\nquences in an autoregressive manner, resulting in a slow in-\\nference speed and error accumulation effects, especially for\\nlong-term predictions. Informer designs a generative-style\\ndecoder for DMS forecasting. Other Transformer variants\\nemploy similar DMS strategies. For instance, Pyraformer\\nuses a fully-connected layer concatenating Spatio-temporal\\naxes as the decoder. Autoformer sums up two reï¬ned de-\\ncomposed features from trend-cyclical components and the\\nstacked auto-correlation mechanism for seasonal compo-\\nnents to get the ï¬nal prediction. FEDformer also uses a\\ndecomposition scheme with the proposed frequency atten-\\ntion block to decode the ï¬nal results.\\nThe premise of Transformer models is the semantic cor-\\nrelations between paired elements, while the self-attention\\nmechanism itself is permutation-invariant, and its capabil-\\nity of modeling temporal relations largely depends on posi-\\ntional encodings associated with input tokens. Considering\\nthe raw numerical data in time series (e.g., stock prices or\\nelectricity values), there are hardly any point-wise semantic\\ncorrelations between them. In time series modeling, we are\\nmainly interested in the temporal relations among a contin-\\nuous set of points, and the order of these elements instead\\nof the paired relationship plays the most crucial role. While\\nemploying positional encoding and using tokens to embed\\nsub-series facilitate preserving some ordering information,\\nthe nature of the permutation-invariant self-attention mech-\\nanism inevitably results in temporal information loss. Due\\nto the above observations, we are interested in revisiting the\\neffectiveness of Transformer-based LTSF solutions.\\n4. An Embarrassingly Simple Baseline\\nIn the experiments of existing Transformer-based LTSF\\nsolutions ( T1), all the compared (non-Transformer)\\n3\\nbaselines are IMS forecasting techniques, which are known\\nto suffer from signiï¬cant error accumulation effects. We\\nhypothesize that the performance improvements in these\\nworks are largely due to the DMS strategy used in them.\\nLinear ğ‘Š ï¿½\\nLinear ğ‘Š ï¿½Look-back WindowRemainder\\nTrendForecasting Output\\nğ‘‹ï¿½âˆˆ â„ï¿½Ã—ï¿½\\nğ‘‹ï¿½âˆˆ â„ï¿½Ã—ï¿½ğ»ï¿½âˆˆ â„ï¿½Ã—ï¿½ğ»ï¿½âˆˆ â„ï¿½Ã—ï¿½\\nğ‘‹ï¿½âˆˆ â„ï¿½Ã—ï¿½ ğ‘‹ âˆˆ â„ï¿½Ã—ï¿½\\nHistory ğ¿timestepsFuture ğ‘‡timesteps\\n(a) The whole structure of DLinear (b) One Linear Layer\\nFigure 2. Illustration of the basic linear model.\\nTo validate this hypothesis, we present the simplest DMS\\nmodel via a temporal linear layer, named LTSF-Linear , as a\\nbaseline for comparison. The basic formulation of LTSF-\\nLinear directly regresses historical time series for future\\nprediction via a weighted sum operation (as illustrated in\\nFigure 2). The mathematical expression is ^Xi=WX i,\\nwhere W2RTLis a linear layer along the temporal axis.\\n^XiandXiare the prediction and input for each ithvari-\\nate. Note that LTSF-Linear shares weights across different\\nvariates and does not model any spatial correlations.\\nLTSF-Linear is a set of linear models. Vanilla Linear is\\na one-layer linear model. To handle time series across dif-\\nferent domains (e.g., ï¬nance, trafï¬c, and energy domains),\\nwe further introduce two variants with two preprocessing\\nmethods, named DLinear andNLinear .\\nâ€¢ Speciï¬cally, DLinear is a combination of a Decom-\\nposition scheme used in Autoformer and FEDformer\\nwith linear layers. It ï¬rst decomposes a raw data in-\\nput into a trend component by a moving average ker-\\nnel and a remainder (seasonal) component. Then, two\\none-layer linear layers are applied to each component,\\nand we sum up the two features to get the ï¬nal predic-\\ntion. By explicitly handling trend, DLinear enhances\\nthe performance of a vanilla linear when there is a clear\\ntrend in the data.\\nâ€¢ Meanwhile, to boost the performance of LTSF-Linear\\nwhen there is a distribution shift in the dataset, NLin-\\nearï¬rst subtracts the input by the last value of the se-\\nquence. Then, the input goes through a linear layer,\\nand the subtracted part is added back before making\\nthe ï¬nal prediction. The subtraction and addition in\\nNLinear are a simple normalization for the input se-\\nquence.5. Experiments\\n5.1. Experimental Settings\\nDataset. We conduct extensive experiments on nine\\nwidely-used real-world datasets, including ETT (Electricity\\nTransformer Temperature) [30] (ETTh1, ETTh2, ETTm1,\\nETTm2), Trafï¬c, Electricity, Weather, ILI, Exchange-\\nRate [15]. All of them are multivariate time series. We\\nleave data descriptions in the Appendix.\\nEvaluation metric. Following previous works [28, 30,\\n31], we use Mean Squared Error (MSE) and Mean Absolute\\nError (MAE) as the core metrics to compare performance.\\nCompared methods. We include ï¬ve recent\\nTransformer-based methods: FEDformer [31], Aut-\\noformer [28], Informer [30], Pyraformer [18], and\\nLogTrans [16]. Besides, we include a naive DMS method:\\nClosest Repeat ( Repeat ), which repeats the last value in the\\nlook-back window, as another simple baseline. Since there\\nare two variants of FEDformer, we compare the one with\\nbetter accuracy (FEDformer-f via Fourier transform).\\n5.2. Comparison with Transformers\\nQuantitative results. In Table 2, we extensively eval-\\nuate all mentioned Transformers on nine benchmarks, fol-\\nlowing the experimental setting of previous work [28, 30,\\n31]. Surprisingly, the performance of LTSF-Linear sur-\\npasses the SOTA FEDformer in most cases by 20% 50%\\nimprovements on the multivariate forecasting , where LTSF-\\nLinear even does not model correlations among variates.\\nFor different time series benchmarks, NLinear andDLin-\\nearshow the superiority to handle the distribution shift and\\ntrend-seasonality features. We also provide results for uni-\\nvariate forecasting of ETT datasets in the Appendix, where\\nLTSF-Linear still consistently outperforms Transformer-\\nbased LTSF solutions by a large margin.\\nFEDformer achieves competitive forecasting accuracy\\non ETTh1. This because FEDformer employs classical time\\nseries analysis techniques such as frequency processing,\\nwhich brings in time series inductive bias and beneï¬ts the\\nability of temporal feature extraction. In summary, these re-\\nsults reveal that existing complex Transformer-based LTSF\\nsolutions are not seemingly effective on the existing nine\\nbenchmarks while LTSF-Linear can be a powerful baseline.\\nAnother interesting observation is that even though the\\nnaive Repeat method shows worse results when predict-\\ning long-term seasonal data (e.g., Electricity and Trafï¬c), it\\nsurprisingly outperforms all Transformer-based methods on\\nExchange-Rate (around 45%). This is mainly caused by the\\nwrong prediction of trends in Transformer-based solutions,\\nwhich may overï¬t toward sudden change noises in the train-\\ning data, resulting in signiï¬cant accuracy degradation (see\\nFigure 3(b)). Instead, Repeat does not have the bias.\\nQualitative results. As shown in Figure 3, we plot\\n4\\nDatasets ETTh1 &ETTh2 ETTm1 &ETTm2 Trafï¬c Electricity Exchange-Rate Weather ILI\\nVariates 7 7 862 321 8 21 7\\nTimesteps 17,420 69,680 17,544 26,304 7,588 52,696 966\\nGranularity 1hour 5min 1hour 1hour 1day 10min 1week\\nTable 1. The statistics of the nine popular datasets for the LTSF problem.\\nMethods IMP. Linear* NLinear* DLinear* FEDformer Autoformer Informer Pyraformer* LogTrans Repeat *\\nMetric MSE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAEElectricity96 27.40% 0.140 0.237 0.141 0.237 0.140 0.237 0.193 0.308 0.201 0.317 0.274 0.368 0.386 0.449 0.258 0.357 1.588 0.946\\n192 23.88% 0.153 0.250 0.154 0.248 0.153 0.249 0.201 0.315 0.222 0.334 0.296 0.386 0.386 0.443 0.266 0.368 1.595 0.950\\n336 21.02% 0.169 0.268 0.171 0.265 0.169 0.267 0.214 0.329 0.231 0.338 0.300 0.394 0.378 0.443 0.280 0.380 1.617 0.961\\n720 17.47% 0.203 0.301 0.210 0.297 0.203 0.301 0.246 0.355 0.254 0.361 0.373 0.439 0.376 0.445 0.283 0.376 1.647 0.975Exchange96 45.27% 0.082 0.207 0.089 0.208 0.081 0.203 0.148 0.278 0.197 0.323 0.847 0.752 0.376 1.105 0.968 0.812 0.081 0.196\\n192 42.06% 0.167 0.304 0.180 0.300 0.157 0.293 0.271 0.380 0.300 0.369 1.204 0.895 1.748 1.151 1.040 0.851 0.167 0.289\\n336 33.69% 0.328 0.432 0.331 0.415 0.305 0.414 0.460 0.500 0.509 0.524 1.672 1.036 1.874 1.172 1.659 1.081 0.305 0.396\\n720 46.19% 0.964 0.750 1.033 0.780 0.643 0.601 1.195 0.841 1.447 0.941 2.478 1.310 1.943 1.206 1.941 1.127 0.823 0.681Trafï¬c96 30.15% 0.410 0.282 0.410 0.279 0.410 0.282 0.587 0.366 0.613 0.388 0.719 0.391 2.085 0.468 0.684 0.384 2.723 1.079\\n192 29.96% 0.423 0.287 0.423 0.284 0.423 0.287 0.604 0.373 0.616 0.382 0.696 0.379 0.867 0.467 0.685 0.390 2.756 1.087\\n336 29.95% 0.436 0.295 0.435 0.290 0.436 0.296 0.621 0.383 0.622 0.337 0.777 0.420 0.869 0.469 0.734 0.408 2.791 1.095\\n720 25.87% 0.466 0.315 0.464 0.307 0.466 0.315 0.626 0.382 0.660 0.408 0.864 0.472 0.881 0.473 0.717 0.396 2.811 1.097Weather96 18.89% 0.176 0.236 0.182 0.232 0.176 0.237 0.217 0.296 0.266 0.336 0.300 0.384 0.896 0.556 0.458 0.490 0.259 0.254\\n192 21.01% 0.218 0.276 0.225 0.269 0.220 0.282 0.276 0.336 0.307 0.367 0.598 0.544 0.622 0.624 0.658 0.589 0.309 0.292\\n336 22.71% 0.262 0.312 0.271 0.301 0.265 0.319 0.339 0.380 0.359 0.395 0.578 0.523 0.739 0.753 0.797 0.652 0.377 0.338\\n720 19.85% 0.326 0.365 0.338 0.348 0.323 0.362 0.403 0.428 0.419 0.428 1.059 0.741 1.004 0.934 0.869 0.675 0.465 0.394ILI24 47.86% 1.947 0.985 1.683 0.858 2.215 1.081 3.228 1.260 3.483 1.287 5.764 1.677 1.420 2.012 4.480 1.444 6.587 1.701\\n36 36.43% 2.182 1.036 1.703 0.859 1.963 0.963 2.679 1.080 3.103 1.148 4.755 1.467 7.394 2.031 4.799 1.467 7.130 1.884\\n48 34.43% 2.256 1.060 1.719 0.884 2.130 1.024 2.622 1.078 2.669 1.085 4.763 1.469 7.551 2.057 4.800 1.468 6.575 1.798\\n60 34.33% 2.390 1.104 1.819 0.917 2.368 1.096 2.857 1.157 2.770 1.125 5.264 1.564 7.662 2.100 5.278 1.560 5.893 1.677ETTh196 0.80% 0.375 0.397 0.374 0.394 0.375 0.399 0.376 0.419 0.449 0.459 0.865 0.713 0.664 0.612 0.878 0.740 1.295 0.713\\n192 3.57% 0.418 0.429 0.408 0.415 0.405 0.416 0.420 0.448 0.500 0.482 1.008 0.792 0.790 0.681 1.037 0.824 1.325 0.733\\n336 6.54% 0.479 0.476 0.429 0.427 0.439 0.443 0.459 0.465 0.521 0.496 1.107 0.809 0.891 0.738 1.238 0.932 1.323 0.744\\n720 13.04% 0.624 0.592 0.440 0.453 0.472 0.490 0.506 0.507 0.514 0.512 1.181 0.865 0.963 0.782 1.135 0.852 1.339 0.756ETTh296 19.94% 0.288 0.352 0.277 0.338 0.289 0.353 0.346 0.388 0.358 0.397 3.755 1.525 0.645 0.597 2.116 1.197 0.432 0.422\\n192 19.81% 0.377 0.413 0.344 0.381 0.383 0.418 0.429 0.439 0.456 0.452 5.602 1.931 0.788 0.683 4.315 1.635 0.534 0.473\\n336 25.93% 0.452 0.461 0.357 0.400 0.448 0.465 0.496 0.487 0.482 0.486 4.721 1.835 0.907 0.747 1.124 1.604 0.591 0.508\\n720 14.25% 0.698 0.595 0.394 0.436 0.605 0.551 0.463 0.474 0.515 0.511 3.647 1.625 0.963 0.783 3.188 1.540 0.588 0.517ETTm196 21.10% 0.308 0.352 0.306 0.348 0.299 0.343 0.379 0.419 0.505 0.475 0.672 0.571 0.543 0.510 0.600 0.546 1.214 0.665\\n192 21.36% 0.340 0.369 0.349 0.375 0.335 0.365 0.426 0.441 0.553 0.496 0.795 0.669 0.557 0.537 0.837 0.700 1.261 0.690\\n336 17.07% 0.376 0.393 0.375 0.388 0.369 0.386 0.445 0.459 0.621 0.537 1.212 0.871 0.754 0.655 1.124 0.832 1.283 0.707\\n720 21.73% 0.440 0.435 0.433 0.422 0.425 0.421 0.543 0.490 0.671 0.561 1.166 0.823 0.908 0.724 1.153 0.820 1.319 0.729ETTm296 17.73% 0.168 0.262 0.167 0.255 0.167 0.260 0.203 0.287 0.255 0.339 0.365 0.453 0.435 0.507 0.768 0.642 0.266 0.328\\n192 17.84% 0.232 0.308 0.221 0.293 0.224 0.303 0.269 0.328 0.281 0.340 0.533 0.563 0.730 0.673 0.989 0.757 0.340 0.371\\n336 15.69% 0.320 0.373 0.274 0.327 0.281 0.342 0.325 0.366 0.339 0.372 1.363 0.887 1.201 0.845 1.334 0.872 0.412 0.410\\n720 12.58% 0.413 0.435 0.368 0.384 0.397 0.421 0.421 0.415 0.433 0.432 3.379 1.338 3.625 1.451 3.048 1.328 0.521 0.465\\n- Methods* are implemented by us; Other results are from FEDformer [31].\\nTable 2. Multivariate long-term forecasting errors in terms of MSE and MAE, the lower the better. Among them, ILI dataset is with\\nforecasting horizon T2 f24;36;48;60g. For the others, T2 f96;192;336;720g.Repeat repeats the last value in the look-back window.\\nThebest results are highlighted in bold and the best results of Transformers are highlighted with a underline . Accordingly, IMP. is the\\nbest result of linear models compared to the results of Transformer-based solutions.\\nthe prediction results on three selected time series datasets\\nwith Transformer-based solutions and LTSF-Linear : Elec-\\ntricity (Sequence 1951, Variate 36), Exchange-Rate (Se-\\nquence 676, Variate 3), and ETTh2 ( Sequence 1241, Vari-\\nate 2), where these datasets have different temporal patterns.\\nWhen the input length is 96 steps, and the output horizon\\nis 336 steps, Transformers [28, 30, 31] fail to capture the\\nscale and bias of the future data on Electricity and ETTh2.\\nMoreover, they can hardly predict a proper trend on aperi-\\nodic data such as Exchange-Rate. These phenomena further\\nindicate the inadequacy of existing Transformer-based solu-\\ntions for the LTSF task.\\n5.3. More Analyses on LTSF-Transformers\\nCan existing LTSF-Transformers extract temporal re-\\nlations well from longer input sequences? The size of the\\nlook-back window greatly impacts forecasting accuracy asit determines how much we can learn from historical data.\\nGenerally speaking, a powerful TSF model with a strong\\ntemporal relation extraction capability should be able to\\nachieve better results with larger look-back window sizes.\\nTo study the impact of input look-back win-\\ndow sizes, we conduct experiments with L2\\nf24;48;72;96;120;144;168;192;336;504;672;720g\\nfor long-term forecasting (T=720). Figure 4 demonstrates\\nthe MSE results on two datasets. Similar to the observations\\nfrom previous studies [27, 30], existing Transformer-based\\nmodelsâ€™ performance deteriorates or stays stable when\\nthe look-back window size increases. In contrast, the\\nperformances of all LTSF-Linear are signiï¬cantly boosted\\nwith the increase of look-back window size. Thus, existing\\nsolutions tend to overï¬t temporal noises instead of extract-\\ning temporal information if given a longer sequence, and\\nthe input size 96 is exactly suitable for most Transformers.\\n5\\n0 50 100 150 200 250 3001.5\\n1.0\\n0.5\\n0.00.51.0GrouthTruth Autoformer Informer FEDformer DLinear(a) Electricity\\n0 50 100 150 200 250 3001\\n01234GrouthTruth Autoformer Informer FEDformer DLinear (b) Exchange-Rate\\n0 50 100 150 200 250 3001.5\\n1.0\\n0.5\\n0.00.51.0GrouthTruth Autoformer Informer FEDformer DLinear (c) ETTh2\\nFigure 3. Illustration of the long-term forecasting output (Y-axis) of ï¬ve models with an input length L=96 and output length T=192\\n(X-axis) on Electricity, Exchange-Rate, and ETTh2, respectively.\\nAdditionally, we provide more quantitative results in the\\nAppendix, and our conclusion holds in almost all cases.\\n24 48 72 96 120 144 168 192 336 504 672 7200.40.60.81.01.21.4\\nTransformer\\nInformerAutoformer\\nFEDformerPyraformer\\nLinearNLinear\\nDLinear\\n(a)720steps- Trafï¬c\\n24 48 72 96 120 144 168 192 336 504 672 7200.200.250.300.350.40\\nTransformer\\nInformerAutoformer\\nFEDformerPyraformer\\nLinearNLinear\\nDLinear (b)720steps- Electricity\\nFigure 4. The MSE results (Y-axis) of models with different look-\\nback window sizes (X-axis) of long-term forecasting (T= 720) on\\nthe Trafï¬c and Electricity datasets.\\nWhat can be learned for long-term forecasting? While\\nthe temporal dynamics in the look-back window signiï¬-\\ncantly impact the forecasting accuracy of short-term time\\nseries forecasting, we hypothesize that long-term forecast-\\ning depends on whether models can capture the trend and\\nperiodicity well only. That is, the farther the forecasting\\nhorizon, the less impact the look-back window itself has.\\nMethods FEDformer Autoformer\\nInput Close Far Close Far\\nElectricity 0.251 0.265 0.255 0.287\\nTrafï¬c 0.631 0.645 0.677 0.675\\nTable 3. Comparison of different input sequences under the MSE\\nmetric to explore what LTSF-Transformers depend on. If the in-\\nput is Close , we use the 96th;:::;191thtime steps as the input\\nsequence. If the input is Far, we use the 0th;:::;95thtime steps.\\nBoth of them forecast the 192th;:::;(192 + 720) thtime steps.\\nTo validate the above hypothesis, in Table 3, we compare\\nthe forecasting accuracy for the same future 720 time steps\\nwith data from two different look-back windows: (i). the\\noriginal input L=96 setting (called Close ) and (ii). the far\\ninput L=96 setting (called Far) that is before the original96 time steps. From the experimental results, the perfor-\\nmance of the SOTA Transformers drops slightly, indicat-\\ning these models only capture similar temporal information\\nfrom the adjacent time series sequence. Since capturing the\\nintrinsic characteristics of the dataset generally does not re-\\nquire a large number of parameters, i,e. one parameter can\\nrepresent the periodicity. Using too many parameters will\\neven cause overï¬tting, which partially explains why LTSF-\\nLinear performs better than Transformer-based methods.\\nAre the self-attention scheme effective for LTSF? We\\nverify whether these complex designs in the existing Trans-\\nformer (e.g., Informer) are essential. In Table 4, we gradu-\\nally transform Informer to Linear. First, we replace each\\nself-attention layer by a linear layer, called Att.-Linear ,\\nsince a self-attention layer can be regarded as a fully-\\nconnected layer where weights are dynamically changed.\\nFurthermore, we discard other auxiliary designs (e.g., FFN)\\nin Informer to leave embedding layers and linear layers,\\nnamed Embed + Linear . Finally, we simplify the model to\\none linear layer. Surprisingly, the performance of Informer\\ngrows with the gradual simpliï¬cation, indicating the unnec-\\nessary of the self-attention scheme and other complex mod-\\nules at least for existing LTSF benchmarks.\\nMethods Informer Att.-Linear Embed + Linear LinearExchange96 0.847 1.003 0.173 0.084\\n192 1.204 0.979 0.443 0.155\\n336 1.672 1.498 1.288 0.301\\n720 2.478 2.102 2.026 0.763ETTh196 0.865 0.613 0.454 0.400\\n192 1.008 0.759 0.686 0.438\\n336 1.107 0.921 0.821 0.479\\n720 1.181 0.902 1.051 0.515\\nTable 4. The MSE comparisons of gradually transforming In-\\nformer to a Linear from the left to right columns. Att.-Linear is\\na structure that replaces each attention layer with a linear layer.\\nEmbed + Linear is to drop other designs and only keeps embed-\\nding layers and a linear layer. The look-back window size is 96.\\nCan existing LTSF-Transformers preserve temporal\\norder well? Self-attention is inherently permutation-\\n6\\nMethods Linear FEDformer Autoformer Informer\\nPredict Length Ori. Shuf. Half-Ex. Ori. Shuf. Half-Ex. Ori. Shuf. Half-Ex. Ori. Shuf. Half-Ex.Exchange96 0.080 0.133 0.169 0.161 0.160 0.162 0.152 0.158 0.160 0.952 1.004 0.959\\n192 0.162 0.208 0.243 0.274 0.275 0.275 0.278 0.271 0.277 1.012 1.023 1.014\\n336 0.286 0.320 0.345 0.439 0.439 0.439 0.435 0.430 0.435 1.177 1.181 1.177\\n720 0.806 0.819 0.836 1.122 1.122 1.122 1.113 1.113 1.113 1.198 1.210 1.196\\nAverage Drop N/A 27.26% 46.81% N/A -0.09% 0.20% N/A 0.09% 1.12% N/A -0.12% -0.18%ETTh196 0.395 0.824 0.431 0.376 0.753 0.405 0.455 0.838 0.458 0.974 0.971 0.971\\n192 0.447 0.824 0.471 0.419 0.730 0.436 0.486 0.774 0.491 1.233 1.232 1.231\\n336 0.490 0.825 0.505 0.447 0.736 0.453 0.496 0.752 0.497 1.693 1.693 1.691\\n720 0.520 0.846 0.528 0.468 0.720 0.470 0.525 0.696 0.524 2.720 2.716 2.715\\nAverage Drop N/A 81.06% 4.78% N/A 73.28% 3.44% N/A 56.91% 0.46% N/A 1.98% 0.18%\\nTable 5. The MSE comparisons of models when shufï¬‚ing the raw input sequence. Shuf. randomly shufï¬‚es the input sequence. Half-EX.\\nrandomly exchanges the ï¬rst half of the input sequences with the second half. Average Drop is the average performance drop under all\\nforecasting lengths after shufï¬‚ing. All results are the average test MSE of ï¬ve runs.\\ninvariant, i.e., regardless of the order. However, in time-\\nseries forecasting, the sequence order often plays a crucial\\nrole. We argue that even with positional and temporal em-\\nbeddings, existing Transformer-based methods still suffer\\nfrom temporal information loss. In Table 5, we shufï¬‚e the\\nraw input before the embedding strategies. Two shufï¬‚ing\\nstrategies are presented: Shuf. randomly shufï¬‚es the whole\\ninput sequences and Half-Ex. exchanges the ï¬rst half of\\nthe input sequence with the second half. Interestingly, com-\\npared with the original setting ( Ori.) on the Exchange Rate,\\nthe performance of all Transformer-based methods does not\\nï¬‚uctuate even when the input sequence is randomly shuf-\\nï¬‚ed. By contrary, the performance of LTSF-Linear is dam-\\naged signiï¬cantly. These indicate that LTSF-Transformers\\nwith different positional and temporal embeddings preserve\\nquite limited temporal relations and are prone to overï¬t on\\nnoisy ï¬nancial data, while the LTSF-Linear can model the\\norder naturally and avoid overï¬tting with fewer parameters.\\nFor the ETTh1 dataset, FEDformer and Autoformer in-\\ntroduce time series inductive bias into their models, mak-\\ning them can extract certain temporal information when the\\ndataset has more clear temporal patterns (e.g., periodicity)\\nthan the Exchange Rate. Therefore, the average drops of\\nthe two Transformers are 73.28% and 56.91% under the\\nShuf. setting, where it loses the whole order information.\\nMoreover, Informer still suffers less from both Shuf. and\\nHalf-Ex. settings due to its no such temporal inductive bias.\\nOverall, the average drops of LTSF-Linear are larger than\\nTransformer-based methods for all cases, indicating the ex-\\nisting Transformers do not preserve temporal order well.\\nHow effective are different embedding strategies? We\\nstudy the beneï¬ts of position and timestamp embeddings\\nused in Transformer-based methods. In Table 6, the fore-\\ncasting errors of Informer largely increase without posi-\\ntional embeddings (wo/Pos.). Without timestamp embed-\\ndings (wo/Temp.) will gradually damage the performance\\nof Informer as the forecasting lengths increase. Since In-\\nformer uses a single time step for each token, it is necessary\\nto introduce temporal information in tokens.Methods EmbeddingTrafï¬c\\n96 192 336 720\\nFEDformerAll 0.597 0.606 0.627 0.649\\nwo/Pos. 0.587 0.604 0.621 0.626\\nwo/Temp. 0.613 0.623 0.650 0.677\\nwo/Pos.-Temp. 0.613 0.622 0.648 0.663\\nAutoformerAll 0.629 0.647 0.676 0.638\\nwo/Pos. 0.613 0.616 0.622 0.660\\nwo/Temp. 0.681 0.665 0.908 0.769\\nwo/Pos.-Temp. 0.672 0.811 1.133 1.300\\nInformerAll 0.719 0.696 0.777 0.864\\nwo/Pos. 1.035 1.186 1.307 1.472\\nwo/Temp. 0.754 0.780 0.903 1.259\\nwo/Pos.-Temp. 1.038 1.351 1.491 1.512\\nTable 6. The MSE comparisons of different embedding strate-\\ngies on Transformer-based methods with look-back window size\\n96and forecasting lengths f96;192;336;720g.\\nRather than using a single time step in each token, FED-\\nformer and Autoformer input a sequence of timestamps to\\nembed the temporal information. Hence, they can achieve\\ncomparable or even better performance without ï¬xed po-\\nsitional embeddings. However, without timestamp embed-\\ndings, the performance of Autoformer declines rapidly be-\\ncause of the loss of global temporal information. Instead,\\nthanks to the frequency-enhanced module proposed in FED-\\nformer to introduce temporal inductive bias, it suffers less\\nfrom removing any position/timestamp embeddings.\\nIs training data size a limiting factor for existing LTSF-\\nTransformers? Some may argue that the poor performance\\nof Transformer-based solutions is due to the small sizes of\\nthe benchmark datasets. Unlike computer vision or nat-\\nural language processing tasks, TSF is performed on col-\\nlected time series, and it is difï¬cult to scale up the training\\ndata size. In fact, the size of the training data would in-\\ndeed have a signiï¬cant impact on the model performance.\\nAccordingly, we conduct experiments on Trafï¬c, compar-\\ning the performance of the model trained on a full dataset\\n(17,544*0.7 hours), named Ori., with that trained on a\\nshortened dataset (8,760 hours, i.e., 1 year), called Short .\\nUnexpectedly, Table 7 presents that the prediction errors\\n7\\nwith reduced training data are lower in most cases. This\\nmight because the whole-year data maintains more clear\\ntemporal features than a longer but incomplete data size.\\nWhile we cannot conclude that we should use less data for\\ntraining, it demonstrates that the training data scale is not\\nthe limiting reason for the performances of Autoformer and\\nFEDformer.\\nMethods FEDformer Autoformer\\nDataset Ori. Short Ori. Short\\n96 0.587 0.568 0.613 0.594\\n192 0.604 0.584 0.616 0.621\\n336 0.621 0.601 0.622 0.621\\n720 0.626 0.608 0.660 0.650\\nTable 7. The MSE comparison of two training data sizes.\\nIs efï¬ciency really a top-level priority? Existing LTSF-\\nTransformers claim that the O\\nL2\\ncomplexity of the\\nvanilla Transformer is unaffordable for the LTSF problem.\\nAlthough they prove to be able to improve the theoretical\\ntime and memory complexity from O\\nL2\\ntoO(L), it is\\nunclear whether 1) the actual inference time and memory\\ncost on devices are improved, and 2) the memory issue is\\nunacceptable and urgent for todayâ€™s GPU (e.g., an NVIDIA\\nTitan XP here). In Table 8, we compare the average prac-\\ntical efï¬ciencies with 5 runs. Interestingly, compared with\\nthe vanilla Transformer (with the same DMS decoder), most\\nTransformer variants incur similar or even worse inference\\ntime and parameters in practice. These follow-ups introduce\\nmore additional design elements to make practical costs\\nhigh. Moreover, the memory cost of the vanilla Transformer\\nis practically acceptable, even for output length L= 720 ,\\nwhich weakens the importance of developing a memory-\\nefï¬cient Transformers, at least for existing benchmarks.\\nMethod MACs Parameter Time Memory\\nDLinear 0.04G 139.7K 0.4ms 687MiB\\nTransformer 4.03G 13.61M 26.8ms 6091MiB\\nInformer 3.93G 14.39M 49.3ms 3869MiB\\nAutoformer 4.41G 14.91M 164.1ms 7607MiB\\nPyraformer 0.80G 241.4M3.4ms 7017MiB\\nFEDformer 4.41G 20.68M 40.5ms 4143MiB\\n-is modiï¬ed into the same one-step decoder, which is implemented in the source code from Autoformer.\\n-236.7M parameters of Pyraformer come from its linear decoder.\\nTable 8. Comparison of practical efï¬ciency of LTSF-Transformers\\nunder L=96 and T=720 on the Electricity. MACs are the number of\\nmultiply-accumulate operations. We use Dlinear for comparison\\nsince it has the double cost in LTSF-Linear . The inference time\\naverages 5 runs.\\n6. Conclusion and Future Work\\nConclusion. This work questions the effectiveness of\\nemerging favored Transformer-based solutions for the long-\\nterm time series forecasting problem. We use an em-\\nbarrassingly simple linear model LTSF-Linear as a DMS\\nforecasting baseline to verify our claims. Note that ourcontributions do not come from proposing a linear model\\nbut rather from throwing out an important question, show-\\ning surprising comparisons, and demonstrating why LTSF-\\nTransformers are not as effective as claimed in these works\\nthrough various perspectives. We sincerely hope our com-\\nprehensive studies can beneï¬t future work in this area.\\nFuture work. LTSF-Linear has a limited model ca-\\npacity, and it merely serves a simple yet competitive base-\\nline with strong interpretability for future research. For ex-\\nample, the one-layer linear network is hard to capture the\\ntemporal dynamics caused by change points [25]. Conse-\\nquently, we believe there is a great potential for new model\\ndesigns, data processing, and benchmarks to tackle the chal-\\nlenging LTSF problem.\\n8\\nAppendix:\\nAre Transformers Effective for Time Series Forecasting?\\nIn this Appendix, we provide descriptions of non-\\nTransformer-based TSF solutions, detailed experimental\\nsettings, more comparisons under different look-back win-\\ndow sizes, and the visualization of LTSF-Linear on all\\ndatasets. We also append our code to reproduce the results\\nshown in the paper.\\nA. Related Work: Non-Transformer-Based\\nTSF Solutions\\nAs a long-standing problem with a wide range of ap-\\nplications, statistical approaches (e.g., autoregressive inte-\\ngrated moving average (ARIMA) [1], exponential smooth-\\ning [12], and structural models [14]) for time series fore-\\ncasting have been used from the 1970s onward. Generally\\nspeaking, the parametric models used in statistical methods\\nrequire signiï¬cant domain expertise to build.\\nTo relieve this burden, many machine learning\\ntechniques such as gradient boosting regression tree\\n(GBRT) [10, 11] gain popularity, which learns the tempo-\\nral dynamics of time series in a data-driven manner. How-\\never, these methods still require manual feature engineer-\\ning and model designs. With the powerful representation\\nlearning capability of deep neural networks (DNNs) from\\nabundant data, various deep learning-based TSF solutions\\nare proposed in the literature, achieving better forecasting\\naccuracy than traditional techniques in many cases.\\nBesides Transformers, the other two popular DNN archi-\\ntectures are also applied for time series forecasting:\\nâ€¢ Recurrent neural networks (RNNs) based methods\\n(e.g., [21]) summarize the past information compactly\\nin internal memory states and recursively update them-\\nselves for forecasting.\\nâ€¢ Convolutional neural networks (CNNs) based meth-\\nods (e.g., [3]), wherein convolutional ï¬lters are used\\nto capture local temporal features.\\nRNN-based TSF methods belong to IMS forecasting\\ntechniques. Depending on whether the decoder is imple-\\nmented in an autoregressive manner, there are either IMS\\nor DMS forecasting techniques for CNN-based TSF meth-\\nods [3, 17].\\nB. Experimental Details\\nB.1. Data Descriptions\\nWe use nine wildly-used datasets in the main paper. The\\ndetails are listed in the following.â€¢ ETT (Electricity Transformer Temperature) [30]2con-\\nsists of two hourly-level datasets (ETTh) and two 15-\\nminute-level datasets (ETTm). Each of them contains\\nseven oil and load features of electricity transformers\\nfrom July 2016 to July 2018 .\\nâ€¢ Trafï¬c3describes the road occupancy rates. It con-\\ntains the hourly data recorded by the sensors of San\\nFrancisco freeways from 2015 to2016 .\\nâ€¢ Electricity4collects the hourly electricity consumption\\nof321clients from 2012 to2014 .\\nâ€¢ Exchange-Rate [15]5collects the daily exchange rates\\nof8countries from 1990 to2016 .\\nâ€¢ Weather6includes 21indicators of weather, such as air\\ntemperature, and humidity. Its data is recorded every\\n10 min for 2020 in Germany.\\nâ€¢ ILI7describes the ratio of patients seen with inï¬‚uenza-\\nlike illness and the number of patients. It includes\\nweekly data from the Centers for Disease Control and\\nPrevention of the United States from 2002 to2021 .\\nB.2. Implementation Details\\nFor existing Transformer-based TSF solutions: the im-\\nplementation of Autoformer [28], Informer [30], and the\\nvanilla Transformer [26] are all taken from the Autoformer\\nwork [28]; the implementation of FEDformer [31] and\\nPyraformer [18] are from their respective code repository.\\nWe also adopt their default hyper-parameters to train the\\nmodels. For DLinear , the moving average kernel size for\\ndecomposition is 25, which is the same as Autoformer. The\\ntotal parameters of a vanilla linear model and a NLinear\\nare TL. The total parameters of the DLinear are 2TL. Since\\nLTSF-Linear will be underï¬tting when the input length is\\nshort, and LTSF-Transformers tend to overï¬t on a long\\nlookback window size. To compare the best performance\\nof existing LTSF-Transformers with LTSF-Linear , we re-\\nport L=336 for LTSF-Linear and L=96 for Transformers by\\ndefault. For more hyper-parameters of LTSF-Linear , please\\nrefer to our code.\\n2https://github.com/zhouhaoyi/ETDataset\\n3http://pems.dot.ca.gov\\n4https : / / archive . ics . uci . edu / ml / datasets /\\nElectricityLoadDiagrams20112014\\n5https : / / github . com / laiguokun / multivariate -\\ntime-series-data\\n6https://www.bgc-jena.mpg.de/wetter/\\n7https : / / gis . cdc . gov / grasp / fluview /\\nfluportaldashboard.html\\n9\\nC. Additional Comparison with Transformers\\nWe further compare LTSF-Linear with LTSF-\\nTransformer for Univariate Forecasting on four ETT\\ndatasets. Moreover, in Figure 4of the main paper, we\\ndemonstrate that existing Transformers fail to exploit large\\nlook-back window sizes with two examples. Here, we give\\ncomprehensive comparisons between LTSF-Linear and\\nTransformer-based TSF solutions under various look-back\\nwindow sizes on all benchmarks .\\nC.1. Comparison of Univariate Forecasting\\nWe present the univariate forecasting results on the four\\nETT datasets in table 9. Similarly, LTSF-Linear , especially\\nforNLinear can consistently outperform all transformer-\\nbased methods by a large margin in most time. We ï¬nd\\nthat there are serious distribution shifts between training\\nand test sets (as shown in Fig. 5 (a), (b)) on ETTh1 and\\nETTh2 datasets. Simply normalization via the last value\\nfrom the lookback window can greatly relieve the distribu-\\ntion shift problem.\\nC.2. Comparison under Different Look-back Win-\\ndows\\nIn Figure 6, we provide the MSE comparisons of ï¬ve\\nLTSF-Transformers with LTSF-Linear under different look-\\nback window sizes to explore whether existing Transform-\\ners can extract temporal well from longer input sequences.\\nFor hourly granularity datasets (ETTh1, ETTh2, Trafï¬c,\\nand Electricity), the increasing look-back window sizes are\\n{24, 48, 72, 96, 120, 144, 168, 192, 336, 504, 672, 720},\\nwhich represent {1, 2, 3, 4, 5, 6, 7, 8, 14, 21, 28, 30}\\ndays. The forecasting steps are {24, 720}, which mean {1,\\n30} days. For 5-minute granularity datasets (ETTm1 and\\nETTm2), we set the look-back window size as {24, 36, 48,\\n60, 72, 144, 288}, which represent {2, 3, 4, 5, 6, 12, 24}\\nhours. For 10-minute granularity datasets (Weather), we set\\nthe look-back window size as {24, 48, 72, 96, 120, 144,\\n168, 192, 336, 504, 672, 720}, which mean {4, 8, 12, 16,\\n20, 24, 28, 32, 56, 84, 112, 120} hours. The forecasting\\nsteps are {24, 720} that are {4, 120} hours. For weekly\\ngranularity dataset (ILI), we set the look-back window size\\nas {26, 52, 78, 104, 130, 156, 208}, which represent {0.5, 1,\\n1.5, 2, 2.5, 3, 3.5, 4} years. The corresponding forecasting\\nsteps are {26, 208}, meaning {0.5, 4} years.\\nAs shown in Figure 6, with increased look-back win-\\ndow sizes, the performance of LTSF-Linear is signiï¬cantly\\nboosted for most datasets (e.g., ETTm1 and Trafï¬c), while\\nthis is not the case for Transformer-based TSF solutions.\\nMost of their performance ï¬‚uctuates or gets worse as\\nthe input lengths increase. To be speciï¬c, the results of\\nExchange-Rate do not show improved results with a long\\nlook-back window (from Figure 6(m) and (n)), and we at-tribute it to the low information-to-noise ratio in such ï¬nan-\\ncial data.\\nD. Ablation study on the LTSF-Linear\\nD.1. Motivation of NLinear\\nIf we normalize the test data by the mean and variance of\\ntrain data, there could be a distribution shift in testing data,\\ni.e, the mean value of testing data is not 0. If the model\\nmade a prediction that is out of the distribution of true value,\\na large error would occur. For example, there is a large er-\\nror between the true value and the true value minus/add one.\\nTherefore, in NLinear , we use the subtraction and addition\\nto shift the model prediction toward the distribution of true\\nvalue. Then, large errors are avoided, and the model perfor-\\nmances can be improved. Figure 5 illustrates histograms of\\nthe trainset-test set distributions, where each bar represents\\nthe number of data points. Clear distribution shifts between\\ntraining and testing data can be observed in ETTh1, ETTh2,\\nand ILI. Accordingly, from Table 9 and Table 2 in the main\\npaper, we can observe that there are great improvements\\nin the three datasets comparing the NLinear to the Linear ,\\nshowing the effectiveness of the NLinear in relieving dis-\\ntribution shifts. Moreover, for the datasets without obvi-\\nous distribution shifts, like Electricity in Figure 5(c), using\\nthe vanilla Linear can be enough, demonstrating the similar\\nperformance with NLinear andDLinear .\\nD.2. The Features of LTSF-Linear\\nAlthough LTSF-Linear is simple, it has some compelling\\ncharacteristics:\\nâ€¢AnO(1)maximum signal traversing path length :\\nThe shorter the path, the better the dependencies are\\ncaptured [18], making LTSF-Linear capable of cap-\\nturing both short-range and long-range temporal rela-\\ntions.\\nâ€¢High-efï¬ciency: AsLTSF-Linear is a linear model\\nwith two linear layers at most, it costs much lower\\nmemory and fewer parameters and has a faster infer-\\nence speed than existing Transformers (see Table 8in\\nmain paper).\\nâ€¢Interpretability: After training, we can visualize\\nweights from the seasonality and trend branches to\\nhave some insights on the predicted values [9].\\nâ€¢Easy-to-use: LTSF-Linear can be obtained easily\\nwithout tuning model hyper-parameters.\\nD.3. Interpretability of LTSF-Linear\\nBecause LTSF-Linear is a set of linear models, the\\nweights of linear layers can directly reveal how LTSF-\\nLinear works. The weight visualization of LTSF-Linear can\\n10\\nMethods Linear NLinear DLinear FEDformer-f FEDformer-w Autoformer Informer LogTrans\\nMetric MSE MAE MSE MSE MAE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAEETTh 196 0.189 0.359 0.053 0.177 0.056 0.180 0.079 0.215 0.080 0.214 0.071 0.206 0.193 0.377 0.283 0.468\\n192 0.078 0.212 0.069 0.204 0.071 0.204 0.104 0.245 0.105 0.256 0.114 0.262 0.217 0.395 0.234 0.409\\n336 0.091 0.237 0.081 0.226 0.098 0.244 0.119 0.270 0.120 0.269 0.107 0.258 0.202 0.381 0.386 0.546\\n720 0.172 0.340 0.080 0.226 0.189 0.359 0.142 0.299 0.127 0.280 0.126 0.283 0.183 0.355 0.475 0.629ETTh 296 0.133 0.283 0.129 0.278 0.131 0.279 0.128 0.271 0.156 0.306 0.153 0.306 0.213 0.373 0.217 0.379\\n192 0.176 0.330 0.169 0.324 0.176 0.329 0.185 0.330 0.238 0.380 0.204 0.351 0.227 0.387 0.281 0.429\\n336 0.213 0.371 0.194 0.355 0.209 0.367 0.231 0.378 0.271 0.412 0.246 0.389 0.242 0.401 0.293 0.437\\n720 0.292 0.440 0.225 0.381 0.276 0.426 0.278 0.420 0.288 0.438 0.268 0.409 0.291 0.439 0.218 0.387ETTm 196 0.028 0.125 0.026 0.122 0.028 0.123 0.033 0.140 0.036 0.149 0.056 0.183 0.109 0.277 0.049 0.171\\n192 0.043 0.154 0.039 0.149 0.045 0.156 0.058 0.186 0.069 0.206 0.081 0.216 0.151 0.310 0.157 0.317\\n336 0.059 0.180 0.052 0.172 0.061 0.182 0.084 0.231 0.071 0.209 0.076 0.218 0.427 0.591 0.289 0.459\\n720 0.080 0.211 0.073 0.207 0.080 0.210 0.102 0.250 0.105 0.248 0.110 0.267 0.438 0.586 0.430 0.579ETTm 296 0.066 0.189 0.063 0.182 0.063 0.183 0.067 0.198 0.063 0.189 0.065 0.189 0.088 0.225 0.075 0.208\\n192 0.094 0.230 0.090 0.223 0.092 0.227 0.102 0.245 0.110 0.252 0.118 0.256 0.132 0.283 0.129 0.275\\n336 0.120 0.263 0.117 0.259 0.119 0.261 0.130 0.279 0.147 0.301 0.154 0.305 0.180 0.336 0.154 0.302\\n720 0.175 0.320 0.170 0.318 0.175 0.320 0.178 0.325 0.219 0.368 0.182 0.335 0.300 0.435 0.160 0.321\\nTable 9. Univariate long sequence time-series forecasting results on ETT full benchmark. The best results are highlighted in bold and the\\nbest results of Transformers are highlighted with a underline .\\n(a) ETTh1 channel6\\n(b) ETTh2 channel3\\n(c) Electricity channel3\\n(d) ILI channel6\\nFigure 5. Distribution of ETTh1, ETTh2, Electricity, and ILI dataset. A clear distribution shift between training and testing data can be\\nobserved in ETTh1, ETTh2, and ILI.\\nalso reveal certain characteristics in the data used for fore-\\ncasting.\\nHere we take DLinear as an example. Accordingly, we\\nvisualize the trend and remainder weights of all datasets\\nwith a ï¬xed input length of 96 and four different forecasting\\nhorizons. To obtain a smooth weight with a clear pattern in\\nvisualization, we initialize the weights of the linear layers\\nin DLinear as 1=Lrather than random initialization. That\\nis, we use the same weight for every forecasting time step\\nin the look-back window at the start of training.\\nHow the model works: Figure 7(c) visualize the\\nweights of the trend and the remaining layers on the\\nExchange-Rate dataset. Due to the lack of periodicity and\\nseasonality in ï¬nancial data, it is hard to observe clear pat-\\nterns, but the trend layer reveals greater weights of informa-\\ntion closer to the outputs, representing their larger contribu-\\ntions to the predicted values.\\nPeriodicity of data: For Trafï¬c data, as shown in Fig-\\nure 7(d), the model gives high weights to the latest time\\nstep of the look-back window for the 0,23,47...719 forecast-ing steps. Among these forecasting time steps, the 0, 167,\\n335, 503, 671 time steps have higher weights. Note that 24\\ntime steps are a day, and 168 time steps are a week. This\\nindicates that Trafï¬c has a daily periodicity and a weekly\\nperiodicity.\\nReferences\\n[1] Adebiyi A Ariyo, Adewumi O Adewumi, and\\nCharles K Ayo. Stock price prediction using the arima\\nmodel. In 2014 UKSim-AMSS 16th International Con-\\nference on Computer Modelling and Simulation , pages\\n106â€“112. IEEE, 2014. 1, 2, 9\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua\\nBengio. Neural machine translation by jointly learn-\\ning to align and translate. arXiv: Computation and\\nLanguage , 2014. 2\\n[3] Shaojie Bai, J Zico Kolter, and Vladlen Koltun.\\nAn empirical evaluation of generic convolutional and\\n11\\n24 48 72 96 120 144 168 192 336 504 672 7200.30.40.50.60.70.80.91.0\\nTransformer\\nInformerAutoformer\\nFEDformerPyraformer\\nLinearNLinear\\nDLinear(a)24steps- ETTh1\\n24 48 72 96 120 144 168 192 336 504 672 7200.40.60.81.01.21.4\\nTransformer\\nInformerAutoformer\\nFEDformerPyraformer\\nLinearNLinear\\nDLinear (b)720steps- ETTh1\\n24 48 72 96 120 144 168 192 336 504 672 7200.20.40.60.81.01.2\\nTransformer\\nInformerAutoformer\\nFEDformerPyraformer\\nLinearNLinear\\nDLinear (c)24steps- ETTh2\\n24 48 72 96 120 144 168 192 336 504 672 72012345\\nTransformer\\nInformerAutoformer\\nFEDformerPyraformer\\nLinearNLinear\\nDLinear (d)720steps- ETTh2\\n24 36 48 60 72 144 2880.20.30.40.50.6\\nTransformer\\nInformerAutoformer\\nFEDformerPyraformer\\nLinearNLinear\\nDLinear\\n(e)24steps- ETTm1\\n24 36 48 60 72 144 2880.40.50.60.70.80.91.01.11.2\\nTransformer\\nInformerAutoformer\\nFEDformerPyraformer\\nLinearNLinear\\nDLinear (f)576steps- ETTm1\\n24 36 48 60 72 144 2880.100.150.200.250.30\\nTransformer\\nInformerAutoformer\\nFEDformerPyraformer\\nLinearNLinear\\nDLinear (g)24steps- ETTm2\\n24 36 48 60 72 144 2881234\\nTransformer\\nInformerAutoformer\\nFEDformerPyraformer\\nLinearNLinear\\nDLinear (h)576steps- ETTm2\\n24 48 72 96 120 144 168 192 336 504 672 7200.100.150.200.250.300.350.400.450.50\\nTransformer\\nInformerAutoformer\\nFEDformerPyraformer\\nLinearNLinear\\nDLinear\\n(i)24steps- Weather\\n24 48 72 96 120 144 168 192 336 504 672 7200.40.60.81.01.21.41.6\\nTransformer\\nInformerAutoformer\\nFEDformerPyraformer\\nLinearNLinear\\nDLinear (j)720steps- Weather\\n24 48 72 96 120 144 168 192 336 504 672 7200.40.50.60.70.80.9\\nTransformer\\nInformerAutoformer\\nFEDformerPyraformer\\nLinearNLinear\\nDLinear (k)24steps- Trafï¬c\\n24 48 72 96 120 144 168 192 336 504 672 7200.40.60.81.01.21.4\\nTransformer\\nInformerAutoformer\\nFEDformerPyraformer\\nLinearNLinear\\nDLinear (l)720steps- Trafï¬c\\n24 48 72 96 120 144 168 192 336 504 672 7200.00.20.40.60.81.01.21.4\\nTransformer\\nInformerAutoformer\\nFEDformerPyraformer\\nLinearNLinear\\nDLinear\\n(m)24steps- Exchange\\n24 48 72 96 120 144 168 192 336 504 672 7200.51.01.52.02.53.0\\nTransformer\\nInformerAutoformer\\nFEDformerPyraformer\\nLinearNLinear\\nDLinear (n)720steps- Exchange\\n26 52 78 104 130 156 208234567\\nTransformer\\nInformerAutoformer\\nFEDformerPyraformer\\nLinearNLinear\\nDLinear (o)24steps- ILI\\n26 52 78 104 130 156 2082345678\\nTransformer\\nInformerAutoformer\\nFEDformerPyraformer\\nLinearNLinear\\nDLinear (p)60steps- ILI\\nFigure 6. The MSE results (Y-axis) of models with different look-back window sizes (X-axis) of the long-term forecasting (e.g., 720-time\\nsteps) and the short-term forecasting (e.g., 24time steps) on different benchmarks.\\nrecurrent networks for sequence modeling. arXiv\\npreprint arXiv:1803.01271 , 2018. 1, 9\\n[4] Guillaume Chevillon. Direct multi-step estimation\\nand forecasting. Journal of Economic Surveys ,\\n21(4):746â€“785, 2007. 2\\n[5] Razvan-Gabriel Cirstea, Chenjuan Guo, Bin Yang,\\nTung Kieu, Xuanyi Dong, and Shirui Pan. Triformer:Triangular, variable-speciï¬c attentions for long se-\\nquence multivariate time series forecastingâ€“full ver-\\nsion. arXiv preprint arXiv:2204.13767 , 2022. 1\\n[6] R. B. Cleveland. Stl : A seasonal-trend decomposition\\nprocedure based on loess. Journal of Ofï¬ce Statistics ,\\n1990. 3\\n[7] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\\n12\\nIn-96, Out-96(b1) Remainder(b2) TrendIn-96, Out-192(b3) Remainder(b4) TrendIn-96, Out-336(b5) Remainder(b6) TrendIn-96, Out-720(b7) Remainder(b8) TrendElectricity\\nIn-96, Out-96(c1) Remainder(c2) TrendIn-96, Out-192(c3) Remainder(c4) TrendIn-96, Out-336(c5) Remainder(c6) Trend(c7) Remainder(c8) TrendExchange-Rate\\nIn-36, Out-24(f1) Remainder(f2) TrendIn-36, Out-36(f4) TrendIn-36, Out-48(f5) Remainder(f6) Trend(f7) Remainder(f8) TrendILI\\n(f3) RemainderIn-96, Out-96(d1) Remainder(d2) TrendIn-96, Out-192(d3) Remainder(d4) TrendIn-96, Out-336(d5) Remainder(d6) Trend(d7) Remainder(d8) TrendTraffic\\nIn-96, Out-96(e1) Remainder(e2) Trend\\nIn-96, Out-192(e3) Remainder(e4) TrendIn-96, Out-336(e5) Remainder(e6) Trend(e7) Remainder(e8) TrendWeatherIn-96, Out-720\\nIn-36, Out-60In-96, Out-720\\nIn-96, Out-720In-96, Out-96(a1) Remainder(a2) TrendIn-96, Out-168(a3) Remainder(a4) TrendIn-96, Out-336(a5) Remainder(a6) TrendIn-96, Out-720(a7) Remainder(a8) TrendETTh1\\nFigure 7. Visualization of the weights(T*L) of LTSF-Linear on several benchmarks. Models are trained with a look-back window L\\n(X-axis) and different forecasting time steps T (Y-axis). We show weights in the remainder and trend layer.\\nKristina Toutanova. Bert: Pre-training of deep bidirec- tional transformers for language understanding. arXiv\\n13\\npreprint arXiv:1810.04805 , 2018. 1\\n[8] Linhao Dong, Shuang Xu, and Bo Xu. Speech-\\ntransformer: a no-recurrence sequence-to-sequence\\nmodel for speech recognition. In 2018 IEEE Inter-\\nnational Conference on Acoustics, Speech and Signal\\nProcessing (ICASSP) , pages 5884â€“5888. IEEE, 2018.\\n1\\n[9] Ruijun Dong and Witold Pedrycz. A granular time se-\\nries approach to long-term forecasting and trend fore-\\ncasting. Physica A: Statistical Mechanics and its Ap-\\nplications , 387(13):3253â€“3270, 2008. 10\\n[10] Shereen Elsayed, Daniela Thyssens, Ahmed Rashed,\\nHadi Samer Jomaa, and Lars Schmidt-Thieme. Do\\nwe really need deep learning models for time series\\nforecasting? arXiv preprint arXiv:2101.02118 , 2021.\\n9\\n[11] Jerome H Friedman. Greedy function approximation:\\na gradient boosting machine. Annals of statistics ,\\npages 1189â€“1232, 2001. 1, 9\\n[12] Everette S Gardner Jr. Exponential smoothing: The\\nstate of the art. Journal of forecasting , 4(1):1â€“28,\\n1985. 9\\n[13] James Douglas Hamilton. Time series analysis .\\nPrinceton university press, 2020. 3\\n[14] Andrew C Harvey. Forecasting, structural time series\\nmodels and the kalman ï¬lter. 1990. 9\\n[15] Guokun Lai, Wei-Cheng Chang, Yiming Yang, and\\nHanxiao Liu. Modeling long- and short-term tempo-\\nral patterns with deep neural networks. international\\nacm sigir conference on research and development in\\ninformation retrieval , 2017. 1, 4, 9\\n[16] Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou,\\nWenhu Chen, Yu-Xiang Wang, and Xifeng Yan. En-\\nhancing the locality and breaking the memory bottle-\\nneck of transformer on time series forecasting. Ad-\\nvances in Neural Information Processing Systems , 32,\\n2019. 1, 2, 3, 4\\n[17] Minhao Liu, Ailing Zeng, Zhijian Xu, Qiuxia Lai, and\\nQiang Xu. Time series is a special sequence: Fore-\\ncasting with sample convolution and interaction. arXiv\\npreprint arXiv:2106.09305 , 2021. 1, 9\\n[18] Shizhan Liu, Hang Yu, Cong Liao, Jianguo Li, Weiyao\\nLin, Alex X Liu, and Schahram Dustdar. Pyraformer:\\nLow-complexity pyramidal attention for long-range\\ntime series modeling and forecasting. In International\\nConference on Learning Representations , 2021. 1, 2,\\n3, 4, 9, 10\\n[19] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei,\\nZheng Zhang, Stephen Lin, and Baining Guo. Swin\\ntransformer: Hierarchical vision transformer usingshifted windows. In Proceedings of the IEEE/CVF\\nInternational Conference on Computer Vision , pages\\n10012â€“10022, 2021. 1\\n[20] LIU Minhao, Ailing Zeng, LAI Qiuxia, Ruiyuan Gao,\\nMin Li, Jing Qin, and Qiang Xu. T-wavenet: A tree-\\nstructured wavelet neural network for time series sig-\\nnal analysis. In International Conference on Learning\\nRepresentations , 2021. 2\\n[21] GÃ¡bor PetnehÃ¡zi. Recurrent neural networks for time\\nseries forecasting. arXiv preprint arXiv:1901.00069 ,\\n2019. 9\\n[22] David Salinas, Valentin Flunkert, and Jan Gasthaus.\\nDeepar: Probabilistic forecasting with autoregressive\\nrecurrent networks. International Journal of Forecast-\\ning, 2017. 2\\n[23] Souhaib Ben Taieb, Rob J Hyndman, et al. Recur-\\nsive and direct multi-step forecasting: the best of both\\nworlds , volume 19. Citeseer, 2012. 2\\n[24] Sean J. Taylor and Benjamin Letham. Forecasting at\\nscale. PeerJ Prepr. , 2017. 2\\n[25] Gerrit JJ van den Burg and Christopher KI Williams.\\nAn evaluation of change point detection algorithms.\\narXiv preprint arXiv:2003.06222 , 2020. 8\\n[26] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\\nUszkoreit, Llion Jones, Aidan N Gomez, Åukasz\\nKaiser, and Illia Polosukhin. Attention is all you need.\\nAdvances in neural information processing systems ,\\n30, 2017. 1, 2, 9\\n[27] Qingsong Wen, Tian Zhou, Chaoli Zhang, Weiqi\\nChen, Ziqing Ma, Junchi Yan, and Liang Sun. Trans-\\nformers in time series: A survey. arXiv preprint\\narXiv:2202.07125 , 2022. 1, 2, 5\\n[28] Jiehui Xu, Jianmin Wang, Mingsheng Long, et al.\\nAutoformer: Decomposition transformers with auto-\\ncorrelation for long-term series forecasting. Advances\\nin Neural Information Processing Systems , 34, 2021.\\n1, 2, 3, 4, 5, 9\\n[29] Ailing Zeng, Xuan Ju, Lei Yang, Ruiyuan Gao,\\nXizhou Zhu, Bo Dai, and Qiang Xu. Deciwatch: A\\nsimple baseline for 10x efï¬cient 2d and 3d pose esti-\\nmation. arXiv preprint arXiv:2203.08713 , 2022. 1\\n[30] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai\\nZhang, Jianxin Li, Hui Xiong, and Wancai Zhang.\\nInformer: Beyond efï¬cient transformer for long se-\\nquence time-series forecasting. In The Thirty-Fifth\\nAAAI Conference on Artiï¬cial Intelligence, AAAI\\n2021, Virtual Conference , volume 35, pages 11106â€“\\n11115. AAAI Press, 2021. 1, 2, 3, 4, 5, 9\\n[31] Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang,\\nLiang Sun, and Rong Jin. Fedformer: Frequency en-\\nhanced decomposed transformer for long-term series\\n14\\nforecasting. In International Conference on Machine\\nLearning , 2022. 1, 2, 3, 4, 5, 9\\n15\"}]"},{"type":"thinking","thinking":"æˆ‘å·²ç»è·å–äº†è®ºæ–‡çš„å®Œæ•´å†…å®¹å’ŒGitHubä»“åº“çš„ä¿¡æ¯ã€‚ç°åœ¨è®©æˆ‘æ›´æ–°ä»»åŠ¡çŠ¶æ€å¹¶æ•´ç†è¯¦ç»†çš„è®ºæ–‡é˜…è¯»ç¬”è®°ã€‚","signature":"709568e24c01443e99f0c034"},{"type":"tool_use","id":"call_72d7f88912944c6ba271768b","name":"TodoWrite","input":{"todos":[{"activeForm":"å·²å®Œæˆæœç´¢DLinearè®ºæ–‡","content":"æœç´¢å¹¶è®¿é—®DLinearè®ºæ–‡","status":"completed"},{"activeForm":"æ­£åœ¨é˜…è¯»è®ºæ–‡","content":"é˜…è¯»å¹¶ç†è§£è®ºæ–‡æ ¸å¿ƒå†…å®¹","status":"in_progress"},{"activeForm":"æ­£åœ¨æ•´ç†æ¨¡å‹æ¶æ„","content":"æ•´ç†æ¨¡å‹æ¶æ„å’Œæ•°å­¦åŸç†","status":"pending"},{"activeForm":"æ­£åœ¨è®°å½•å®éªŒè®¾ç½®","content":"è®°å½•å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡","status":"pending"},{"activeForm":"æ­£åœ¨æ’°å†™ç¬”è®°","content":"æ’°å†™å®Œæ•´çš„è®ºæ–‡é˜…è¯»ç¬”è®°","status":"pending"}]}}],"stop_reason":"tool_use","stop_sequence":null,"usage":{"input_tokens":26394,"output_tokens":347,"cache_read_input_tokens":66432,"server_tool_use":{"web_search_requests":0},"service_tier":"standard"}},"type":"assistant","uuid":"00b2349c-3acc-46aa-8742-7918412c5047","timestamp":"2026-02-23T12:50:54.956Z"}
{"parentUuid":"00b2349c-3acc-46aa-8742-7918412c5047","isSidechain":false,"userType":"external","cwd":"/app/workspace","sessionId":"f068886d-0e50-4922-9128-2594a90f9c43","version":"2.1.50","gitBranch":"HEAD","type":"user","message":{"role":"user","content":[{"tool_use_id":"call_72d7f88912944c6ba271768b","type":"tool_result","content":"Todos have been modified successfully. Ensure that you continue to use the todo list to track your progress. Please proceed with the current tasks if applicable"}]},"uuid":"7eecefb5-3f14-4fa5-9d93-38f8048626d2","timestamp":"2026-02-23T12:50:54.957Z","toolUseResult":{"oldTodos":[{"content":"æœç´¢å¹¶è®¿é—®DLinearè®ºæ–‡","status":"in_progress","activeForm":"æ­£åœ¨æœç´¢DLinearè®ºæ–‡"},{"content":"é˜…è¯»å¹¶ç†è§£è®ºæ–‡æ ¸å¿ƒå†…å®¹","status":"pending","activeForm":"æ­£åœ¨é˜…è¯»è®ºæ–‡"},{"content":"æ•´ç†æ¨¡å‹æ¶æ„å’Œæ•°å­¦åŸç†","status":"pending","activeForm":"æ­£åœ¨æ•´ç†æ¨¡å‹æ¶æ„"},{"content":"è®°å½•å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡","status":"pending","activeForm":"æ­£åœ¨è®°å½•å®éªŒè®¾ç½®"},{"content":"æ’°å†™å®Œæ•´çš„è®ºæ–‡é˜…è¯»ç¬”è®°","status":"pending","activeForm":"æ­£åœ¨æ’°å†™ç¬”è®°"}],"newTodos":[{"content":"æœç´¢å¹¶è®¿é—®DLinearè®ºæ–‡","status":"completed","activeForm":"å·²å®Œæˆæœç´¢DLinearè®ºæ–‡"},{"content":"é˜…è¯»å¹¶ç†è§£è®ºæ–‡æ ¸å¿ƒå†…å®¹","status":"in_progress","activeForm":"æ­£åœ¨é˜…è¯»è®ºæ–‡"},{"content":"æ•´ç†æ¨¡å‹æ¶æ„å’Œæ•°å­¦åŸç†","status":"pending","activeForm":"æ­£åœ¨æ•´ç†æ¨¡å‹æ¶æ„"},{"content":"è®°å½•å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡","status":"pending","activeForm":"æ­£åœ¨è®°å½•å®éªŒè®¾ç½®"},{"content":"æ’°å†™å®Œæ•´çš„è®ºæ–‡é˜…è¯»ç¬”è®°","status":"pending","activeForm":"æ­£åœ¨æ’°å†™ç¬”è®°"}]},"sourceToolAssistantUUID":"00b2349c-3acc-46aa-8742-7918412c5047"}
{"parentUuid":"7eecefb5-3f14-4fa5-9d93-38f8048626d2","isSidechain":false,"userType":"external","cwd":"/app/workspace","sessionId":"f068886d-0e50-4922-9128-2594a90f9c43","version":"2.1.50","gitBranch":"HEAD","message":{"id":"msg_202602232050559b25f95319a748ea","type":"message","role":"assistant","model":"glm-5","content":[{"type":"thinking","thinking":"æˆ‘å·²ç»è·å–äº†è®ºæ–‡çš„å®Œæ•´å†…å®¹å’ŒGitHubä»“åº“ä¿¡æ¯ã€‚ç°åœ¨è®©æˆ‘åˆ›å»ºä¸€ä¸ªå®Œæ•´çš„è®ºæ–‡é˜…è¯»ç¬”è®°ã€‚æˆ‘éœ€è¦å°†è¿™äº›ä¿¡æ¯æ•´ç†æˆä¸€ä»½ç³»ç»Ÿæ€§çš„ç¬”è®°æ–‡ä»¶ã€‚","signature":""}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"output_tokens":0}},"type":"assistant","uuid":"f9d65089-3dca-4a77-8370-8022d28faec7","timestamp":"2026-02-23T12:51:08.505Z"}
{"parentUuid":"f9d65089-3dca-4a77-8370-8022d28faec7","isSidechain":false,"userType":"external","cwd":"/app/workspace","sessionId":"f068886d-0e50-4922-9128-2594a90f9c43","version":"2.1.50","gitBranch":"HEAD","message":{"id":"msg_202602232050559b25f95319a748ea","type":"message","role":"assistant","model":"glm-5","content":[{"type":"text","text":"ç°åœ¨è®©æˆ‘æ•´ç†å¹¶æ’°å†™å®Œæ•´çš„è®ºæ–‡é˜…è¯»ç¬”è®°ã€‚"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"output_tokens":0}},"type":"assistant","uuid":"011c4b74-5817-409d-8e88-5be0fb994610","timestamp":"2026-02-23T12:51:12.597Z"}
{"parentUuid":"011c4b74-5817-409d-8e88-5be0fb994610","isSidechain":false,"userType":"external","cwd":"/app/workspace","sessionId":"f068886d-0e50-4922-9128-2594a90f9c43","version":"2.1.50","gitBranch":"HEAD","message":{"id":"msg_202602232050559b25f95319a748ea","type":"message","role":"assistant","model":"glm-5","content":[{"type":"tool_use","id":"call_c1cd118e837d426db2b15e41","name":"TodoWrite","input":{"todos":[{"activeForm":"å·²å®Œæˆæœç´¢DLinearè®ºæ–‡","content":"æœç´¢å¹¶è®¿é—®DLinearè®ºæ–‡","status":"completed"},{"activeForm":"å·²å®Œæˆé˜…è¯»è®ºæ–‡","content":"é˜…è¯»å¹¶ç†è§£è®ºæ–‡æ ¸å¿ƒå†…å®¹","status":"completed"},{"activeForm":"æ­£åœ¨æ•´ç†æ¨¡å‹æ¶æ„","content":"æ•´ç†æ¨¡å‹æ¶æ„å’Œæ•°å­¦åŸç†","status":"in_progress"},{"activeForm":"æ­£åœ¨è®°å½•å®éªŒè®¾ç½®","content":"è®°å½•å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡","status":"pending"},{"activeForm":"æ­£åœ¨æ’°å†™ç¬”è®°","content":"æ’°å†™å®Œæ•´çš„è®ºæ–‡é˜…è¯»ç¬”è®°","status":"pending"}]}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"output_tokens":0}},"type":"assistant","uuid":"fe834b02-0893-4d72-b4d7-41e710bc59ae","timestamp":"2026-02-23T12:51:12.598Z"}
{"parentUuid":"fe834b02-0893-4d72-b4d7-41e710bc59ae","isSidechain":false,"userType":"external","cwd":"/app/workspace","sessionId":"f068886d-0e50-4922-9128-2594a90f9c43","version":"2.1.50","gitBranch":"HEAD","type":"user","message":{"role":"user","content":[{"tool_use_id":"call_c1cd118e837d426db2b15e41","type":"tool_result","content":"Todos have been modified successfully. Ensure that you continue to use the todo list to track your progress. Please proceed with the current tasks if applicable"}]},"uuid":"ba78048f-0bff-4090-a3b7-c4536af57736","timestamp":"2026-02-23T12:51:12.636Z","toolUseResult":{"oldTodos":[{"content":"æœç´¢å¹¶è®¿é—®DLinearè®ºæ–‡","status":"completed","activeForm":"å·²å®Œæˆæœç´¢DLinearè®ºæ–‡"},{"content":"é˜…è¯»å¹¶ç†è§£è®ºæ–‡æ ¸å¿ƒå†…å®¹","status":"in_progress","activeForm":"æ­£åœ¨é˜…è¯»è®ºæ–‡"},{"content":"æ•´ç†æ¨¡å‹æ¶æ„å’Œæ•°å­¦åŸç†","status":"pending","activeForm":"æ­£åœ¨æ•´ç†æ¨¡å‹æ¶æ„"},{"content":"è®°å½•å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡","status":"pending","activeForm":"æ­£åœ¨è®°å½•å®éªŒè®¾ç½®"},{"content":"æ’°å†™å®Œæ•´çš„è®ºæ–‡é˜…è¯»ç¬”è®°","status":"pending","activeForm":"æ­£åœ¨æ’°å†™ç¬”è®°"}],"newTodos":[{"content":"æœç´¢å¹¶è®¿é—®DLinearè®ºæ–‡","status":"completed","activeForm":"å·²å®Œæˆæœç´¢DLinearè®ºæ–‡"},{"content":"é˜…è¯»å¹¶ç†è§£è®ºæ–‡æ ¸å¿ƒå†…å®¹","status":"completed","activeForm":"å·²å®Œæˆé˜…è¯»è®ºæ–‡"},{"content":"æ•´ç†æ¨¡å‹æ¶æ„å’Œæ•°å­¦åŸç†","status":"in_progress","activeForm":"æ­£åœ¨æ•´ç†æ¨¡å‹æ¶æ„"},{"content":"è®°å½•å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡","status":"pending","activeForm":"æ­£åœ¨è®°å½•å®éªŒè®¾ç½®"},{"content":"æ’°å†™å®Œæ•´çš„è®ºæ–‡é˜…è¯»ç¬”è®°","status":"pending","activeForm":"æ­£åœ¨æ’°å†™ç¬”è®°"}]},"sourceToolAssistantUUID":"fe834b02-0893-4d72-b4d7-41e710bc59ae"}
{"parentUuid":"ba78048f-0bff-4090-a3b7-c4536af57736","isSidechain":false,"userType":"external","cwd":"/app/workspace","sessionId":"f068886d-0e50-4922-9128-2594a90f9c43","version":"2.1.50","gitBranch":"HEAD","message":{"id":"msg_20260223205112a516f80bdfdb47ea","type":"message","role":"assistant","model":"glm-5","content":[{"type":"thinking","thinking":"è®©æˆ‘ç°åœ¨åˆ›å»ºå®Œæ•´çš„è®ºæ–‡é˜…è¯»ç¬”è®°æ–‡ä»¶ã€‚","signature":""}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"output_tokens":0}},"type":"assistant","uuid":"5e25f321-1cd6-46ee-bff2-22f6ae697cba","timestamp":"2026-02-23T12:53:13.470Z"}
{"parentUuid":"5e25f321-1cd6-46ee-bff2-22f6ae697cba","isSidechain":false,"userType":"external","cwd":"/app/workspace","sessionId":"f068886d-0e50-4922-9128-2594a90f9c43","version":"2.1.50","gitBranch":"HEAD","message":{"id":"msg_20260223205112a516f80bdfdb47ea","type":"message","role":"assistant","model":"glm-5","content":[{"type":"tool_use","id":"call_dbeb32664da5471397cdba2c","name":"Write","input":{"file_path":"/app/workspace/DLinear_Paper_Notes.md","content":"# DLinear è®ºæ–‡é˜…è¯»ç¬”è®°\n\n## è®ºæ–‡åŸºæœ¬ä¿¡æ¯\n\n| é¡¹ç›® | å†…å®¹ |\n|------|------|\n| **æ ‡é¢˜** | Are Transformers Effective for Time Series Forecasting? |\n| **arXiv ID** | 2205.13504 |\n| **ä¼šè®®** | AAAI 2023 (Oral, ä¸‰å¼ºæ¥å—) |\n| **ä½œè€…** | Ailing Zeng, Muxi Chen, Lei Zhang, Qiang Xu |\n| **æœºæ„** | é¦™æ¸¯ä¸­æ–‡å¤§å­¦ (CUHK), å›½é™…æ•°å­—ç»æµå­¦é™¢ (IDEA) |\n| **ä»£ç ä»“åº“** | https://github.com/cure-lab/LTSF-Linear |\n| **æäº¤æ—¥æœŸ** | 2022å¹´5æœˆ26æ—¥ (æœ€åä¿®è®¢: 2022å¹´8æœˆ17æ—¥) |\n\n---\n\n## 1. ç ”ç©¶èƒŒæ™¯ä¸åŠ¨æœº\n\n### 1.1 é—®é¢˜æå‡º\nè¿‘å¹´æ¥ï¼Œå¤§é‡åŸºäºTransformerçš„æ—¶é—´åºåˆ—é•¿æœŸé¢„æµ‹(LTSF)æ¨¡å‹æ¶Œç°ï¼ŒåŒ…æ‹¬ï¼š\n- **LogTrans** (NeurIPS 2019)\n- **Informer** (AAAI 2021 Best Paper)\n- **Autoformer** (NeurIPS 2021)\n- **Pyraformer** (ICLR 2022 Oral)\n- **FEDformer** (ICML 2022)\n\n**æ ¸å¿ƒé—®é¢˜**ï¼šTransformerçœŸçš„å¯¹æ—¶é—´åºåˆ—é¢„æµ‹æœ‰æ•ˆå—ï¼Ÿ\n\n### 1.2 ä½œè€…çš„è´¨ç–‘\n1. **è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„å±€é™æ€§**ï¼šè‡ªæ³¨æ„åŠ›æœºåˆ¶æ˜¯**æ’åˆ—ä¸å˜çš„(permutation-invariant)**ï¼Œå¯¹é¡ºåºä¸æ•æ„Ÿ\n2. **æ—¶é—´ä¿¡æ¯ä¸¢å¤±**ï¼šå°½ç®¡ä½¿ç”¨ä½ç½®ç¼–ç å¯ä»¥ä¿ç•™éƒ¨åˆ†é¡ºåºä¿¡æ¯ï¼Œä½†è‡ªæ³¨æ„åŠ›çš„æœ¬è´¨ä¼šå¯¼è‡´æ—¶é—´ä¿¡æ¯ä¸¢å¤±\n3. **æ—¶é—´åºåˆ—çš„ç‰¹æ€§**ï¼šä¸NLPä¸åŒï¼Œæ—¶é—´åºåˆ—æ•°æ®æœ¬èº«ç¼ºä¹è¯­ä¹‰ä¿¡æ¯ï¼Œä¸»è¦å…³æ³¨è¿ç»­ç‚¹ä¹‹é—´çš„æ—¶é—´å…³ç³»\n4. **åŸºå‡†æ–¹æ³•çš„é—®é¢˜**ï¼šä¹‹å‰çš„å¯¹æ¯”å®éªŒä¸­ï¼ŒéTransformeråŸºçº¿éƒ½ä½¿ç”¨äº†è‡ªå›å½’/è¿­ä»£å¤šæ­¥é¢„æµ‹(IMS)ï¼Œå­˜åœ¨è¯¯å·®ç´¯ç§¯é—®é¢˜\n\n---\n\n## 2. LTSF-Linear æ¨¡å‹å®¶æ—\n\n### 2.1 é—®é¢˜å½¢å¼åŒ–å®šä¹‰\n\nç»™å®šåŒ…å«Cä¸ªå˜é‡çš„æ—¶é—´åºåˆ—ï¼Œå†å²æ•°æ®ä¸º $X = \\{X_t^1, ..., X_t^C\\}_{t=1}^L$ï¼š\n- $L$ = å›çœ‹çª—å£å¤§å° (look-back window size)\n- $X_t^i$ = ç¬¬iä¸ªå˜é‡åœ¨æ—¶åˆ»tçš„å€¼\n- ç›®æ ‡ï¼šé¢„æµ‹æœªæ¥Tä¸ªæ—¶é—´æ­¥çš„å€¼ $\\hat{X} = \\{\\hat{X}_t^1, ..., \\hat{X}_t^C\\}_{t=L+1}^{L+T}$\n\n### 2.2 é¢„æµ‹ç­–ç•¥å¯¹æ¯”\n\n| ç­–ç•¥ | æè¿° | ä¼˜ç‚¹ | ç¼ºç‚¹ |\n|------|------|------|------|\n| **IMS** (è¿­ä»£å¤šæ­¥) | å­¦ä¹ å•æ­¥é¢„æµ‹å™¨ï¼Œè¿­ä»£åº”ç”¨ | æ–¹å·®è¾ƒå° | è¯¯å·®ç´¯ç§¯ |\n| **DMS** (ç›´æ¥å¤šæ­¥) | ç›´æ¥ä¼˜åŒ–å¤šæ­¥é¢„æµ‹ç›®æ ‡ | æ— è¯¯å·®ç´¯ç§¯ï¼Œé€‚åˆé•¿æœŸé¢„æµ‹ | - |\n\n### 2.3 LTSF-Linear æ¨¡å‹æ¶æ„\n\n#### 2.3.1 åŸºç¡€Linearæ¨¡å‹\n```\næ•°å­¦è¡¨è¾¾ï¼š\\hat{X}_i = W X_i\nå…¶ä¸­ W \\in R^{T \\times L} æ˜¯æ²¿æ—¶é—´è½´çš„çº¿æ€§å±‚\n```\n- ç‰¹ç‚¹ï¼šè·¨å˜é‡å…±äº«æƒé‡ï¼Œä¸å»ºæ¨¡ç©ºé—´ç›¸å…³æ€§\n- å‚æ•°é‡ï¼š$T \\times L$\n\n#### 2.3.2 DLinear (Decomposition Linear)\n\n**æ ¸å¿ƒæ€æƒ³**ï¼šå°†æ—¶é—´åºåˆ—åˆ†è§£ä¸ºè¶‹åŠ¿å’Œå­£èŠ‚æ€§åˆ†é‡ï¼Œåˆ†åˆ«ç”¨çº¿æ€§å±‚å¤„ç†\n\n```\næ¶æ„æµç¨‹ï¼š\nè¾“å…¥ X\n    â†“\nç§»åŠ¨å¹³å‡åˆ†è§£\n    â†“\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  è¶‹åŠ¿åˆ†é‡       â”‚  æ®‹å·®(å­£èŠ‚)åˆ†é‡  â”‚\nâ”‚  X_trend       â”‚  X_seasonal     â”‚\nâ”‚  = AvgPool(X)  â”‚  = X - X_trend  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n         â†“                  â†“\n    Linear Layer      Linear Layer\n         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                  â†“\n              ç›¸åŠ æ±‚å’Œ\n                  â†“\n              é¢„æµ‹è¾“å‡º\n```\n\n**å…³é”®å‚æ•°**ï¼š\n- ç§»åŠ¨å¹³å‡æ ¸å¤§å°ï¼š25 (ä¸Autoformerä¸€è‡´)\n- å‚æ•°é‡ï¼š$2 \\times T \\times L$\n\n**è®¾è®¡åŠ¨æœº**ï¼šé€šè¿‡æ˜¾å¼å¤„ç†è¶‹åŠ¿ï¼ŒDLinearåœ¨å…·æœ‰æ˜æ˜¾è¶‹åŠ¿çš„æ•°æ®ä¸Šå¢å¼ºæ€§èƒ½\n\n#### 2.3.3 NLinear (Normalized Linear)\n\n**æ ¸å¿ƒæ€æƒ³**ï¼šå¤„ç†è®­ç»ƒ-æµ‹è¯•é›†ä¹‹é—´çš„åˆ†å¸ƒåç§»\n\n```\nå¤„ç†æµç¨‹ï¼š\n1. å‡å»æœ€åä¸€ä¸ªå€¼ï¼šX' = X - X_{last}\n2. é€šè¿‡çº¿æ€§å±‚ï¼šH = Linear(X')\n3. åŠ å›å‡å»çš„å€¼ï¼š\\hat{X} = H + X_{last}\n```\n\n**è®¾è®¡åŠ¨æœº**ï¼š\n- å½“å­˜åœ¨åˆ†å¸ƒåç§»æ—¶ï¼Œç®€å•çš„å½’ä¸€åŒ–å¯ä»¥æ˜¾è‘—æå‡æ€§èƒ½\n- å‡æ³•å’ŒåŠ æ³•æ“ä½œæ˜¯å¯¹è¾“å…¥åºåˆ—çš„ç®€å•å½’ä¸€åŒ–\n\n### 2.4 æ¨¡å‹ç‰¹æ€§æ€»ç»“\n\n| ç‰¹æ€§ | è¯´æ˜ |\n|------|------|\n| **O(1)æœ€å¤§ä¿¡å·è·¯å¾„é•¿åº¦** | è·¯å¾„è¶ŠçŸ­ï¼Œä¾èµ–å…³ç³»æ•è·è¶Šå¥½ï¼Œèƒ½åŒæ—¶æ•è·çŸ­æœŸå’Œé•¿æœŸæ—¶é—´å…³ç³» |\n| **é«˜æ•ˆç‡** | å†…å­˜å ç”¨ä½ï¼Œå‚æ•°å°‘ï¼Œæ¨ç†é€Ÿåº¦å¿« |\n| **å¯è§£é‡Šæ€§** | è®­ç»ƒåå¯å¯è§†åŒ–æƒé‡ï¼Œæ´å¯Ÿé¢„æµ‹å€¼ |\n| **æ˜“ç”¨æ€§** | æ— éœ€è°ƒæ•´æ¨¡å‹è¶…å‚æ•° |\n\n---\n\n## 3. å®éªŒè®¾ç½®\n\n### 3.1 æ•°æ®é›†è¯¦æƒ…\n\n| æ•°æ®é›† | å˜é‡æ•° | æ—¶é—´æ­¥æ•° | ç²’åº¦ | æ—¶é—´èŒƒå›´ |\n|--------|--------|----------|------|----------|\n| ETTh1 | 7 | 17,420 | 1å°æ—¶ | 2016.7-2018.7 |\n| ETTh2 | 7 | 17,420 | 1å°æ—¶ | 2016.7-2018.7 |\n| ETTm1 | 7 | 69,680 | 5åˆ†é’Ÿ | 2016.7-2018.7 |\n| ETTm2 | 7 | 69,680 | 5åˆ†é’Ÿ | 2016.7-2018.7 |\n| Traffic | 862 | 17,544 | 1å°æ—¶ | 2015-2016 |\n| Electricity | 321 | 26,304 | 1å°æ—¶ | 2012-2014 |\n| Exchange-Rate | 8 | 7,588 | 1å¤© | 1990-2016 |\n| Weather | 21 | 52,696 | 10åˆ†é’Ÿ | 2020å¹´ |\n| ILI | 7 | 966 | 1å‘¨ | 2002-2021 |\n\n### 3.2 è¯„ä¼°æŒ‡æ ‡\n\n- **MSE** (Mean Squared Error) - å‡æ–¹è¯¯å·®\n- **MAE** (Mean Absolute Error) - å¹³å‡ç»å¯¹è¯¯å·®\n\n### 3.3 é¢„æµ‹é•¿åº¦è®¾ç½®\n\n| æ•°æ®é›† | é¢„æµ‹é•¿åº¦ T |\n|--------|-----------|\n| ILI | {24, 36, 48, 60} |\n| å…¶ä»– | {96, 192, 336, 720} |\n\n### 3.4 å›çœ‹çª—å£è®¾ç½®\n\n- **LTSF-Linear é»˜è®¤**ï¼šL = 336\n- **Transformers é»˜è®¤**ï¼šL = 96 (è¾“å…¥é•¿åº¦96å¯¹å¤§å¤šæ•°Transformersæœ€åˆé€‚)\n\n---\n\n## 4. æ ¸å¿ƒå®éªŒç»“æœ\n\n### 4.1 å¤šå˜é‡é¢„æµ‹ç»“æœæ‘˜è¦\n\nLTSF-Linear vs æœ€ä½³Transformer (FEDformer) çš„æ€§èƒ½æå‡ï¼š\n\n| æ•°æ®é›† | å¹³å‡æå‡ | æœ€å¤§æå‡ |\n|--------|---------|---------|\n| Electricity | 17-27% | - |\n| Exchange-Rate | 33-46% | 46% (T=720) |\n| Traffic | 25-30% | - |\n| Weather | 18-22% | - |\n| ILI | 34-47% | 47% (T=24) |\n| ETTh2 | 14-25% | - |\n| ETTm1 | 17-21% | - |\n\n### 4.2 å…³é”®å‘ç°\n\n1. **LTSF-Linearå…¨é¢è¶…è¶ŠTransformer**ï¼šåœ¨æ‰€æœ‰9ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šï¼ŒLTSF-Linearéƒ½ä¼˜äºå¤æ‚çš„Transformeræ¨¡å‹\n\n2. **NLinearå¤„ç†åˆ†å¸ƒåç§»**ï¼šåœ¨ETTh1ã€ETTh2ã€ILIç­‰æœ‰æ˜æ˜¾åˆ†å¸ƒåç§»çš„æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚\n\n3. **DLinearå¤„ç†è¶‹åŠ¿**ï¼šåœ¨å…·æœ‰æ˜æ˜¾è¶‹åŠ¿çš„æ•°æ®é›†ä¸Šè¡¨ç°æ›´å¥½\n\n4. **RepeatåŸºçº¿çš„æ„å¤–å‘ç°**ï¼šç®€å•çš„é‡å¤æœ€åä¸€ä¸ªå€¼åœ¨Exchange-Rateä¸Šè¶…è¿‡æ‰€æœ‰Transformerçº¦45%\n\n### 4.3 æ•ˆç‡å¯¹æ¯” (Electricity, L=96, T=720)\n\n| æ¨¡å‹ | MACs | å‚æ•°é‡ | æ¨ç†æ—¶é—´ | å†…å­˜ |\n|------|------|--------|----------|------|\n| **DLinear** | 0.04G | 139.7K | 0.4ms | 687MiB |\n| Transformer | 4.03G | 13.61M | 26.8ms | 6091MiB |\n| Informer | 3.93G | 14.39M | 49.3ms | 3869MiB |\n| Autoformer | 4.41G | 14.91M | 164.1ms | 7607MiB |\n| FEDformer | 4.41G | 20.68M | 40.5ms | 4143MiB |\n\n---\n\n## 5. æ·±å…¥åˆ†æ\n\n### 5.1 å›çœ‹çª—å£å¤§å°çš„å½±å“\n\n**å®éªŒè®¾ç½®**ï¼šL âˆˆ {24, 48, 72, 96, 120, 144, 168, 192, 336, 504, 672, 720}, T = 720\n\n**å…³é”®å‘ç°**ï¼š\n- **LTSF-Linear**ï¼šéšç€å›çœ‹çª—å£å¢å¤§ï¼Œæ€§èƒ½æ˜¾è‘—æå‡\n- **Transformers**ï¼šéšç€å›çœ‹çª—å£å¢å¤§ï¼Œæ€§èƒ½**ä¸‹é™æˆ–ä¿æŒç¨³å®š**\n\n**ç»“è®º**ï¼šç°æœ‰Transformerå€¾å‘äºè¿‡æ‹Ÿåˆæ—¶é—´å™ªå£°ï¼Œè€Œéæå–æ—¶é—´ä¿¡æ¯ï¼›è¾“å…¥é•¿åº¦96æ­£å¥½é€‚åˆå¤§å¤šæ•°Transformers\n\n### 5.2 è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„æœ‰æ•ˆæ€§\n\n**æ¶ˆèå®éªŒ**ï¼šé€æ­¥å°†Informerç®€åŒ–ä¸ºLinear\n\n| é˜¶æ®µ | Exchange (T=96) MSE | ETTh1 (T=96) MSE |\n|------|---------------------|------------------|\n| Informer | 0.847 | 0.865 |\n| Att.-Linear | 1.003 | 0.613 |\n| Embed + Linear | 0.173 | 0.454 |\n| **Linear** | **0.084** | **0.400** |\n\n**ç»“è®º**ï¼šéšç€ç®€åŒ–ç¨‹åº¦å¢åŠ ï¼Œæ€§èƒ½åè€Œæå‡ï¼Œè¯´æ˜è‡ªæ³¨æ„åŠ›æœºåˆ¶å’Œå…¶ä»–å¤æ‚æ¨¡å—åœ¨ç°æœ‰LTSFåŸºå‡†ä¸Šæ˜¯ä¸å¿…è¦çš„\n\n### 5.3 æ—¶é—´é¡ºåºä¿æŒèƒ½åŠ›\n\n**å®éªŒè®¾è®¡**ï¼šæ‰“ä¹±è¾“å…¥åºåˆ—æµ‹è¯•æ¨¡å‹å¯¹é¡ºåºçš„æ•æ„Ÿåº¦\n- **Shuf.**ï¼šéšæœºæ‰“ä¹±æ•´ä¸ªè¾“å…¥åºåˆ—\n- **Half-Ex.**ï¼šäº¤æ¢è¾“å…¥åºåˆ—çš„å‰åä¸¤åŠ\n\n**Exchange-Rateæ•°æ®é›†ç»“æœ**ï¼š\n| æ–¹æ³• | åŸå§‹ MSE | Shuf. MSE | æ€§èƒ½ä¸‹é™ |\n|------|----------|-----------|----------|\n| Linear | 0.080 | 0.133 | 27.26% |\n| FEDformer | 0.161 | 0.160 | -0.09% |\n| Autoformer | 0.152 | 0.158 | 0.09% |\n| Informer | 0.952 | 1.004 | -0.12% |\n\n**ç»“è®º**ï¼š\n- Transformerå¯¹è¾“å…¥é¡ºåºä¸æ•æ„Ÿï¼Œå³ä½¿éšæœºæ‰“ä¹±ä¹Ÿå‡ ä¹ä¸å½±å“æ€§èƒ½\n- Linearæ¨¡å‹å¯¹é¡ºåºéå¸¸æ•æ„Ÿï¼Œæ‰“ä¹±åæ€§èƒ½æ˜¾è‘—ä¸‹é™\n- ç°æœ‰Transformerä¿ç•™çš„æ—¶é—´å…³ç³»éå¸¸æœ‰é™\n\n### 5.4 åµŒå…¥ç­–ç•¥çš„å½±å“\n\n| æ–¹æ³• | è®¾ç½® | Traffic MSE (T=96/192/336/720) |\n|------|------|-------------------------------|\n| FEDformer | All | 0.597/0.606/0.627/0.649 |\n| FEDformer | wo/Pos. | 0.587/0.604/0.621/0.626 |\n| FEDformer | wo/Temp. | 0.613/0.623/0.650/0.677 |\n| Autoformer | All | 0.629/0.647/0.676/0.638 |\n| Autoformer | wo/Temp. | 0.681/0.665/0.908/0.769 |\n| Informer | All | 0.719/0.696/0.777/0.864 |\n| Informer | wo/Pos. | 1.035/1.186/1.307/1.472 |\n\n**ç»“è®º**ï¼š\n- ä½ç½®ç¼–ç å¯¹Informerè‡³å…³é‡è¦\n- æ—¶é—´æˆ³åµŒå…¥å¯¹Autoformerå½±å“è¾ƒå¤§\n- FEDformerå› å…¶é¢‘ç‡å¢å¼ºæ¨¡å—ï¼Œå¯¹åµŒå…¥ç­–ç•¥çš„ä¾èµ–è¾ƒå°\n\n---\n\n## 6. ä»£ç å®ç°è¦ç‚¹\n\n### 6.1 ç¯å¢ƒé…ç½®\n\n```bash\nconda create -n LTSF_Linear python=3.6.9\nconda activate LTSF_Linear\npip install -r requirements.txt\n```\n\n### 6.2 è¿è¡Œç¤ºä¾‹\n\n```bash\n# è®­ç»ƒDLinearåœ¨Exchange-Rateæ•°æ®é›†ä¸Š\nsh scripts/EXP-LongForecasting/Linear/exchange_rate.sh\n\n# é»˜è®¤å›çœ‹çª—å£ä¸º336ï¼Œå¯åœ¨è„šæœ¬ä¸­æŒ‡å®šæ¨¡å‹ç±»å‹\n# æ”¯æŒçš„æ¨¡å‹: Linear, DLinear, NLinear\n```\n\n### 6.3 å…³é”®è¶…å‚æ•°\n\n| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |\n|------|--------|------|\n| look-back window | 336 | å›çœ‹çª—å£å¤§å° |\n| moving avg kernel | 25 | DLinearçš„ç§»åŠ¨å¹³å‡æ ¸å¤§å° |\n| individual | False | æ˜¯å¦æ¯ä¸ªå˜é‡ä½¿ç”¨ç‹¬ç«‹çº¿æ€§å±‚ |\n\n### 6.4 æ•°æ®å‡†å¤‡\n\næ•°æ®é›†éœ€æ”¾ç½®åœ¨ `./dataset` ç›®å½•ä¸‹ï¼Œå¯ä»Autoformeré¡¹ç›®çš„Google Driveè·å–é¢„å¤„ç†å¥½çš„æ•°æ®ã€‚\n\n---\n\n## 7. è®ºæ–‡è´¡çŒ®æ€»ç»“\n\n### 7.1 ä¸»è¦è´¡çŒ®\n\n1. **é¦–æ¬¡æŒ‘æˆ˜Transformeråœ¨LTSFä»»åŠ¡ä¸Šçš„æœ‰æ•ˆæ€§**\n2. **æå‡ºLTSF-Linearä½œä¸ºæ–°çš„åŸºå‡†æ–¹æ³•**\n3. **å…¨é¢çš„å®è¯åˆ†æ**ï¼šæ¶µç›–é•¿è¾“å…¥å»ºæ¨¡èƒ½åŠ›ã€æ—¶é—´é¡ºåºæ•æ„Ÿæ€§ã€ä½ç½®ç¼–ç å½±å“ã€æ•ˆç‡å¯¹æ¯”ç­‰\n\n### 7.2 æ ¸å¿ƒç»“è®º\n\n> **Transformeråœ¨æ—¶é—´åºåˆ—ä¸Šçš„æ—¶åºå»ºæ¨¡èƒ½åŠ›è¢«å¤¸å¤§äº†ï¼Œè‡³å°‘åœ¨ç°æœ‰LTSFåŸºå‡†ä¸Šå¦‚æ­¤ã€‚**\n\n### 7.3 æœªæ¥ç ”ç©¶æ–¹å‘\n\n1. LTSF-Linearæ¨¡å‹å®¹é‡æœ‰é™ï¼Œä»…ä½œä¸ºç®€å•ä½†æœ‰ç«äº‰åŠ›çš„åŸºçº¿\n2. å•å±‚çº¿æ€§ç½‘ç»œéš¾ä»¥æ•è·ç”±å˜åŒ–ç‚¹å¼•èµ·çš„æ—¶åºåŠ¨æ€\n3. éœ€è¦æ–°çš„æ¨¡å‹è®¾è®¡ã€æ•°æ®å¤„ç†æ–¹æ³•å’ŒåŸºå‡†æµ‹è¯•\n\n---\n\n## 8. å¤ç°è®¡åˆ’è¦ç‚¹\n\n### 8.1 å…³é”®å®éªŒé…ç½®\n\n- [ ] å¤ç°ETTæ•°æ®é›†ä¸Šçš„å¤šå˜é‡é¢„æµ‹ç»“æœ\n- [ ] éªŒè¯ä¸åŒå›çœ‹çª—å£å¤§å°çš„å½±å“\n- [ ] å¯¹æ¯”Linear/DLinear/NLinearçš„æ€§èƒ½å·®å¼‚\n- [ ] æµ‹è¯•è¾“å…¥æ‰“ä¹±å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“\n\n### 8.2 é¢„æœŸç»“æœå¯¹æ¯”åŸºå‡†\n\nä»¥ETTh1å¤šå˜é‡é¢„æµ‹ä¸ºä¾‹ï¼Œè®ºæ–‡æŠ¥å‘Šçš„MSEï¼š\n\n| T | Linear | NLinear | DLinear | FEDformer |\n|---|--------|---------|---------|-----------|\n| 96 | 0.375 | 0.374 | 0.375 | 0.376 |\n| 192 | 0.418 | 0.408 | 0.405 | 0.420 |\n| 336 | 0.479 | 0.429 | 0.439 | 0.459 |\n| 720 | 0.624 | 0.440 | 0.472 | 0.506 |\n\n### 8.3 å¤ç°æˆåŠŸæ ‡å‡†\n\n- ä¸è®ºæ–‡ç»“æœçš„ç›¸å¯¹è¯¯å·®åœ¨ **Â±5%** ä»¥å†…è§†ä¸ºå¤ç°æˆåŠŸ\n\n---\n\n## 9. å‚è€ƒæ–‡çŒ®\n\n1. Vaswani et al. \"Attention is All You Need\" (NeurIPS 2017)\n2. Zhou et al. \"Informer\" (AAAI 2021)\n3. Wu et al. \"Autoformer\" (NeurIPS 2021)\n4. Liu et al. \"Pyraformer\" (ICLR 2022)\n5. Zhou et al. \"FEDformer\" (ICML 2022)\n\n---\n\n## é™„å½•ï¼šæ¨¡å‹æƒé‡å¯è§†åŒ–\n\nDLinearçš„æƒé‡å¯ä»¥æ­ç¤ºæ•°æ®ç‰¹å¾ï¼š\n- **è¶‹åŠ¿å±‚æƒé‡**ï¼šæ˜¾ç¤ºå“ªäº›å†å²æ—¶é—´æ­¥å¯¹é¢„æµ‹è´¡çŒ®æ›´å¤§\n- **æ®‹å·®å±‚æƒé‡**ï¼šå¯æ­ç¤ºæ•°æ®çš„å‘¨æœŸæ€§æ¨¡å¼\n\nä¾‹å¦‚ï¼ŒTrafficæ•°æ®é›†çš„æƒé‡å¯è§†åŒ–æ˜¾ç¤ºï¼š\n- æ—¥å‘¨æœŸæ€§ï¼ˆ24ä¸ªæ—¶é—´æ­¥ï¼‰\n- å‘¨å‘¨æœŸæ€§ï¼ˆ168ä¸ªæ—¶é—´æ­¥ï¼‰\n\nè¿™è¯æ˜äº†Linearæ¨¡å‹çš„å¯è§£é‡Šæ€§ä¼˜åŠ¿ã€‚\n\n---\n\n*ç¬”è®°æ•´ç†æ—¥æœŸï¼š2026å¹´2æœˆ23æ—¥*\n*è®ºæ–‡ç‰ˆæœ¬ï¼šarXiv:2205.13504v3*\n"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"output_tokens":0}},"type":"assistant","uuid":"2ef679c5-96e0-4386-8669-9833c30b23b2","timestamp":"2026-02-23T12:53:13.503Z"}
{"parentUuid":"2ef679c5-96e0-4386-8669-9833c30b23b2","isSidechain":false,"userType":"external","cwd":"/app/workspace","sessionId":"f068886d-0e50-4922-9128-2594a90f9c43","version":"2.1.50","gitBranch":"HEAD","slug":"functional-sparking-llama","type":"user","message":{"role":"user","content":[{"tool_use_id":"call_dbeb32664da5471397cdba2c","type":"tool_result","content":"File created successfully at: /app/workspace/DLinear_Paper_Notes.md"}]},"uuid":"da137d5a-f232-406c-b380-dadc82fd42a7","timestamp":"2026-02-23T12:53:13.519Z","toolUseResult":{"type":"create","filePath":"/app/workspace/DLinear_Paper_Notes.md","content":"# DLinear è®ºæ–‡é˜…è¯»ç¬”è®°\n\n## è®ºæ–‡åŸºæœ¬ä¿¡æ¯\n\n| é¡¹ç›® | å†…å®¹ |\n|------|------|\n| **æ ‡é¢˜** | Are Transformers Effective for Time Series Forecasting? |\n| **arXiv ID** | 2205.13504 |\n| **ä¼šè®®** | AAAI 2023 (Oral, ä¸‰å¼ºæ¥å—) |\n| **ä½œè€…** | Ailing Zeng, Muxi Chen, Lei Zhang, Qiang Xu |\n| **æœºæ„** | é¦™æ¸¯ä¸­æ–‡å¤§å­¦ (CUHK), å›½é™…æ•°å­—ç»æµå­¦é™¢ (IDEA) |\n| **ä»£ç ä»“åº“** | https://github.com/cure-lab/LTSF-Linear |\n| **æäº¤æ—¥æœŸ** | 2022å¹´5æœˆ26æ—¥ (æœ€åä¿®è®¢: 2022å¹´8æœˆ17æ—¥) |\n\n---\n\n## 1. ç ”ç©¶èƒŒæ™¯ä¸åŠ¨æœº\n\n### 1.1 é—®é¢˜æå‡º\nè¿‘å¹´æ¥ï¼Œå¤§é‡åŸºäºTransformerçš„æ—¶é—´åºåˆ—é•¿æœŸé¢„æµ‹(LTSF)æ¨¡å‹æ¶Œç°ï¼ŒåŒ…æ‹¬ï¼š\n- **LogTrans** (NeurIPS 2019)\n- **Informer** (AAAI 2021 Best Paper)\n- **Autoformer** (NeurIPS 2021)\n- **Pyraformer** (ICLR 2022 Oral)\n- **FEDformer** (ICML 2022)\n\n**æ ¸å¿ƒé—®é¢˜**ï¼šTransformerçœŸçš„å¯¹æ—¶é—´åºåˆ—é¢„æµ‹æœ‰æ•ˆå—ï¼Ÿ\n\n### 1.2 ä½œè€…çš„è´¨ç–‘\n1. **è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„å±€é™æ€§**ï¼šè‡ªæ³¨æ„åŠ›æœºåˆ¶æ˜¯**æ’åˆ—ä¸å˜çš„(permutation-invariant)**ï¼Œå¯¹é¡ºåºä¸æ•æ„Ÿ\n2. **æ—¶é—´ä¿¡æ¯ä¸¢å¤±**ï¼šå°½ç®¡ä½¿ç”¨ä½ç½®ç¼–ç å¯ä»¥ä¿ç•™éƒ¨åˆ†é¡ºåºä¿¡æ¯ï¼Œä½†è‡ªæ³¨æ„åŠ›çš„æœ¬è´¨ä¼šå¯¼è‡´æ—¶é—´ä¿¡æ¯ä¸¢å¤±\n3. **æ—¶é—´åºåˆ—çš„ç‰¹æ€§**ï¼šä¸NLPä¸åŒï¼Œæ—¶é—´åºåˆ—æ•°æ®æœ¬èº«ç¼ºä¹è¯­ä¹‰ä¿¡æ¯ï¼Œä¸»è¦å…³æ³¨è¿ç»­ç‚¹ä¹‹é—´çš„æ—¶é—´å…³ç³»\n4. **åŸºå‡†æ–¹æ³•çš„é—®é¢˜**ï¼šä¹‹å‰çš„å¯¹æ¯”å®éªŒä¸­ï¼ŒéTransformeråŸºçº¿éƒ½ä½¿ç”¨äº†è‡ªå›å½’/è¿­ä»£å¤šæ­¥é¢„æµ‹(IMS)ï¼Œå­˜åœ¨è¯¯å·®ç´¯ç§¯é—®é¢˜\n\n---\n\n## 2. LTSF-Linear æ¨¡å‹å®¶æ—\n\n### 2.1 é—®é¢˜å½¢å¼åŒ–å®šä¹‰\n\nç»™å®šåŒ…å«Cä¸ªå˜é‡çš„æ—¶é—´åºåˆ—ï¼Œå†å²æ•°æ®ä¸º $X = \\{X_t^1, ..., X_t^C\\}_{t=1}^L$ï¼š\n- $L$ = å›çœ‹çª—å£å¤§å° (look-back window size)\n- $X_t^i$ = ç¬¬iä¸ªå˜é‡åœ¨æ—¶åˆ»tçš„å€¼\n- ç›®æ ‡ï¼šé¢„æµ‹æœªæ¥Tä¸ªæ—¶é—´æ­¥çš„å€¼ $\\hat{X} = \\{\\hat{X}_t^1, ..., \\hat{X}_t^C\\}_{t=L+1}^{L+T}$\n\n### 2.2 é¢„æµ‹ç­–ç•¥å¯¹æ¯”\n\n| ç­–ç•¥ | æè¿° | ä¼˜ç‚¹ | ç¼ºç‚¹ |\n|------|------|------|------|\n| **IMS** (è¿­ä»£å¤šæ­¥) | å­¦ä¹ å•æ­¥é¢„æµ‹å™¨ï¼Œè¿­ä»£åº”ç”¨ | æ–¹å·®è¾ƒå° | è¯¯å·®ç´¯ç§¯ |\n| **DMS** (ç›´æ¥å¤šæ­¥) | ç›´æ¥ä¼˜åŒ–å¤šæ­¥é¢„æµ‹ç›®æ ‡ | æ— è¯¯å·®ç´¯ç§¯ï¼Œé€‚åˆé•¿æœŸé¢„æµ‹ | - |\n\n### 2.3 LTSF-Linear æ¨¡å‹æ¶æ„\n\n#### 2.3.1 åŸºç¡€Linearæ¨¡å‹\n```\næ•°å­¦è¡¨è¾¾ï¼š\\hat{X}_i = W X_i\nå…¶ä¸­ W \\in R^{T \\times L} æ˜¯æ²¿æ—¶é—´è½´çš„çº¿æ€§å±‚\n```\n- ç‰¹ç‚¹ï¼šè·¨å˜é‡å…±äº«æƒé‡ï¼Œä¸å»ºæ¨¡ç©ºé—´ç›¸å…³æ€§\n- å‚æ•°é‡ï¼š$T \\times L$\n\n#### 2.3.2 DLinear (Decomposition Linear)\n\n**æ ¸å¿ƒæ€æƒ³**ï¼šå°†æ—¶é—´åºåˆ—åˆ†è§£ä¸ºè¶‹åŠ¿å’Œå­£èŠ‚æ€§åˆ†é‡ï¼Œåˆ†åˆ«ç”¨çº¿æ€§å±‚å¤„ç†\n\n```\næ¶æ„æµç¨‹ï¼š\nè¾“å…¥ X\n    â†“\nç§»åŠ¨å¹³å‡åˆ†è§£\n    â†“\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  è¶‹åŠ¿åˆ†é‡       â”‚  æ®‹å·®(å­£èŠ‚)åˆ†é‡  â”‚\nâ”‚  X_trend       â”‚  X_seasonal     â”‚\nâ”‚  = AvgPool(X)  â”‚  = X - X_trend  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n         â†“                  â†“\n    Linear Layer      Linear Layer\n         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                  â†“\n              ç›¸åŠ æ±‚å’Œ\n                  â†“\n              é¢„æµ‹è¾“å‡º\n```\n\n**å…³é”®å‚æ•°**ï¼š\n- ç§»åŠ¨å¹³å‡æ ¸å¤§å°ï¼š25 (ä¸Autoformerä¸€è‡´)\n- å‚æ•°é‡ï¼š$2 \\times T \\times L$\n\n**è®¾è®¡åŠ¨æœº**ï¼šé€šè¿‡æ˜¾å¼å¤„ç†è¶‹åŠ¿ï¼ŒDLinearåœ¨å…·æœ‰æ˜æ˜¾è¶‹åŠ¿çš„æ•°æ®ä¸Šå¢å¼ºæ€§èƒ½\n\n#### 2.3.3 NLinear (Normalized Linear)\n\n**æ ¸å¿ƒæ€æƒ³**ï¼šå¤„ç†è®­ç»ƒ-æµ‹è¯•é›†ä¹‹é—´çš„åˆ†å¸ƒåç§»\n\n```\nå¤„ç†æµç¨‹ï¼š\n1. å‡å»æœ€åä¸€ä¸ªå€¼ï¼šX' = X - X_{last}\n2. é€šè¿‡çº¿æ€§å±‚ï¼šH = Linear(X')\n3. åŠ å›å‡å»çš„å€¼ï¼š\\hat{X} = H + X_{last}\n```\n\n**è®¾è®¡åŠ¨æœº**ï¼š\n- å½“å­˜åœ¨åˆ†å¸ƒåç§»æ—¶ï¼Œç®€å•çš„å½’ä¸€åŒ–å¯ä»¥æ˜¾è‘—æå‡æ€§èƒ½\n- å‡æ³•å’ŒåŠ æ³•æ“ä½œæ˜¯å¯¹è¾“å…¥åºåˆ—çš„ç®€å•å½’ä¸€åŒ–\n\n### 2.4 æ¨¡å‹ç‰¹æ€§æ€»ç»“\n\n| ç‰¹æ€§ | è¯´æ˜ |\n|------|------|\n| **O(1)æœ€å¤§ä¿¡å·è·¯å¾„é•¿åº¦** | è·¯å¾„è¶ŠçŸ­ï¼Œä¾èµ–å…³ç³»æ•è·è¶Šå¥½ï¼Œèƒ½åŒæ—¶æ•è·çŸ­æœŸå’Œé•¿æœŸæ—¶é—´å…³ç³» |\n| **é«˜æ•ˆç‡** | å†…å­˜å ç”¨ä½ï¼Œå‚æ•°å°‘ï¼Œæ¨ç†é€Ÿåº¦å¿« |\n| **å¯è§£é‡Šæ€§** | è®­ç»ƒåå¯å¯è§†åŒ–æƒé‡ï¼Œæ´å¯Ÿé¢„æµ‹å€¼ |\n| **æ˜“ç”¨æ€§** | æ— éœ€è°ƒæ•´æ¨¡å‹è¶…å‚æ•° |\n\n---\n\n## 3. å®éªŒè®¾ç½®\n\n### 3.1 æ•°æ®é›†è¯¦æƒ…\n\n| æ•°æ®é›† | å˜é‡æ•° | æ—¶é—´æ­¥æ•° | ç²’åº¦ | æ—¶é—´èŒƒå›´ |\n|--------|--------|----------|------|----------|\n| ETTh1 | 7 | 17,420 | 1å°æ—¶ | 2016.7-2018.7 |\n| ETTh2 | 7 | 17,420 | 1å°æ—¶ | 2016.7-2018.7 |\n| ETTm1 | 7 | 69,680 | 5åˆ†é’Ÿ | 2016.7-2018.7 |\n| ETTm2 | 7 | 69,680 | 5åˆ†é’Ÿ | 2016.7-2018.7 |\n| Traffic | 862 | 17,544 | 1å°æ—¶ | 2015-2016 |\n| Electricity | 321 | 26,304 | 1å°æ—¶ | 2012-2014 |\n| Exchange-Rate | 8 | 7,588 | 1å¤© | 1990-2016 |\n| Weather | 21 | 52,696 | 10åˆ†é’Ÿ | 2020å¹´ |\n| ILI | 7 | 966 | 1å‘¨ | 2002-2021 |\n\n### 3.2 è¯„ä¼°æŒ‡æ ‡\n\n- **MSE** (Mean Squared Error) - å‡æ–¹è¯¯å·®\n- **MAE** (Mean Absolute Error) - å¹³å‡ç»å¯¹è¯¯å·®\n\n### 3.3 é¢„æµ‹é•¿åº¦è®¾ç½®\n\n| æ•°æ®é›† | é¢„æµ‹é•¿åº¦ T |\n|--------|-----------|\n| ILI | {24, 36, 48, 60} |\n| å…¶ä»– | {96, 192, 336, 720} |\n\n### 3.4 å›çœ‹çª—å£è®¾ç½®\n\n- **LTSF-Linear é»˜è®¤**ï¼šL = 336\n- **Transformers é»˜è®¤**ï¼šL = 96 (è¾“å…¥é•¿åº¦96å¯¹å¤§å¤šæ•°Transformersæœ€åˆé€‚)\n\n---\n\n## 4. æ ¸å¿ƒå®éªŒç»“æœ\n\n### 4.1 å¤šå˜é‡é¢„æµ‹ç»“æœæ‘˜è¦\n\nLTSF-Linear vs æœ€ä½³Transformer (FEDformer) çš„æ€§èƒ½æå‡ï¼š\n\n| æ•°æ®é›† | å¹³å‡æå‡ | æœ€å¤§æå‡ |\n|--------|---------|---------|\n| Electricity | 17-27% | - |\n| Exchange-Rate | 33-46% | 46% (T=720) |\n| Traffic | 25-30% | - |\n| Weather | 18-22% | - |\n| ILI | 34-47% | 47% (T=24) |\n| ETTh2 | 14-25% | - |\n| ETTm1 | 17-21% | - |\n\n### 4.2 å…³é”®å‘ç°\n\n1. **LTSF-Linearå…¨é¢è¶…è¶ŠTransformer**ï¼šåœ¨æ‰€æœ‰9ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šï¼ŒLTSF-Linearéƒ½ä¼˜äºå¤æ‚çš„Transformeræ¨¡å‹\n\n2. **NLinearå¤„ç†åˆ†å¸ƒåç§»**ï¼šåœ¨ETTh1ã€ETTh2ã€ILIç­‰æœ‰æ˜æ˜¾åˆ†å¸ƒåç§»çš„æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚\n\n3. **DLinearå¤„ç†è¶‹åŠ¿**ï¼šåœ¨å…·æœ‰æ˜æ˜¾è¶‹åŠ¿çš„æ•°æ®é›†ä¸Šè¡¨ç°æ›´å¥½\n\n4. **RepeatåŸºçº¿çš„æ„å¤–å‘ç°**ï¼šç®€å•çš„é‡å¤æœ€åä¸€ä¸ªå€¼åœ¨Exchange-Rateä¸Šè¶…è¿‡æ‰€æœ‰Transformerçº¦45%\n\n### 4.3 æ•ˆç‡å¯¹æ¯” (Electricity, L=96, T=720)\n\n| æ¨¡å‹ | MACs | å‚æ•°é‡ | æ¨ç†æ—¶é—´ | å†…å­˜ |\n|------|------|--------|----------|------|\n| **DLinear** | 0.04G | 139.7K | 0.4ms | 687MiB |\n| Transformer | 4.03G | 13.61M | 26.8ms | 6091MiB |\n| Informer | 3.93G | 14.39M | 49.3ms | 3869MiB |\n| Autoformer | 4.41G | 14.91M | 164.1ms | 7607MiB |\n| FEDformer | 4.41G | 20.68M | 40.5ms | 4143MiB |\n\n---\n\n## 5. æ·±å…¥åˆ†æ\n\n### 5.1 å›çœ‹çª—å£å¤§å°çš„å½±å“\n\n**å®éªŒè®¾ç½®**ï¼šL âˆˆ {24, 48, 72, 96, 120, 144, 168, 192, 336, 504, 672, 720}, T = 720\n\n**å…³é”®å‘ç°**ï¼š\n- **LTSF-Linear**ï¼šéšç€å›çœ‹çª—å£å¢å¤§ï¼Œæ€§èƒ½æ˜¾è‘—æå‡\n- **Transformers**ï¼šéšç€å›çœ‹çª—å£å¢å¤§ï¼Œæ€§èƒ½**ä¸‹é™æˆ–ä¿æŒç¨³å®š**\n\n**ç»“è®º**ï¼šç°æœ‰Transformerå€¾å‘äºè¿‡æ‹Ÿåˆæ—¶é—´å™ªå£°ï¼Œè€Œéæå–æ—¶é—´ä¿¡æ¯ï¼›è¾“å…¥é•¿åº¦96æ­£å¥½é€‚åˆå¤§å¤šæ•°Transformers\n\n### 5.2 è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„æœ‰æ•ˆæ€§\n\n**æ¶ˆèå®éªŒ**ï¼šé€æ­¥å°†Informerç®€åŒ–ä¸ºLinear\n\n| é˜¶æ®µ | Exchange (T=96) MSE | ETTh1 (T=96) MSE |\n|------|---------------------|------------------|\n| Informer | 0.847 | 0.865 |\n| Att.-Linear | 1.003 | 0.613 |\n| Embed + Linear | 0.173 | 0.454 |\n| **Linear** | **0.084** | **0.400** |\n\n**ç»“è®º**ï¼šéšç€ç®€åŒ–ç¨‹åº¦å¢åŠ ï¼Œæ€§èƒ½åè€Œæå‡ï¼Œè¯´æ˜è‡ªæ³¨æ„åŠ›æœºåˆ¶å’Œå…¶ä»–å¤æ‚æ¨¡å—åœ¨ç°æœ‰LTSFåŸºå‡†ä¸Šæ˜¯ä¸å¿…è¦çš„\n\n### 5.3 æ—¶é—´é¡ºåºä¿æŒèƒ½åŠ›\n\n**å®éªŒè®¾è®¡**ï¼šæ‰“ä¹±è¾“å…¥åºåˆ—æµ‹è¯•æ¨¡å‹å¯¹é¡ºåºçš„æ•æ„Ÿåº¦\n- **Shuf.**ï¼šéšæœºæ‰“ä¹±æ•´ä¸ªè¾“å…¥åºåˆ—\n- **Half-Ex.**ï¼šäº¤æ¢è¾“å…¥åºåˆ—çš„å‰åä¸¤åŠ\n\n**Exchange-Rateæ•°æ®é›†ç»“æœ**ï¼š\n| æ–¹æ³• | åŸå§‹ MSE | Shuf. MSE | æ€§èƒ½ä¸‹é™ |\n|------|----------|-----------|----------|\n| Linear | 0.080 | 0.133 | 27.26% |\n| FEDformer | 0.161 | 0.160 | -0.09% |\n| Autoformer | 0.152 | 0.158 | 0.09% |\n| Informer | 0.952 | 1.004 | -0.12% |\n\n**ç»“è®º**ï¼š\n- Transformerå¯¹è¾“å…¥é¡ºåºä¸æ•æ„Ÿï¼Œå³ä½¿éšæœºæ‰“ä¹±ä¹Ÿå‡ ä¹ä¸å½±å“æ€§èƒ½\n- Linearæ¨¡å‹å¯¹é¡ºåºéå¸¸æ•æ„Ÿï¼Œæ‰“ä¹±åæ€§èƒ½æ˜¾è‘—ä¸‹é™\n- ç°æœ‰Transformerä¿ç•™çš„æ—¶é—´å…³ç³»éå¸¸æœ‰é™\n\n### 5.4 åµŒå…¥ç­–ç•¥çš„å½±å“\n\n| æ–¹æ³• | è®¾ç½® | Traffic MSE (T=96/192/336/720) |\n|------|------|-------------------------------|\n| FEDformer | All | 0.597/0.606/0.627/0.649 |\n| FEDformer | wo/Pos. | 0.587/0.604/0.621/0.626 |\n| FEDformer | wo/Temp. | 0.613/0.623/0.650/0.677 |\n| Autoformer | All | 0.629/0.647/0.676/0.638 |\n| Autoformer | wo/Temp. | 0.681/0.665/0.908/0.769 |\n| Informer | All | 0.719/0.696/0.777/0.864 |\n| Informer | wo/Pos. | 1.035/1.186/1.307/1.472 |\n\n**ç»“è®º**ï¼š\n- ä½ç½®ç¼–ç å¯¹Informerè‡³å…³é‡è¦\n- æ—¶é—´æˆ³åµŒå…¥å¯¹Autoformerå½±å“è¾ƒå¤§\n- FEDformerå› å…¶é¢‘ç‡å¢å¼ºæ¨¡å—ï¼Œå¯¹åµŒå…¥ç­–ç•¥çš„ä¾èµ–è¾ƒå°\n\n---\n\n## 6. ä»£ç å®ç°è¦ç‚¹\n\n### 6.1 ç¯å¢ƒé…ç½®\n\n```bash\nconda create -n LTSF_Linear python=3.6.9\nconda activate LTSF_Linear\npip install -r requirements.txt\n```\n\n### 6.2 è¿è¡Œç¤ºä¾‹\n\n```bash\n# è®­ç»ƒDLinearåœ¨Exchange-Rateæ•°æ®é›†ä¸Š\nsh scripts/EXP-LongForecasting/Linear/exchange_rate.sh\n\n# é»˜è®¤å›çœ‹çª—å£ä¸º336ï¼Œå¯åœ¨è„šæœ¬ä¸­æŒ‡å®šæ¨¡å‹ç±»å‹\n# æ”¯æŒçš„æ¨¡å‹: Linear, DLinear, NLinear\n```\n\n### 6.3 å…³é”®è¶…å‚æ•°\n\n| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |\n|------|--------|------|\n| look-back window | 336 | å›çœ‹çª—å£å¤§å° |\n| moving avg kernel | 25 | DLinearçš„ç§»åŠ¨å¹³å‡æ ¸å¤§å° |\n| individual | False | æ˜¯å¦æ¯ä¸ªå˜é‡ä½¿ç”¨ç‹¬ç«‹çº¿æ€§å±‚ |\n\n### 6.4 æ•°æ®å‡†å¤‡\n\næ•°æ®é›†éœ€æ”¾ç½®åœ¨ `./dataset` ç›®å½•ä¸‹ï¼Œå¯ä»Autoformeré¡¹ç›®çš„Google Driveè·å–é¢„å¤„ç†å¥½çš„æ•°æ®ã€‚\n\n---\n\n## 7. è®ºæ–‡è´¡çŒ®æ€»ç»“\n\n### 7.1 ä¸»è¦è´¡çŒ®\n\n1. **é¦–æ¬¡æŒ‘æˆ˜Transformeråœ¨LTSFä»»åŠ¡ä¸Šçš„æœ‰æ•ˆæ€§**\n2. **æå‡ºLTSF-Linearä½œä¸ºæ–°çš„åŸºå‡†æ–¹æ³•**\n3. **å…¨é¢çš„å®è¯åˆ†æ**ï¼šæ¶µç›–é•¿è¾“å…¥å»ºæ¨¡èƒ½åŠ›ã€æ—¶é—´é¡ºåºæ•æ„Ÿæ€§ã€ä½ç½®ç¼–ç å½±å“ã€æ•ˆç‡å¯¹æ¯”ç­‰\n\n### 7.2 æ ¸å¿ƒç»“è®º\n\n> **Transformeråœ¨æ—¶é—´åºåˆ—ä¸Šçš„æ—¶åºå»ºæ¨¡èƒ½åŠ›è¢«å¤¸å¤§äº†ï¼Œè‡³å°‘åœ¨ç°æœ‰LTSFåŸºå‡†ä¸Šå¦‚æ­¤ã€‚**\n\n### 7.3 æœªæ¥ç ”ç©¶æ–¹å‘\n\n1. LTSF-Linearæ¨¡å‹å®¹é‡æœ‰é™ï¼Œä»…ä½œä¸ºç®€å•ä½†æœ‰ç«äº‰åŠ›çš„åŸºçº¿\n2. å•å±‚çº¿æ€§ç½‘ç»œéš¾ä»¥æ•è·ç”±å˜åŒ–ç‚¹å¼•èµ·çš„æ—¶åºåŠ¨æ€\n3. éœ€è¦æ–°çš„æ¨¡å‹è®¾è®¡ã€æ•°æ®å¤„ç†æ–¹æ³•å’ŒåŸºå‡†æµ‹è¯•\n\n---\n\n## 8. å¤ç°è®¡åˆ’è¦ç‚¹\n\n### 8.1 å…³é”®å®éªŒé…ç½®\n\n- [ ] å¤ç°ETTæ•°æ®é›†ä¸Šçš„å¤šå˜é‡é¢„æµ‹ç»“æœ\n- [ ] éªŒè¯ä¸åŒå›çœ‹çª—å£å¤§å°çš„å½±å“\n- [ ] å¯¹æ¯”Linear/DLinear/NLinearçš„æ€§èƒ½å·®å¼‚\n- [ ] æµ‹è¯•è¾“å…¥æ‰“ä¹±å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“\n\n### 8.2 é¢„æœŸç»“æœå¯¹æ¯”åŸºå‡†\n\nä»¥ETTh1å¤šå˜é‡é¢„æµ‹ä¸ºä¾‹ï¼Œè®ºæ–‡æŠ¥å‘Šçš„MSEï¼š\n\n| T | Linear | NLinear | DLinear | FEDformer |\n|---|--------|---------|---------|-----------|\n| 96 | 0.375 | 0.374 | 0.375 | 0.376 |\n| 192 | 0.418 | 0.408 | 0.405 | 0.420 |\n| 336 | 0.479 | 0.429 | 0.439 | 0.459 |\n| 720 | 0.624 | 0.440 | 0.472 | 0.506 |\n\n### 8.3 å¤ç°æˆåŠŸæ ‡å‡†\n\n- ä¸è®ºæ–‡ç»“æœçš„ç›¸å¯¹è¯¯å·®åœ¨ **Â±5%** ä»¥å†…è§†ä¸ºå¤ç°æˆåŠŸ\n\n---\n\n## 9. å‚è€ƒæ–‡çŒ®\n\n1. Vaswani et al. \"Attention is All You Need\" (NeurIPS 2017)\n2. Zhou et al. \"Informer\" (AAAI 2021)\n3. Wu et al. \"Autoformer\" (NeurIPS 2021)\n4. Liu et al. \"Pyraformer\" (ICLR 2022)\n5. Zhou et al. \"FEDformer\" (ICML 2022)\n\n---\n\n## é™„å½•ï¼šæ¨¡å‹æƒé‡å¯è§†åŒ–\n\nDLinearçš„æƒé‡å¯ä»¥æ­ç¤ºæ•°æ®ç‰¹å¾ï¼š\n- **è¶‹åŠ¿å±‚æƒé‡**ï¼šæ˜¾ç¤ºå“ªäº›å†å²æ—¶é—´æ­¥å¯¹é¢„æµ‹è´¡çŒ®æ›´å¤§\n- **æ®‹å·®å±‚æƒé‡**ï¼šå¯æ­ç¤ºæ•°æ®çš„å‘¨æœŸæ€§æ¨¡å¼\n\nä¾‹å¦‚ï¼ŒTrafficæ•°æ®é›†çš„æƒé‡å¯è§†åŒ–æ˜¾ç¤ºï¼š\n- æ—¥å‘¨æœŸæ€§ï¼ˆ24ä¸ªæ—¶é—´æ­¥ï¼‰\n- å‘¨å‘¨æœŸæ€§ï¼ˆ168ä¸ªæ—¶é—´æ­¥ï¼‰\n\nè¿™è¯æ˜äº†Linearæ¨¡å‹çš„å¯è§£é‡Šæ€§ä¼˜åŠ¿ã€‚\n\n---\n\n*ç¬”è®°æ•´ç†æ—¥æœŸï¼š2026å¹´2æœˆ23æ—¥*\n*è®ºæ–‡ç‰ˆæœ¬ï¼šarXiv:2205.13504v3*\n","structuredPatch":[],"originalFile":null},"sourceToolAssistantUUID":"2ef679c5-96e0-4386-8669-9833c30b23b2"}
{"parentUuid":"da137d5a-f232-406c-b380-dadc82fd42a7","isSidechain":false,"userType":"external","cwd":"/app/workspace","sessionId":"f068886d-0e50-4922-9128-2594a90f9c43","version":"2.1.50","gitBranch":"HEAD","slug":"functional-sparking-llama","message":{"id":"msg_202602232053134355d70e5b064a4e","type":"message","role":"assistant","model":"glm-5","content":[{"type":"thinking","thinking":"ç¬”è®°å·²ç»åˆ›å»ºå®Œæˆã€‚ç°åœ¨è®©æˆ‘æ›´æ–°ä»»åŠ¡çŠ¶æ€å¹¶å®Œæˆæ‰€æœ‰ä»»åŠ¡ã€‚","signature":""}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"output_tokens":0}},"type":"assistant","uuid":"f966e865-da5d-46ff-b3b4-f82f17db5bd2","timestamp":"2026-02-23T12:53:26.909Z"}
{"parentUuid":"f966e865-da5d-46ff-b3b4-f82f17db5bd2","isSidechain":false,"userType":"external","cwd":"/app/workspace","sessionId":"f068886d-0e50-4922-9128-2594a90f9c43","version":"2.1.50","gitBranch":"HEAD","slug":"functional-sparking-llama","message":{"id":"msg_202602232053134355d70e5b064a4e","type":"message","role":"assistant","model":"glm-5","content":[{"type":"tool_use","id":"call_21ae3d21d6b747e8a03d3b56","name":"TodoWrite","input":{"todos":[{"activeForm":"å·²å®Œæˆæœç´¢DLinearè®ºæ–‡","content":"æœç´¢å¹¶è®¿é—®DLinearè®ºæ–‡","status":"completed"},{"activeForm":"å·²å®Œæˆé˜…è¯»è®ºæ–‡","content":"é˜…è¯»å¹¶ç†è§£è®ºæ–‡æ ¸å¿ƒå†…å®¹","status":"completed"},{"activeForm":"å·²å®Œæˆæ•´ç†æ¨¡å‹æ¶æ„","content":"æ•´ç†æ¨¡å‹æ¶æ„å’Œæ•°å­¦åŸç†","status":"completed"},{"activeForm":"å·²å®Œæˆè®°å½•å®éªŒè®¾ç½®","content":"è®°å½•å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡","status":"completed"},{"activeForm":"å·²å®Œæˆæ’°å†™ç¬”è®°","content":"æ’°å†™å®Œæ•´çš„è®ºæ–‡é˜…è¯»ç¬”è®°","status":"completed"}]}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"output_tokens":0}},"type":"assistant","uuid":"d9a13f27-e7f2-4362-adb2-87f8dfd82138","timestamp":"2026-02-23T12:53:26.913Z"}
{"parentUuid":"d9a13f27-e7f2-4362-adb2-87f8dfd82138","isSidechain":false,"userType":"external","cwd":"/app/workspace","sessionId":"f068886d-0e50-4922-9128-2594a90f9c43","version":"2.1.50","gitBranch":"HEAD","slug":"functional-sparking-llama","type":"user","message":{"role":"user","content":[{"tool_use_id":"call_21ae3d21d6b747e8a03d3b56","type":"tool_result","content":"Todos have been modified successfully. Ensure that you continue to use the todo list to track your progress. Please proceed with the current tasks if applicable"}]},"uuid":"ea1b596b-a1db-448d-93ff-3e252a7e8ba0","timestamp":"2026-02-23T12:53:26.928Z","toolUseResult":{"oldTodos":[{"content":"æœç´¢å¹¶è®¿é—®DLinearè®ºæ–‡","status":"completed","activeForm":"å·²å®Œæˆæœç´¢DLinearè®ºæ–‡"},{"content":"é˜…è¯»å¹¶ç†è§£è®ºæ–‡æ ¸å¿ƒå†…å®¹","status":"completed","activeForm":"å·²å®Œæˆé˜…è¯»è®ºæ–‡"},{"content":"æ•´ç†æ¨¡å‹æ¶æ„å’Œæ•°å­¦åŸç†","status":"in_progress","activeForm":"æ­£åœ¨æ•´ç†æ¨¡å‹æ¶æ„"},{"content":"è®°å½•å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡","status":"pending","activeForm":"æ­£åœ¨è®°å½•å®éªŒè®¾ç½®"},{"content":"æ’°å†™å®Œæ•´çš„è®ºæ–‡é˜…è¯»ç¬”è®°","status":"pending","activeForm":"æ­£åœ¨æ’°å†™ç¬”è®°"}],"newTodos":[{"content":"æœç´¢å¹¶è®¿é—®DLinearè®ºæ–‡","status":"completed","activeForm":"å·²å®Œæˆæœç´¢DLinearè®ºæ–‡"},{"content":"é˜…è¯»å¹¶ç†è§£è®ºæ–‡æ ¸å¿ƒå†…å®¹","status":"completed","activeForm":"å·²å®Œæˆé˜…è¯»è®ºæ–‡"},{"content":"æ•´ç†æ¨¡å‹æ¶æ„å’Œæ•°å­¦åŸç†","status":"completed","activeForm":"å·²å®Œæˆæ•´ç†æ¨¡å‹æ¶æ„"},{"content":"è®°å½•å®éªŒè®¾ç½®å’Œè¯„ä¼°æŒ‡æ ‡","status":"completed","activeForm":"å·²å®Œæˆè®°å½•å®éªŒè®¾ç½®"},{"content":"æ’°å†™å®Œæ•´çš„è®ºæ–‡é˜…è¯»ç¬”è®°","status":"completed","activeForm":"å·²å®Œæˆæ’°å†™ç¬”è®°"}]},"sourceToolAssistantUUID":"d9a13f27-e7f2-4362-adb2-87f8dfd82138"}
{"parentUuid":"ea1b596b-a1db-448d-93ff-3e252a7e8ba0","isSidechain":false,"userType":"external","cwd":"/app/workspace","sessionId":"f068886d-0e50-4922-9128-2594a90f9c43","version":"2.1.50","gitBranch":"HEAD","slug":"functional-sparking-llama","message":{"id":"msg_20260223205327cf53811203e0432a","type":"message","role":"assistant","model":"glm-5","content":[{"type":"thinking","thinking":"æ‰€æœ‰ä»»åŠ¡å·²å®Œæˆã€‚ç°åœ¨è®©æˆ‘ç»™ç”¨æˆ·æä¾›ä¸€ä¸ªç®€æ´çš„æ€»ç»“ã€‚","signature":""}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"output_tokens":0}},"type":"assistant","uuid":"0e3e61be-7883-4b9a-a156-dd71ed485b93","timestamp":"2026-02-23T12:53:30.917Z"}
{"parentUuid":"0e3e61be-7883-4b9a-a156-dd71ed485b93","isSidechain":false,"userType":"external","cwd":"/app/workspace","sessionId":"f068886d-0e50-4922-9128-2594a90f9c43","version":"2.1.50","gitBranch":"HEAD","slug":"functional-sparking-llama","message":{"id":"msg_20260223205327cf53811203e0432a","type":"message","role":"assistant","model":"glm-5","content":[{"type":"text","text":"è®ºæ–‡é˜…è¯»ç¬”è®°å·²å®Œæˆï¼Œæ–‡ä»¶ä¿å­˜åœ¨ `/app/workspace/DLinear_Paper_Notes.md`ã€‚\n\n## å®Œæˆå†…å®¹æ€»ç»“\n\n### è®ºæ–‡æ ¸å¿ƒå‘ç°\n\n**æ ‡é¢˜**: \"Are Transformers Effective for Time Series Forecasting?\" (AAAI 2023 Oral)\n\n**æ ¸å¿ƒè®ºç‚¹**: Transformerçš„è‡ªæ³¨æ„åŠ›æœºåˆ¶æ˜¯**æ’åˆ—ä¸å˜çš„**ï¼Œå¯¼è‡´æ—¶é—´ä¿¡æ¯ä¸¢å¤±ï¼Œåœ¨æ—¶é—´åºåˆ—é•¿æœŸé¢„æµ‹(LTSF)ä»»åŠ¡ä¸Šè¢«è¿‡åº¦å¹æ§ã€‚\n\n### LTSF-Linearæ¨¡å‹å®¶æ—\n\n| æ¨¡å‹ | æ ¸å¿ƒè®¾è®¡ | é€‚ç”¨åœºæ™¯ |\n|------|---------|---------|\n| **Linear** | å•å±‚çº¿æ€§æ˜ å°„ | åŸºç¡€åŸºå‡† |\n| **DLinear** | è¶‹åŠ¿-å­£èŠ‚åˆ†è§£ + åŒçº¿æ€§å±‚ | æœ‰æ˜æ˜¾è¶‹åŠ¿çš„æ•°æ® |\n| **NLinear** | å‡å»æœ«å€¼ â†’ çº¿æ€§ â†’ åŠ å› | æœ‰åˆ†å¸ƒåç§»çš„æ•°æ® |\n\n### å…³é”®å®éªŒç»“æœ\n\n1. **æ€§èƒ½**: LTSF-Linearåœ¨æ‰€æœ‰9ä¸ªåŸºå‡†ä¸Šè¶…è¶ŠTransformer 20-50%\n2. **æ•ˆç‡**: DLinearä»…0.04G MACs vs Transformer 4.03G\n3. **å›çœ‹çª—å£**: Linearéšçª—å£å¢å¤§æ€§èƒ½æå‡ï¼ŒTransformeråè€Œä¸‹é™æˆ–æŒå¹³\n4. **é¡ºåºæ•æ„Ÿæ€§**: æ‰“ä¹±è¾“å…¥é¡ºåºåï¼ŒTransformeræ€§èƒ½å‡ ä¹ä¸å˜ï¼ŒLinearæ˜¾è‘—ä¸‹é™\n\n### å®éªŒé…ç½®è¦ç‚¹\n\n- **æ•°æ®é›†**: ETTh1/2, ETTm1/2, Traffic, Electricity, Exchange, Weather, ILI\n- **é¢„æµ‹é•¿åº¦**: {96, 192, 336, 720} (ILIä¸º{24,36,48,60})\n- **è¯„ä¼°æŒ‡æ ‡**: MSE, MAE\n- **å›çœ‹çª—å£**: Linearé»˜è®¤336ï¼ŒTransformeré»˜è®¤96\n\n### ä¸‹ä¸€æ­¥å¤ç°å»ºè®®\n\n1. ä»ETTæ•°æ®é›†å¼€å§‹éªŒè¯\n2. é‡ç‚¹å¯¹æ¯”Linear/DLinear/NLinearæ€§èƒ½å·®å¼‚\n3. éªŒè¯ä¸åŒå›çœ‹çª—å£å¤§å°çš„å½±å“\n4. å¤ç°æˆåŠŸæ ‡å‡†ï¼šä¸è®ºæ–‡ç»“æœè¯¯å·®åœ¨Â±5%ä»¥å†…\n\n---\n\n**Sources:**\n- [arXiv:2205.13504 - Are Transformers Effective for Time Series Forecasting?](https://arxiv.org/abs/2205.13504)\n- [GitHub - cure-lab/LTSF-Linear](https://github.com/cure-lab/LTSF-Linear)"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"output_tokens":0}},"type":"assistant","uuid":"4c875a08-b46b-47f2-86fe-fbd405ef7aa8","timestamp":"2026-02-23T12:53:50.306Z"}
